TCP/IP Performance over 3G Wireless Links with Rate and Delay Variation
ABSTRACT
Wireless link losses result in poor TCP throughput since
losses are perceived as congestion by TCP, resulting in source
throttling. In order to mitigate this effect, 3G wireless link
designers have augmented their system with extensive local
retransmission mechanisms. In addition, in order to increase
throughput, intelligent channel state based scheduling have
also been introduced. While these mechanisms have reduced
the impact of losses on TCP throughput and improved the
channel utilization, these gains have come at the expense
of increased delay and rate variability. In this paper, we
comprehensively evaluate the impact of variable rate and
variable delay on long-lived TCP performance. We propose
a model to explain and predict TCP's throughput over a link
with variable rate and/or delay. We also propose a network-based
solution called Ack Regulator that mitigates the effect
of variable rate and/or delay without significantly increasing
the round trip time, while improving TCP performance by
up to 40%.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Wireless
communication

General Terms
Algorithms, Performance, Design

INTRODUCTION
Third generation wide-area wireless networks are currently
being deployed in the United States in the form of 3G1X
technology [10] with speeds up to 144Kbps. Data-only enhancements
to 3G1X have already been standardized in the
3G1X-EVDO standard (also called High Data Rate or HDR)
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MOBICOM'02 September 23­28, Atlanta, Georgia, USA
Copyright 2002 ACM 1-58113-486-X/02/0009 ...
$
5.00.
with speeds up to 2Mbps [6]. UMTS [24] is the third generation
wireless technology in Europe and Asia with deploy-ments
planned this year. As these 3G networks provide pervasive
internet access, good performance of TCP over these
wireless links will be critical for end user satisfaction.
While the performance of TCP has been studied extensively
over wireless links [3, 4, 15, 20], most attention has
been paid to the impact of wireless channel losses on TCP.
Losses are perceived as congestion by TCP, resulting in
source throttling and very low net throughput.
In order to mitigate the effects of losses, 3G wireless link
designers have augmented their system with extensive local
retransmission mechanisms. For example, link layer retransmission
protocols such as RLP and RLC are used in
3G1X [22] and UMTS [21], respectively. These mechanisms
ensure packet loss probability of less than 1% on the wireless
link, thereby mitigating the adverse impact of loss on TCP.
While these mechanisms mitigate losses, they also increase
delay variability. For example, as we shall see in Section 3,
ping latencies vary between 179ms to over 1 second in a
3G1X system.
In addition, in order to increase throughput, intelligent
channel state based scheduling have also been introduced.
Channel state based scheduling [7] refers to scheduling techniques
which take the quality of wireless channel into account
while scheduling data packets of different users at the
base station.
The intuition behind this approach is that
since the channel quality varies asynchronously with time
due to fading, it is preferable to give priority to a user with
better channel quality at each scheduling epoch.
While
strict priority could lead to starvation of users with inferior
channel quality, a scheduling algorithm such as proportional
fair [6] can provide long-term fairness among different
users. However, while channel-state based scheduling improves
overall throughput, it also increases rate variability.
Thus, while the impact of losses on TCP throughput have
been significantly reduced by local link layer mechanisms
and higher raw throughput achieved by channel-state based
scheduling mechanisms, these gains have come at the expense
of increased delay and rate variability. This rate and
delay variability translates to bursty ack arrivals (also called
ack compression) at the TCP source. Since TCP uses ack
clocking to probe for more bandwidth, bursty ack arrival
leads to release of a burst of packets from the TCP source.
When this burst arrives at a link with variable rate or delay
, it could result in multiple packet losses. These multiple
losses significantly degrade TCPs throughput.
In this paper, we make three main contributions. First,
71
we comprehensively evaluate the impact of variable rate and
variable delay on long-lived TCP performance. Second, we
propose a model to explain and predict TCP's throughput
over a link with variable rate and/or delay. Third, we propose
a network-based solution called Ack Regulator that mitigates
the effect of variable rate and/or delay without significantly
increasing the round trip time, thereby improving
TCP performance.
The remaining sections are organized as follows. In Section
2, we discuss related work. In Section 3, we present the
motivation for our work using traces from a 3G1X system.
In Section 4, we describe a model for computing the throughput
of a long-lived TCP flow over links with variable rate
and variable delay. We then present a simple network-based
solution, called Ack Regulator, to mitigate the effect of variable
rate/delay in Section 5. In Section 6, we present extensive
simulation results that compare TCP performance with
and without Ack Regulator, highlighting the performance
gains using the Ack Regulator when TCP is subjected to
variable rate and delay. Finally, in Section 7, we present
our conclusions.
RELATED WORK
In this section, we review prior work on improving TCP
performance over wireless networks. Related work on the
modeling of TCP performance is presented in Section 4.
A lot of prior work has focused on avoiding the case of
a TCP source misinterpreting packet losses in the wireless
link as congestion signals. In [4], a snoop agent is introduced
inside the network to perform duplicate ack suppression and
local retransmissions on the wireless link to enhance TCP
performance. In [3], the TCP connection is split into two
separate connections, one over the fixed network and the
second over the wireless link. The second connection can
recover from losses quickly, resulting in better throughput.
Link-layer enhancements for reducing wireless link losses including
retransmission and forward error correction have
been proposed in [20]. Link layer retransmission is now part
of both the CDMA2000 and UMTS standards [10, 24]. In
order to handle disconnections (a case of long-lived loss),
M-TCP has been proposed [8]. The idea is to send the last
ack with a zero-sized receiver window so that the sender can
be in persist mode during the disconnection. Link failures
are also common in Ad Hoc networks and techniques to improve
TCP performance in the presence of link failures have
been proposed in [11]. Note that none of these approaches
address specifically the impact of delay and rate variation
on TCP, which is the focus of this paper.
Several generic TCP enhancements with special applica-bility
to wireless links are detailed in [12, 13]. These include
enabling the Time Stamp option, use of large window
size and window scale option, disabling Van Jacobson
header compression, and the use of Selective Acknowledgments
(Sack). Large window size and window scaling are
necessary because of the large delay of wireless link while
Sack could help TCP recover from multiple losses without
the expensive timeout recovery mechanism.
Another issue with large delay variation in wireless links
is spurious timeouts where TCP unnecessarily retransmits
a packet (and lowers its congestion window to a minimum)
after a timeout, when the packet is merely delayed. In [13],
the authors refer to rate variability due to periodic allocation
and de-allocation of high-speed channels in 3G networks
as Bandwidth Oscillation. Bandwidth Oscillation can
also lead to spurious timeouts in TCP because as the rate
changes from high to low, the rtt value increases and a low
Retransmission Timeout (RTO) value causes a spurious retransmission
and unnecessarily forces TCP into slow start.
In [15], the authors conduct experiments of TCP over GSM
circuit channels and show that spurious timeouts are extremely
rare. However, 3G wireless links can have larger
variations than GSM due to processing delays and rate variations
due to channel state based scheduling.
Given the
increased variability on 3G packet channels, the use of TCP
time stamp option for finer tracking of TCP round trip times
and possibly the use of Eifel retransmission timer [16] instead
of the conventional TCP timer can help avoid spurious
timeouts.
As mentioned earlier, the effect of delay and rate variability
is ack compression and this results in increased burstiness
at the source.
Ack compression can also be caused
by bidirectional flows over regular wired networks or single
flow over networks with large asymmetry. This phenomenon
has been studied and several techniques have been proposed
to tackle the burstiness of ack compression.
In order to
tackle burstiness, the authors in [18] propose several schemes
that withholds acks such that there is no packet loss at the
bottleneck router, resulting in full throughput. However,
the round trip time is unbounded and can be very large.
In [23], the authors implement an ack pacing technique at
the bottleneck router to reduce burstiness and ensure fairness
among different flows. In the case of asymmetric channels
, solutions proposed [5] include ack congestion control
and ack filtering (dropping of acks), reducing source burstiness
by sender adaptation and giving priority to acks when
scheduling inside the network. However, the magnitude of
asymmetry in 3G networks is not large enough and can be
tolerated by TCP without ack congestion control or ack filtering
according to [12].
Note that, in our case, ack compression occurs because
of link variation and not due to asymmetry or bidirectional
flows. Thus, we require a solution that specifically adapts
to link variation. Moreover, the node at the edge of the 3G
wireless access network is very likely to be the bottleneck
router (given rates of 144Kbps to 2Mbps on the wireless
link) and is the element that is exposed to varying delays and
service rates. Thus, this node is the ideal place to regulate
the acks in order to improve TCP performance.
This is
discussed in more detail in the next section.
MOTIVATION
MD
RNC
RNC
BS
BS
PDSN/
SGSN
BS:        Base Station
MD:      Mobile Device
HA/
GGSN
HA:       Home Agent
RNC:  Radio Network Controller
PDSN: Packet Data Service Node
SGSN: Serving GPRS Service Node
GGSN: Gateway GPRS Service Node
(RLP/RLC) Link Layer Retransmission
INTER
NET
Figure 1:
3G network architecture
A simplified architecture of a 3G wireless network is shown
72
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
200
400
600
800
1000
1200
1400
Prob.
Ping latency (ms)
Figure 2:
CDF of Ping Latencies
in Figure 1. The base stations are connected to a node called
the Radio Network Controller (RNC). The RNC performs
CDMA specific functions such as soft handoffs, encryption,
power control etc. It also performs link layer retransmission
using RLP(RLC) in 3G1X(UMTS) system. In the 3G1X
system, the RNC is connected to a PDSN using a GRE tunnel
(one form of IP in IP tunnel) and the PDSN terminates
PPP with the mobile device. If Mobile IP service is enabled,
the PDSN also acts as a Foreign Agent and connects to a
Home Agent. In the UMTS system, the RNC is connected
to a SGSN using a GTP tunnel (another form of IP in IP
tunnel); the SGSN is connected to a GGSN, again through
a GTP tunnel. Note that the tunneling between the various
nodes allows for these nodes to be connected directly or
through IP/ATM networks.
In this architecture, the RNC receives a PPP/IP packet
through the GRE/GTP tunnel from the PDSN/SGSN. The
RNC fragments this packet into a number of radio frames
and then performs transmission and local retransmission of
these radio frames using the RLP(RLC) protocol. The base
station (BS) receives the radio frames from the RNC and
then schedules the transmission of the radio frames on the
wireless link using a scheduling algorithm that takes the
wireless channel state into account. The mobile device receives
the radio frames and if it discovers loss of radio frames,
it requests local retransmission using the RLP(RLC) protocol
. Note that, in order to implement RLP(RLC), the RNC
needs to keep a per-user queue of radio frames. The RNC
can typically scale up to tens of base stations and thousands
of active users.
In order to illustrate the variability seen in a 3G system,
we obtained some traces from a 3G1X system. The system
consisted of an integrated BS/RNC, a server connected to
the RNC using a 10Mbps Ethernet and a mobile device connected
to the BS using a 3G1X link with 144Kbps downlink
in infinite burst mode and 8Kbps uplink. The infinite burst
mode implies that the rate is fixed and so the system only
had delay variability.
Figure 2 plots the cumulative distribution function (cdf)
of ping latencies from a set of 1000 pings from the server to
the mobile device (with no observed loss). While about 75%
of the latency values are below 200ms, the latency values go
all the way to over 1s with about 3% of the values higher
than 500ms.
In the second experiment, a TCP source at the server using
Sack with timestamp option transferred a 2MB file to
the mobile device. The MTU was 1500 bytes with user data
size of 1448 bytes. The buffer at the RNC was larger than
the TCP window size
1
. and thus, the transfer resulted in
no TCP packet loss and a maximal throughput of about
1
We did not have control over the buffer size at the RNC in
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Prob.
Interack time Time (s)
0
0.5
1
1.5
2
2.5
3
3.5
0
20
40
60
80
100
120
Rtt (s)
Time (s)
(a)
TCP Ack Inter-arrival
(b) TCP rtt value
Figure 3:
3G Link Delay Variability
135Kb/s. The transmission time at the bottleneck link is
1.448
8/135 = 86ms. If the wireless link delay were constant
, the TCP acks arriving at the source would be evenly
spaced with a duration of 172ms because of the delayed ack
feature of TCP (every 2 packets are acked rather than every
packet). Figure 3(a) plots the cdf of TCP ack inter-arrival
time (time between two consecutive acks) at the server. As
can be seen, there is significant ack compression with over
10% of the acks arriving within 50ms of the previous ack.
Note that the ack packet size is 52 bytes (40 + timestamp)
and ack transmission time on the uplink is 52
8/8=52ms;
an interack spacing of less then 52ms is a result of uplink
delay variation.
Note that the delay variability and the resulting ack compression
did not cause any throughput degradation in our
system. This was due to the fact that the buffering in the
system was greater than the TCP window size resulting in
no buffer overflow loss. Figure 3(b) depicts the TCP round
trip time (rtt) values over time. Since the buffer at the RNC
is able to accommodate the whole TCP window, the rtt increases
to over 3s representing a case of over 30 packets in
the buffer at the RNC (30
0.086 = 2.5s). Given an average
ping latency of 215ms and a transmission time of 86ms
for a 1500 byte packet, the bandwidth delay product of the
link is approximately (0.215 + 0.86)
135=5KB or about 3
packets. Thus, the system had a buffer of over 10 times the
bandwidth delay product. Given that we had only one TCP
flow in the system, a buffer of over 64KB is not a problem.
But, if every TCP flow is allocated a buffer of 64KB, the
buffer requirements at the RNC would be very expensive,
since the RNC supports thousands of active users.
Even discounting the cost of large buffers, the inflated rtt
value due to the excessive buffering has several negative consequences
as identified in [15]. First, an inflated rtt implies
a large retransmission timeout value (rto). In the case of
multiple packet losses (either on the wireless link or in a
router elsewhere in the network), a timeout-based recovery
would cause excessive delay, especially if exponential backoff
gets invoked. Second, if the timestamp option is not used,
the rtt sampling rate is reduced and this can cause spurious
timeouts. Third, there is a higher probability that the data
in the queue becomes obsolete (for e.g., due to user aborting
the transmission), but the queue will still have to be drained
resulting in wasted bandwidth.
Thus, while excessive buffering at the RNC can absorb
the variability of the wireless links without causing TCP
throughput degradation, it has significant negative side effects
, making it an undesirable solution.
our system.
73
MODEL
In this section, we model the performance of a single long-lived
TCP flow over a network with a single bottleneck server
that exhibits rate variation based on a given general distribution
and a single wireless link attached to the bottleneck
server that exhibits delay variation based on another given
distribution.
We use a general distribution of rate and delay values for
the discussion in this section since we would like to capture
the inherent variation in rate and delay that is a characteristic
of the 3G wireless data environment. Given that the
wireless standards are constantly evolving, the actual rate
and delay distribution will vary from one standard or implementation
to another and is outside the scope of this paper.
Later, in Section 6, we will evaluate TCP performance over
a specific wireless link, the 3G1X-EVDO (HDR) system, using
simulation.
We would like to model TCP performance in the case of
variable rate and delay for two reasons. One, we would like
to understand the dynamics so that we can design an appropriate
mechanism to improve TCP performance. Two,
we would like to have a more accurate model that specifically
takes the burstiness caused by ack compression due to
rate/delay variability into account.
TCP performance modeling has been extensively studied
in the literature [1, 2, 9, 14, 17, 19]. Most of these models
assume constant delay and service rate at the bottleneck
router and calculate TCP throughput in terms of packet loss
probability and round trip time. In [19], the authors model
TCP performance assuming deterministic time between congestion
events [1]. In [17], the authors improve the throughput
prediction of [19] assuming exponential time between
congestion events (loss indications as Poisson). In our case,
ack compressions and link variation causes bursty losses and
the deterministic or Poisson loss models are not likely to be
as accurate. In [9], the authors model an UMTS wireless
network by extending the model from [19] and inflating the
rtt value to account for the average additional delay incurred
on the wireless link. However, we believe this will not result
in an accurate model because 1) the rtt value in [19] is already
an end-to-end measured value and 2) the loss process
is much more bursty than the deterministic loss assumption
in [19]. In [2], the authors observe that mean values are
not sufficient to predict the throughput when routers have
varying bandwidth and show that increasing variance for the
same mean service rate decreases TCP throughput. However
, the approach is numerical, and provides little intuition
in the case of delay variance.
Our approach starts with the model in [14] which describes
how TCP functions in an "ideal" environment with
constant round trip time, constant service rate and suffers
loss only through buffer overflow. A brief summary of the
result from [14] is presented here before we proceed to our
model, which can be seen as an extension. We chose to extend
the model in [14] since it makes no assumption about
the nature of loss event process (which is highly bursty in
our case) and explicitly accounts for link delay and service
rate (which are variable in our case). For simplicity, we will
only discuss the analysis of TCP Reno. TCP Sack can be
analyzed similarly. We also assume that the sender is not
limited by the maximum receiver window; simple modifications
can be made to the analysis for handling this case.
Figure 4(a) shows how the TCP congestion window varies
0
5
10
15
20
25
30
35
40
45
0
50
100
150
200
Ideal TCP
0
5
10
15
20
25
30
35
40
45
50
0
50
100
150
200
TCP with Variable Delay
(a)
Constant delay
(b) Variable delay
Figure 4:
TCP Congestion Window Evolution over time
in a constant rate and delay setting. The initial phase where
TCP tries to probe for available bandwidth is the Slow Start
phase.
After slow start, TCP goes to Congestion avoidance
phase. In the case of long-lived TCP flow, one can
focus only on the congestion avoidance phase. Let µ be the
constant service rate,  the constant propagation delay, T
the minimum round trip time ( + 1/µ) and B the buffer
size.
The congestion window follows a regular saw-tooth
pattern, going from W
0
to W
max
, where W
0
= W
max
/2 and
W
max
= µ + B + 1. Due to the regularity of each of the
saw-tooth, consider one such saw-tooth.
Within a single
saw-tooth, the congestion avoidance phase is divided into
two epochs. In the first epoch, say epoch A, the congestion
window increases from W
0
to µT , in time t
A
with number
of packets sent n
A
. In the second epoch, say epoch B, the
congestion window increases from µT to W
max
, in time t
B
with number of packets sent n
B
. TCP throughput (ignoring
slow start) is simply given by (n
A
+ n
B
)/(t
A
+ t
B
) where
t
A
=
T (µT
- W
0
)
(1)
n
A
=
(W
0
t
A
+ t
2
A
/(2T ))/T
(2)
t
B
=
(W
2
max
- (µT )
2
)/(2µ)
(3)
n
B
=
µt
B
(4)
This model, while very accurate for constant µ and T ,
breaks down when the constant propagation and service rate
assumptions are not valid. Figure 4(b) shows how the congestion
window becomes much more irregular when there
is substantial variation in the wireless link delay. This is
because the delay variation and ack compression result in
multiple packet losses.
There are three main differences in the TCP congestion
window behavior under variable rate/delay from the traditional
saw-tooth behavior.
First, while the traditional
saw-tooth behavior always results in one packet loss due
to buffer overflow, we have possibilities for multiple packet
losses due to link variation. To account for this, we augment
our model with parameters p1, p2, p3 representing respectively
the conditional probability of a single packet loss,
double packet loss, and three or more packet losses. Note
that, p1 + p2 + p3 = 1 by this definition. Second, while
the loss in the traditional saw-tooth model always occurs
when window size reaches W
max
= µ + B + 1, in our model
losses can occur at different values of window size, since µ
and  are now both variables instead of constants. We capture
this by a parameter W
f
=
Õ

N
i=1
W
2
max
i
/N , that is the
square root of the second moment of the W
max
values of each
cycle. The reason we do this instead of obtaining a simple
mean of W
max
values is because throughput is related to
W
f
quadratically (since it is the area under the curve in the
74
0
5
10
15
20
25
128
130
132
134
136
138
140
142
cwnd (packets)
Time (s)
0
5
10
15
20
25
100
105
110
115
cwnd (packets)
Time (s)
(a)
Two packet loss
(b) Three packet loss
Figure 5:
Congestion Window with multiple losses
congestion window graph). Third, due to the fact that we
have multiple packet losses in our model, we need to consider
timeouts and slow starts in our throughput calculation. We
represent the timeout duration by the T
0
parameter which
represents the average timeout value, similar to the timeout
parameter in [19].
We now model the highly variable congestion window behavior
of a TCP source under rate/delay variation. We first
use W
f
instead of W
max
. We approximate  (the propagation
delay) by ^
, the average link delay in the presence
of delay variability. We replace µ (the service rate) by ^
µ,
the average service rate in the presence of rate variability.
Thus, T becomes ^
T = (^
+ 1 / ^
µ). Now consider three
different congestion window patterns: with probability p1,
single loss followed by congestion avoidance, with probability
p2, double loss followed by congestion avoidance, and
with probability p3, triple loss and timeout followed by slow
start and congestion avoidance
2
.
First, consider the single loss event in the congestion avoidance
phase. This is the classic saw-tooth pattern with two
epochs as identified in [14].
Lets call these A1 and B1
epochs. In epoch A1, window size grows from W
01
to ^
µ ^
T
in time, t
A1
, with number of packets transmitted, n
A1
. In
epoch B1, window size grows from ^
µ ^
T to W
f
in time, t
B1
,
with number of packets transmitted, n
B1
. Thus, with probability
p1, n
A1
+n
B1
packets are transmitted in time t
A1
+t
B1
where
W
01
=
(int)W
f
/2
(5)
t
A1
=
^
T (^
µ ^
T
- W
01
)
(6)
n
A1
=
(W
01
t
A1
+ t
2
A1
/(2 ^
T ))/ ^
T
(7)
t
B1
=
(W
2
f
- (^µ ^
T )
2
)/(2^
µ)
(8)
n
B1
=
^
µt
B1
(9)
Next, consider the two loss event. An example of this
event is shown in Figure 5(a). The trace is obtained using
ns-2 simulation described in Section 6.
In this case,
after the first fast retransmit (around 130s), the source receives
another set of duplicate acks to trigger the second
fast retransmit (around 131s). This fixes the two losses and
the congestion window starts growing from W
02
. The second
retransmit is triggered by the new set of duplicate acks
in response to the first retransmission. Thus, the duration
between the first and second fast retransmit is the time re-quired
for the first retransmission to reach the receiver (with
a full buffer) plus the time for the duplicate ack to return
2
We assume that three or more packet losses result in a
timeout; this is almost always true if the source is TCP
reno.
to the sender. In other words, this duration can be approximated
by the average link delay with a full buffer, ^
T +
B/^
µ=t
R
. We have three epochs now, epoch t
R
(time 130-131s
)with one retransmission and zero new packet, epoch
A2 (131-137s) with window size growing from W
02
to ^
µ ^
T
in time, t
A2
, with number of packets transmitted, n
A2
, and
epoch B1 (137-143s) as before. Thus, with probability p2,
n
A2
+n
B1
packets are transmitted in time t
R
+t
A2
+t
B1
where
W
02
=
(int)W
01
/2
(10)
t
R
=
^
T + B/^
µ
(11)
t
A2
=
^
T (^
µ ^
T
- W
02
)
(12)
n
A2
=
(W
02
t
A2
+ t
2
A2
/(2 ^
T ))/ ^
T
(13)
Finally, consider the three loss event. An example of this
event is shown in Figure 5(b). In this case, after the first fast
retransmit, we receive another set of duplicate acks to trigger
the second fast retransmit. This does not fixthe three
losses and TCP times out. Thus, we now have five epochs:
first is the retransmission epoch (100-101s) with time t
R
and
zero new packet, second is the timeout epoch (101-103s) with
time T
0
and zero new packet, third is the slow start epoch
(103-106s) where the window grows exponentially up to previous
ssthresh value of W
03
in time t
ss
(Eqn. 15) with number
of packets transmitted n
ss
(Eqn. 16)
3
, fourth is epoch A3
(106-111s) where the window size grows from W
03
to ^
µ ^
T in
time t
A3
(Eqn. 17) with number of packets transmitted n
A3
(Eqn. 18), and fifth is epoch B1 (111-118s) as before. Thus,
with probability p3, n
ss
+n
A3
+n
B1
packets are transmitted
in time t
R
+T
0
+t
ss
+t
A2
+t
B1
where
W
03
=
(int)W
02
/2
(14)
t
ss
=
^
T log
2
(W
03
)
(15)
n
ss
=
W
03
/ ^
T
(16)
t
A3
=
^
T (^
µ ^
T
- W
03
)
(17)
n
A3
=
(W
03
t
A3
+ t
2
A3
/(2 ^
T ))/ ^
T
(18)
Given that the different types of packet loss events are independent
and using p1+p2+p3=1, the average TCP throughput
can now be approximated by a weighted combination of
the three types of loss events to be
p3
(n
ss
+ n
A3
) + p2
n
A2
+ p1
n
A1
+ n
B1
p3
(t
R
+ T
0
+ t
ss
+ t
A3
) + p2
(t
R
+ t
A2
) + p1
t
A1
+ t
B1
(19)
If any of t

are less than 0, those respective epochs do not
occur and we can use the above equation while setting the
respective n

, t

to zero.
In this paper, we infer parameters such as p1, p2, p3, W
f
,
and T
0
from the traces. Models such as [19] also infer the
loss probability, round trip time, and timeout durations from
traces.
Table 1 lists the various parameters used by the different
models for simulations with rate and delay variability. We
use a packet size of 1000 bytes, a buffer of 10 which represents
the product of the average bandwidth times average
delay and we ensure that the source is not window limited.
T D and T O denote the number of loss events that are of
the triple duplicate and timeout type respectively and these
values are used by models in [19] and [17]. The simulation
3
using analysis similar to [14] and assuming adequate buffer
so that there is no loss in slow start.
75
Item
Rate(Kb/s)
Delay(ms)
pkts
TD
TO
T
0
rtt
p1
p2
W
f
^
T
^
µ
1
200
400
89713
401
1
1.76
616.2
0.998
0.000
22.00
440
25.0
2
200
380+e(20)
83426
498
1
1.71
579.3
0.639
0.357
21.38
442
25.0
3
200
350+e(50)
78827
489
12
1.79
595.8
0.599
0.367
21.24
461
25.0
4
200
300+e(100)
58348
496
114
1.92
606.0
0.339
0.279
18.95
517
25.0
5
u(200,20)
400
82180
504
1
1.75
578.1
0.535
0.460
21.61
400
24.74
6
u(200,50)
400
74840
517
29
1.80
579.9
0.510
0.403
20.52
400
23.34
7
u(200,75)
400
62674
516
81
1.86
585.9
0.398
0.348
19.05
400
20.93
8
u(200,50)
350+e(50)
70489
507
43
1.81
595.7
0.496
0.377
20.15
459
23.34
9
u(200,75)
300+e(100)
53357
497
93
2.03
635.7
0.404
0.298
17.78
511
20.93
Table 1:
Simulation and Model parameters
Item
Simulator Goodput
Model 1 [19] (accu.)
Model 2 [17] (accu.)
Model 3[Eqn. 19] (accu.)
1
199.8
228.5(0.86)
201.9(0.99)
199.8(1.0)
2
185.4
208.0(0.88)
186.0(1.0)
186.0(1.0)
3
175.1
195.5(0.88)
177.2(0.99)
180.9(0.97)
4
129.4
145.3(0.88)
153.7(0.81)
137.0(0.94)
5
182.5
205.2(0.88)
184.6(0.99)
181.3(0.99)
6
166.2
186.0(0.88)
174.6(0.95)
165.2(0.99)
7
139.2
158.4(0.86)
163.4(0.83)
137.2(0.99)
8
156.5
174.6(0.88)
166.5(0.94)
160.2(0.97)
9
118.4
134.0(0.87)
142.6(0.80)
125.0(0.94)
Table 2:
Simulation and Model throughput values
is run for 3600 seconds. We simulate delay and rate variability
with exponential and uniform distributions respectively
(u(a, b) in the table represents uniform distribution
with mean a and standard deviation b while e(a) represents
an exponential distribution with mean a). The details of the
simulation are presented in Section 6.
Table 2 compares the throughput of simulation of different
distributions for rate and delay variability at the server
and the throughput predicted by the exact equation of the
model in [19], the Poisson model in [17] and by equation 19.
The accuracy of the prediction, defined as 1 minus the ratio
of the difference between the model and simulation throughput
value over the simulation throughput value, is listed in
the parenthesis. As the last column shows, the match between
our model and simulation is extremely accurate when
the delay/rate variation is small and the match is still well
over 90% even when the variation is large. The Poisson loss
model used in [17] performs very well when the variability
is low but, understandably, does not predict well when
variability increases. The deterministic loss model seems to
consistently overestimate the throughput.
From Table 1, one can clearly see the impact of delay and
rate variability. As the variability increases, the probability
of double loss, p2, and three or more losses, p3=(1-p2-p1),
start increasing while the goodput of the TCP flow starts
decreasing. For example, comparing case 1 to case 4, p1
decreases from 0.998 to 0.339 while p3 increases. Increases
in p2 and p3 come about because when the product ^
T ^
µ
decreases, a pipe that used to accommodate more packets
suddenly becomes smaller causing additional packet losses.
Given that n
A1
/t
A1
&gt; n
A2
/(t
R
+ t
A2
) &gt; (n
ss
+ n
A3
)/(t
R
+
T
0
+t
ss
+t
A3
), any solution that improves TCP performance
must reduce the occurrence of multiple packet losses, p2 and
p3. We present a solution that tries to achieve this in the
next section.
ACK REGULATOR
In this section, we present our network-based solution
for improving TCP performance in the presence of varying
bandwidth and delay. The solution is designed for improving
the performance of TCP flows towards the mobile host
(for downloading-type applications) since links like HDR are
designed for such applications. The solution is im