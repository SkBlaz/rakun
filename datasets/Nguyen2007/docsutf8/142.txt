Obfuscated Databases and Group Privacy
ABSTRACT
We investigate whether it is possible to encrypt a database
and then give it away in such a form that users can still access
it, but only in a restricted way. In contrast to conventional
privacy mechanisms that aim to prevent any access
to individual records, we aim to restrict the set of queries
that can be feasibly evaluated on the encrypted database.
We start with a simple form of database obfuscation which
makes database records indistinguishable from lookup functions
. The only feasible operation on an obfuscated record
is to look up some attribute Y by supplying the value of
another attribute X that appears in the same record (i.e.,
someone who does not know X cannot feasibly retrieve Y ).
We then (i) generalize our construction to conjunctions of
equality tests on any attributes of the database, and (ii)
achieve a new property we call group privacy. This property
ensures that it is easy to retrieve individual records or small
subsets of records from the encrypted database by identifying
them precisely, but "mass harvesting" queries matching
a large number of records are computationally infeasible.
Our constructions are non-interactive. The database is
transformed in such a way that all queries except those ex-plicitly
allowed by the privacy policy become computationally
infeasible, i.e., our solutions do not rely on any access-control
software or hardware.
Categories and Subject Descriptors
:
E.3[Data Encryption]; H.2.7[Database Administration]:
Security, integrity, and protection

General Terms
: Security

INTRODUCTION
Conventional privacy mechanisms usually provide all-or-nothing
privacy. For example, secure multi-party computation
schemes enable two or more parties to compute some
joint function while revealing no information about their respective
inputs except what is leaked by the result of the
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CCS'05, November 7­11, 2005, Alexandria, Virginia, USA.
Copyright 2005 ACM 1-59593-226-7/05/0011 ...
$
5.00.
computation. Privacy-preserving data mining aims to com-pletely
hide individual records while computing global statistical
properties of the database. Search on encrypted data
and private information retrieval enable the user to retrieve
data from an untrusted server without revealing the query.
In this paper, we investigate a different concept of privacy.
Consider a data owner who wants to distribute a database to
potential users. Instead of hiding individual data entries, he
wants to obfuscate the database so that only certain queries
can be evaluated on it, i.e., the goal is to ensure that the
database, after it has been given out to users, can be accessed
only in the ways permitted by the privacy policy.
Note that there is no interaction between the data owner and
the user when the latter accesses the obfuscated database.
Our constructions show how to obfuscate the database
before distributing it to users so that only the queries permitted
by the policy are computationally feasible. This concept
of privacy is incomparable to conventional definitions
because, depending on the policy, a permitted query may or
even should reveal individual data entries.
For example, a college alumni directory may be obfuscated
in such a way that someone who already knows a person's
name and year of graduation is able to look up that person's
email address, yet spammers cannot indiscriminately
harvest addresses listed in the directory. Employees of a
credit bureau need to have access to customers' records so
that they can respond to reports of fraudulent transactions,
yet one may want to restrict the bureau's ability to compile
a list of customers' addresses and sell it to a third party.
We develop provably secure obfuscation techniques for
several types of queries. We do not assume that users of the
obfuscated database access it through a trusted third party,
nor that they use trusted or "tamper-proof" access-control
software or hardware (in practice, such schemes are vulnerable
to circumvention and reverse-engineering, while trusted
third parties are scarce and often impractical). Our constructions
are cryptographically strong, i.e., they assume an
adversary who is limited only by his computational power.
We prove security in the standard "virtual black-box"
model for obfuscation proposed by Barak et al. [2]. Intuitively
, a database is securely obfuscated if the view of any
efficient adversary with access to the obfuscation can be efficiently
simulated by a simulator who has access only to
the ideal functionality, which is secure by definition. The
ideal functionality can be thought of as the desired privacy
policy for the database. One of our contributions is coming
up with several ideal functionalities that capture interesting
privacy policies for databases.
102
Directed-access databases.
Our "warm-up" construction
is a directed-access database. Some attributes of the
database are designated as query attributes, and the rest
as data attributes. The database is securely obfuscated if,
for any record, it is infeasible to retrieve the values of the
data attributes without supplying the values of the query
attributes, yet a user who knows the query attributes can
easily retrieve the corresponding data attributes.
To illustrate by example, a directed-access obfuscation of
a telephone directory has the property that it is easy to
look up the phone number corresponding to a particular
name-address pair, but queries such as "retrieve all phone
numbers stored in the directory" or "retrieve all names"
are computationally infeasible. Such a directory is secure
against abusive harvesting, but still provides useful functionality
. Note that it may be possible to efficiently enumerate
all name-address pairs because these fields have less
entropy than regular cryptographic keys, and thus learn the
entire database through the permitted queries. Because the
database is accessed only in permitted ways, this does not
violate the standard definition of obfuscation. Below, we
give some examples where it is not feasible to enumerate all
possible values for query attributes.
The directed-access property of a single database record
can be modeled as a point function, i.e., a function from
{0, 1}
n
to {0, 1} that returns 1 on exactly one input x (in
our case, query attributes are the arguments of the point
function). Directed-access obfuscation guarantees that the
adversary's view of any obfuscated record can be efficiently
simulated with access only to this point function. Therefore
, for this "warm-up" problem, we can use obfuscation
techniques for point functions such as [22]. Informally, we
encrypt the data attributes with a key derived from hashed
query attributes. The only computationally feasible way to
retrieve the data attributes is to supply the corresponding
query attributes. If the retriever does not know the right
query attributes, no information can be extracted at all.
Group-exponential databases.
We then consider a more
interesting privacy policy, which requires that computational
cost of access be exponential in the number of database
records retrieved. We refer to this new concept of privacy
as group privacy. It ensures that users of the obfuscated
database can retrieve individual records or small subsets of
records by identifying them precisely, i.e., by submitting
queries which are satisfied only by these records. Queries
matching a large number of records are infeasible.
We generalize the idea of directed access to queries consisting
of conjunctions of equality tests on query attributes,
and then to any boolean circuit over attribute equalities.
The user can evaluate any query of the form attribute
j
1
=
value
1
. . .attribute
j
t
= value
t
, as long as it is satisfied by
a small number of records. Our construction is significantly
more general than simple keyword search on encrypted data
because the value of any query attribute or a conjunction
thereof can be used as the "keyword" for searching the obfuscated
database, and the obfuscator does not need to know
what queries will be evaluated on the database.
To distinguish between "small" and "large" queries, suppose
the query predicate is satisfied by n records.
Our
construction uses a form of secret sharing that forces the
retriever to guess n bits before he can access the data attributes
in any matching record. (If n=1, i.e., the record is
unique, the retriever still has to guess 1 bit, but this simply
means that with probability
1
2
he has to repeat the query.)
The policy that requires the retriever to uniquely identify
a single record, i.e., forbids any query that is satisfied by
multiple records, can also be easily implemented using our
techniques. Our construction can be viewed as the noninteractive
analog of hash-reversal "client puzzles" used to
prevent denial of service in network security [21], but, unlike
client puzzles, it comes with a rigorous proof of security.
For example, consider an airline passenger database in
which every record contains the passenger's name, flight
number, date, and ticket purchase details. In our construction
, if the retriever knows the name and date that uniquely
identify a particular record (e.g., because this information
was supplied in a court-issued warrant), he (almost) immediately
learns the key that encrypts the purchase details in
the obfuscated record. If the passenger traveled on k flights
on that date, the retriever learns the key except for k bits.
Since k is small, guessing k bits is still feasible. If, however,
the retriever only knows the date and the flight number, he
learns the key except for m bits, where m is the number of
passengers on the flight, and retrieval of these passengers'
purchase details is infeasible.
A database obfuscated using our method has the group
privacy property in the following sense. It can be accessed
only via queries permitted by the privacy policy. The probability
of successfully evaluating a permitted query is inversely
exponential in the number of records that satisfy the
query predicate. In particular, to extract a large number of
records from the database, the retriever must know a pri-ori
specific information that uniquely identifies each of the
records, or small subsets thereof. The obfuscated database
itself does not help him obtain this information.
In obfuscated databases with group privacy, computational
cost of access depends on the amount of information
retrieved. Therefore, group privacy can be thought of as a
step towards a formal cryptographic model for "economics
of privacy." It is complementary to the existing concepts of
privacy, and appears to be a good fit for applications such
as public directories and customer relationship management
(CRM) databases, where the database user may need to access
an individual record for a legitimate business purpose,
but should be prevented from extracting large subsets of
records for resale and abusive marketing.
While our constructions for group privacy are provably
secure in the "virtual black-box" sense of [2], the cost of
this rigorous security is a quadratic blowup in the size of the
obfuscated database, rendering the technique impractical for
large datasets. We also present some heuristic techniques to
decrease the size of the obfuscated database, and believe
that further progress in this area is possible.
Alternative privacy policies.
Defining rigorous privacy
policies that capture intuitive "database privacy" is an important
challenge, and we hope that this work will serve as
a starting point in the discussion. For example, the group
privacy policy that we use in our constructions permits the
retriever to learn whether a given attribute of a database
record is equal to a particular value. While this leaks more
information than may be desirable, we conjecture that the
privacy policy without this oracle is unrealizable.
We also consider privacy policies that permit any query
rather than just boolean circuits of equality tests on attributes
. We show that this policy is vacuous: regardless
of the database contents, any user can efficiently extract
103
the entire database by policy-compliant queries. Therefore,
even if the obfuscation satisfies the virtual black-box property
, it serves no useful purpose. Of course, there are many
types of queries that are more general than boolean circuits
of equality tests on attributes. Exact characterization of
non-vacuous, yet realizable privacy policies is a challenging
task, and a topic of future research.
Organization of the paper.
We discuss related work in
section 2. The ideas are illustrated with a "warm-up" construction
in section 3. In section 4, we explain group privacy
and the corresponding obfuscation technique. In section 5,
we generalize the class of queries to boolean circuits over attribute
equalities. In section 6, we show that policies which
permit arbitrary queries are vacuous, and give an informal
argument that a policy that does not allow the retriever to
verify his guesses of individual attribute values cannot be realized
. Conclusions are in section 7. All proofs will appear
in the full version of the paper.
RELATED WORK
This paper uses the "virtual black-box" model of obfuscation
due to Barak et al. [2]. In addition to the impossibility
result for general-purpose obfuscation, [2] demonstrates several
classes of circuits that cannot be obfuscated. We focus
on a different class of circuits.
To the best of our knowledge, the first provably secure
constructions for "virtual black-box" obfuscation were proposed
by Canetti et el. [5, 6] in the context of "perfectly
one-way" hash functions, which can be viewed as obfuscators
for point functions (a.k.a. oracle indicators or delta
functions). Dodis and Smith [15] recently showed how to
construct noise-tolerant "perfectly one-way" hash functions.
which they used to obfuscate proximity queries with "en-tropic
security." It is not clear how to apply techniques
of [15] in our setting. In section 6, we present strong evidence
that our privacy definitions may not be realizable if
queries other than equality tests are permitted.
Lynn et al. [22] construct obfuscators for point functions
(and simple extensions, such as public regular expressions
with point functions as symbols) in the random oracle model.
The main advantage of [22] is that it allows the adversary
partial information about the preimage of the hash function,
i.e., secrets do not need to have high entropy. This feature
is essential in our constructions, too, thus we also use the
random oracle model. Wee [27] proposed a construction for
a weaker notion of point function obfuscation, along with
the impossibility result for uniformly black-box obfuscation.
This impossibility result suggests that the use of random
oracles in our proofs (in particular, the simulator's ability
to choose the random oracle) is essential.
Many ad-hoc obfuscation schemes have been proposed in
the literature [1, 10, 9, 12, 13, 11]. Typically, these schemes
contain neither a cryptographic definition of security, nor
proofs, except for theoretical work on software protection
with hardware restrictions on the adversary [19, 20].
Forcing the adversary to pay some computational cost for
accessing a resource is a well-known technique for preventing
malicious resource exhaustion (a.k.a. denial of service
attacks). This approach, usually in the form of presenting
a puzzle to the adversary and forcing him to solve it, has
been proposed for combating junk email [16], website metering
[17], prevention of TCP SYN flooding attacks [21],
protecting Web protocols [14], and many other applications.
Puzzles based on hash reversal, where the adversary must
discover the preimage of a given hash value where he already
knows some of the bits, are an especially popular technique
[21, 14, 26], albeit without any proof of security. Our
techniques are similar, but our task is substantially harder
in the context of non-interactive obfuscation.
The obfuscation problem is superficially similar to the
problem of private information retrieval [3, 8, 18] and keyword
search on encrypted data [25, 4]. These techniques are
concerned, however, with retrieving data from an untrusted
server, whereas we are concerned with encrypting the data
and then giving them away, while preserving some control
over what users can do with them.
A recent paper by Chawla et al. [7] also considers database
privacy in a non-interactive setting, but their objective is
complementary to ours. Their definitions aim to capture privacy
of data, while ours aim to make access to the database
indistinguishable from access to a certain ideal functionality.
DIRECTED-ACCESS DATABASES
As a warm-up example, we show how to construct directed-access
databases in which every record is indistinguishable
from a lookup function. The constructions and theorems in
this section are mainly intended to illustrate the ideas.
Let X be a set of tuples 
x
, Y a set of tuples 
y
, and
Y

= Y  {}. Let D  X × Y be the database. We want to
obfuscate each record of D so that the only operation that
a user can perform on it is to retrieve 
y
if he knows 
x
.
We use the standard approach in secure multi-party computation
, and formally define this privacy policy in terms of
an ideal functionality. The ideal functionality is an (imaginary
) trusted third party that permits only policy-compliant
database accesses. An obfuscation algorithm is secure if any
access to the obfuscated database can be efficiently simulated
with access only to the ideal functionality. This means
that the user can extract no more information from the obfuscated
database than he would be able to extract had all
of his accesses been filtered by the trusted third party.
Definition 1. (Directed-access privacy policy) For
database D, define the corresponding directed-access functionality
DA
D
as the function that, for any input 
x
X
such that 
x , 
y
1
, . . . , 
x , 
y
m
D, outputs {
y
1
, . . . , 
y
m
}.
Intuitively, a directed-access database is indistinguishable
from a lookup function. Given the query attributes of an
individual record (
x
), it is easy to learn the data attributes
(
y
), but the database cannot be feasibly accessed in any
other way. In particular, it is not feasible to discover the
value of 
y
without first discovering a corresponding 
x
.
Moreover, it is not feasible to harvest all 
y
values from
the database without first discovering all values of 
x
.
This definition does not say that, if set X is small, it is
infeasible to efficiently enumerate all possible values of 
x
and stage a dictionary attack on the obfuscated database.
It does guarantee that even for this attack, the attacker is
unable to evaluate any query forbidden by the privacy policy.
In applications where X cannot be efficiently enumerated
(e.g., X is a set of secret keywords known only to some
users of the obfuscated database), nothing can be retrieved
from the obfuscated database by users who don't know the
keywords. Observe that 
x
can contain multiple attributes,
104
and thus multiple keywords may be required for access to

y
in the obfuscated database.
Directed-access databases are easy to construct in the random
oracle model, since lookup functionality is essentially
a point function on query attributes, and random oracles
naturally provide an obfuscation for point functions [22].
The obfuscation algorithm OB
da
takes D and replaces every
record 
x
i
, 
y
i
D with
hash
(r
i
1
||
x
i
), hash(r
i
2
||
x
i
)  
y
i
, r
i
1
, r
i
2
where r
i
1,2
are random numbers, || is concatenation, and
hash
is a hash function implementing the random oracle.
Theorem 1. (Directed-access obfuscation is "virtual
black-box") Let OB
da
be as described above. For any
probabilistic polynomial-time adversarial algorithm A, there
exists a probabilistic polynomial-time simulator algorithm S
and a negligible function  of the security parameter k such
that for any database D:
|P(A(OB
da
(D)) = 1) - P(S
DA
D
(1
|D|
) = 1)|  (k)
where probability P is taken over random oracles (implemented
as hash functions), as well as the the randomness of
A
and S. Intuitively, this theorem holds because retrieving

y
i
requires finding the (partial) pre-image of hash(r
i
2
, 
x
i
).
The standard definition of obfuscation in [2] also requires
that there exist an efficient retrieval algorithm that, given
some 
x

, extracts the corresponding 
y
from the obfuscation
OB
da
(D). Clearly, our construction has this property
. Someone who knows 
x

simply finds the record(s)
in which the first value is equal to hash(r
i
1
||
x

), computes
hash
(r
i
2
||
x

) and uses it as the key to decrypt 
y
.
GROUP-EXPONENTIAL DATABASES
For the purposes of this section, we restrict our attention
to queries P that are conjunctions of equality tests over
attributes (in section 5, we show how this extends to arbitrary
boolean circuits over equality tests). For this class of
queries, we show how to obfuscate the database so that evaluation
of the query is exponential in the size of the answer
to the query. Intuitively, this means that only precise query
predicates, i.e., those that are satisfied by a small number
of records, can be efficiently computed. "Mass harvesting"
queries, i.e., predicates that are satisfied by a large number
of records, are computationally infeasible.
Recall that our goal is to restrict how the database can
be accessed. For some databases, it may be possible to efficiently
enumerate all possible combinations of query attributes
and learn the entire database by querying it on every
combination. For databases where the values of query
attributes are drawn from a large set, our construction prevents
the retriever from extracting any records that he cannot
describe precisely. In either case, we guarantee that the
database can be accessed only through the interface permitted
by the privacy policy, without any trust assumptions
about the retriever's computing environment.
In our construction, each data attribute is encrypted with
a key derived from a randomly generated secret. We use a
different secret for each record. The secret itself is split into
several (unequal) shares, one per each query attribute. Each
share is then encrypted itself, using a key derived from the
output of the hash function on the value of the corresponding
query attribute. If the retriever knows the correct value only
for some query attribute a, he must guess the missing shares.
The size of the revealed share in bits is inversely related to
the number of other records in the database that have the
same value of attribute a. This provides protection against
queries on frequently occurring attribute values.
4.1
Group privacy policy
We define the privacy policy in terms of an ideal functionality
, which consists of two parts. When given an index
of a particular query attribute and a candidate value, it responds
whether the guess is correct, i.e., whether this value
indeed appears in the corresponding attribute of the original
database. When given a predicate, it evaluates this predicate
on every record in the database. For each record on
which the predicate is true, it returns this record's data attributes
with probability 2
-q
, where q is the total number
of records in the database that satisfy the predicate. if no
more information can be extracted this ideal functionality.
Definition 2. (Group privacy policy) Let X be a set
and
Y a set of tuples. Let D be the database
1
,
2
, . . .
N
where
i
= {x
i
1
, x
i
2
, . . . , x
im
, 
y
i
}  X
m
× Y. Let P : X
m

{0, 1} be a predicate of the form X
j
1
= x
j
1
X
j
2
= x
j
2
. . .
X
j
t
= x
j
t
. Let D
[P]
= {
i
D | P(x
i
1
, x
i
2
, . . . , x
im
) = 1}
be the subset of records on which
P is true.
The group-exponential functionality GP
D
consists of two
functions:
- C
D
(x, i, j) is 1 if x = x
ij
and 0 otherwise, where 1  i
N,
1  j  m.
- R
D
(P) =
Ë
1iN
{ i,
i
}, where

i
=

y
i
with probability 2
-|D
[P]
|
if
P(
i
)

with probability 1 - 2
-|D
[P]
|
if
P(
i
)

if
¬P(
i
)
Probability is taken over the internal coin tosses of
GP
D
.
Informally, function C answers whether the jth attribute of
the ith record is equal to x, while function R returns all
records that satisfy some predicate P, but only with probability
inversely exponential in the number of such records.
It may appear that function C is unnecessary. Moreover,
it leaks additional information, making our privacy policy
weaker than it might have been. In section 6, we argue
that it cannot be simply eliminated, because the resulting
functionality would be unrealizable. Of course, there may
exist policies that permit some function C

which leaks less
information than C, but it is unclear what C

might be. We
discuss several alternatives to our definition in section 6.
We note that data attributes are drawn from a set of tuples
Y because there may be multiple data attributes that
need to be obfuscated. Also observe that we have no restrictions
on the values of query attributes, i.e., the same
m
-tuple of query attributes may appear in multiple records,
with different or identical data attributes.
4.2
Obfuscating the database
We now present the algorithm OB
gp
, which, given any
database D, produces its obfuscation. For notational convenience
, we use a set of random hash functions H

: {0, 1}


{0, 1}
k
. Given any hash function H, these can be implemented
simply as H(||x). To convert the k-bit hash function
output into a key as long as the plaintext to which it is
105
applied, we use a set of pseudo-random number generators
prg
,
: {0, 1}
k
{0, 1}

(this implements random oracles
with unbounded output length).
Let N be the number of records in the database. For
each row
i
, 1  i  N , generate a random N -bit secret
r
i
= r
i
1
||r
i
2
|| . . . ||r
iN
, where r
ij

R
{0, 1}. This secret will
be used to protect the data attribute 
y
i
of this record. Note
that there is 1 bit in r
i
for each record of the database.
Next, split r
i
into m shares corresponding to query attributes
. If the retriever can supply the correct value of
the jth attribute, he will learn the jth share (1  j  m).
Denote the share corresponding to the jth attribute as s
ij
.
Shares are also N bits long, i.e., s
ij
= s
ij
1
|| . . . ||s
ijN
.
Each of the N bits of s
ij
has a corresponding bit in r
i
,
which in its turn corresponds to one of the N records in the
database. For each p s.t. 1  p  N , set s
ijp
= r
ip
if x
ij
=
x
pj
, and set s
ijp
= 0 otherwise. In other words, the jth
share s
ij
consists of all bits of r
i
except those corresponding
to the records where the value of the jth attribute is the
same. An example can be found in section 4.4.
The result of this construction is that shares corresponding
to commonly occurring attribute values will be missing
many bits of r
i
, while a share corresponding to an attribute
that uniquely identifies one record will contain all bits of
r
i
except one. Intuitively, this guarantees group privacy. If
the retriever can supply query attribute values that uniquely
identify a single record or a small subset of records, he will
learn the shares that reveal all bits of the secret r
i
except
for a few, which he can easily guess. If the retriever cannot
describe precisely what he is looking for and supplies
attribute values that are common in the database, many of
the bits of r
i
will be missing in the shares that he learns,
and guessing all of them will be computationally infeasible.
Shares corresponding to different query attributes may
overlap. For example, suppose that we are obfuscating a
database in which two records have the same value of attribute
X
1
if and only if they have the same value of attribute
X
2
. In this case, for any record in the database, the
share revealed if the retriever supplies the correct value of
X
1
will be exactly the same as the share revealed if the retriever
supplies the value of X
2
. The retriver gains nothing
by supplying X
2
in conjunction with X
1
because this does
not help him narrow the set of records that match his query.
To construct the obfuscated database, we encrypt each
share with a pseudo-random key derived from the value of
the corresponding query attribute, and encrypt the data attribute
with a key derived from the secret r
i
. More precisely,
we replace each record
i
= x
i
1
, . . . , x
im
, 
y
i
of the original
database with the obfuscated record
v
i
1
, w
i
1
, v
i
2
, w
i
2
, . . . , v
im
, w
im
, u
i
, z
i
where
- v
ij
= H
1,i,j
(x
ij
). This enables the retriever to verify that
he supplied the correct value for the jth query attribute.
- w
ij
= prg
1,i,j
(H
2,i,j
(x
ij
))  s
ij
. This is the jth share of
the secret r
i
, encrypted with a key derived from the value
of the jth query attribute.
- u
i
= H
3,i
(r
i
). This enables the retriever to verify that he
computed the correct secret r
i
.
- z
i
= prg
2,i
(H
4,i
(r
i
))  
y
i
. This is the data attribute 
y
i
,
encrypted with a key derived from the secret r
i
.
Clearly, algorithm OB
gp
runs in time polynomial in N
(the size of the database). The size of the resulting obfuscation
is N
2
m
. Even though it is within a polynomial factor of
N
(and thus satisfies the definition of [2]), quadratic blowup
means that our technique is impractical for large databases.
This issue is discussed further in section 4.5.
We claim that OB
gp
produces a secure obfuscation of D,
i.e., it is not feasible to extract any more information from
OB
gp
(D) than permitted by the privacy policy GP
D
.
Theorem 2. (Group-exponential obfuscation is
"virtual black-box") For any probabilistic polynomial-time
(adversarial) algorithm A, there exists a probabilistic
polynomial-time simulator algorithm S and a negligible function
 of the security parameter k s.t. for any database D:
|P(A(OB
gp
(D)) = 1) - P(S
GP
D
(1
|D|
) = 1)|  (k)
Remark.
An improper implementation of the random oracles
in the above construction could violate privacy under
composition of obfuscation, i.e., when more than one
database is obfuscated and published. For instance, if the
hash of some attribute is the same in two databases, then
the adversary learns that the attributes are equal without
having to guess their value. To prevent this, the same hash
function may not be used more than once. One way to
achieve this is to pick H
i
(.) = H(r
i
||.) where r
i

R
{0, 1}
k
,
and publish r
i
along with the obfuscation. This is an example
of the pitfalls inherent in the random oracle model.
4.3
Accessing the obfuscated database
We now explain how the retriever can efficiently evaluate
queries on the obfuscated database. Recall that the privacy
policy restricts the retriever to queries consisting of conjunctions
of equality tests on query attributes, i.e., every query
predicate P has the form X
j
1
= x
j
1
. . .  X
j
t
= x
j
t
, where
j
1
, . . . , j
t
are some indices between 1 and m.
The retriever processes the obfuscated database record by
record. The ith record of the obfuscated database (1  i 
N
) has the form v
i
1
, w
i
1
, v
i
2
, w
i
2
, . . . , v
im
, w
im
, u
i
, z
i
. The
retriever's goal is to compute the N -bit secret r
i
so that he
can decrypt the ciphertext z
i
and recover the value of 
y
i
.
First, the retriever recovers as many shares as he can from
the ith record. Recall from the construction of section 4.2
that each w
ij
is a ciphertext of some share, but the only way
to decrypt it is to supply the corresponding query attribute
value x
ij
. Let  range over the indices of attributes supplied
by the retriever as part of the query, i.e.,   {j
1
, . . . , j
t
}.
For each , if H
1,i,
(x

) = v
i
, then the retriever extracts
the corresponding share s
i
= prg
1,i,
(H
2,i,
(x

))  w
i
. If
H
1,i,
(x

) = v
i
, this means that the retriever supplied the
wrong attribute value, and he learns nothing about the corresponding
share. Let S be the set of recovered shares.
Each recovered share s

S reveals only some bits of r
i
,
and, as mentioned before, bits revealed by different shares
may overlap. For each p s.t. 1  p  N , the retriever sets the
corresponding bit r
ip
of the candidate secret r
i
as follows:
r
ip
=
s
p
if s

S s.t. v
p
= H
1,1,
(x

)
random
otherwise
Informally, if at least one of recovered shares s

contains
the pth bit of r
i
(this can be verified by checking that the
value of th attribute is not the