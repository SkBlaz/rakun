Query Result Ranking over E-commerce Web Databases
ABSTRACT
To deal with the problem of too many results returned from an E-commerce
Web database in response to a user query, this paper 
proposes a novel approach to rank the query results. Based on the 
user query, we speculate how much the user cares about each 
attribute and assign a corresponding weight to it. Then, for each 
tuple in the query result, each attribute value is assigned a score 
according to its "desirableness" to the user. These attribute value 
scores are combined according to the attribute weights to get a final 
ranking score for each tuple. Tuples with the top ranking scores are 
presented to the user first. Our ranking method is domain 
independent and requires no user feedback. Experimental results 
demonstrate that this ranking method can effectively capture a 
user's preferences.
Categories and Subject Descriptors
H.3.5 [INFORMATION STORAGE AND RETRIEVAL]: 
Online Information Services - Web-based services

General Terms
Algorithms, Design, Experimentation, Human Factors.

INTRODUCTION
With the rapid expansion of the World Wide Web, more and more 
Web databases are available. At the same time, the size of existing 
Web databases is growing rapidly. One common problem faced by 
Web users is that there is usually too many query results returned 
for a submitted query. For example, when a user submits a query to 
autos.yahoo.com to search for a used car within 50 miles of New 
York with a price between $5,000 and $10,000, 10,483 records are 
returned. In order to find "the best deal", the user has to go through 
this long list and compare the cars to each other, which is a tedious 
and time-consuming task.
Most Web databases rank their query results in ascending or 
descending order according to a single attribute (e.g., sorted by date, 
sorted by price, etc.). However, many users probably consider
multiple attributes simultaneously when judging the relevance or 
desirableness of a result. While some extensions to SQL allow the 
user to specify attribute weights according to their importance to 
him/her [21], [26], this approach is cumbersome and most likely 
hard to do for most users since they have no clear idea how to set 
appropriate weights for different attributes. Furthermore, the user-setting
-weight approach is not applicable for categorical attributes.
In this paper, we tackle the many-query-result problem for Web 
databases by proposing an automatic ranking method, QRRE 
(Query Result Ranking for E-commerce), which can rank the query 
results from an E-commerce Web database without any user 
feedback. We focus specifically on E-commerce Web databases 
because they comprise a large part of today's online databases. In 
addition, most E-commerce customers are ordinary users who may 
not know how to precisely express their interests by formulating 
database queries. The carDB Web database is used in the following 
examples to illustrate the intuitions on which QRRE is based.
Example 1:  Consider a used Car-selling Web database D with a 
single table carDB in which the car instances are stored as tuples 
with attributes: Make, Model, Year, Price, Mileage and Location. 
Each tuple t
i
in D represents a used car for sale.
Given a tuple t
i
in the query result T
q
for a query q that is submitted
by a buyer, we assign a ranking score to t
i
, based on its attribute
values, which indicates its desirableness, from an E-commerce 
viewpoint, to the buyer. For instance, it is obvious that a luxury, 
new and cheap car is globally popular and desired in the used car 
market. However, sometimes the desired attribute values conflict 
with each other. For example, a new luxury car with low mileage is 
unlikely to be cheap. Hence, we need to decide which attributes are 
more important for a buyer. Some buyer may care more about the 
model of a car, while some other buyer may be more concerned 
about its price. For each attribute, we use a weight to denote its 
importance to the user.
In this work, we assume that the attributes about which a user cares 
most are present in the query he/she submits, from which the 
attribute importance can be inferred. We define specified attributes 
to be attributes that are specified in a query and unspecified 
attributes to be attributes that are not specified in a query. 
Furthermore, we also consider that a subset of the unspecified 
attributes, namely, those attributes that are closely correlated to the 
query, is also important.
Example 2: Given a query with condition "Year &gt; 2005", the query 
condition suggests that the user wants a relatively new car. 
Intuitively, besides the Year attribute, the user is more concerned 
about the Mileage than he/she is concerned about the Make and 
Location, considering that a relatively new car usually has low 
mileage.
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. 
CIKM'06, November 5­11, 2006, Arlington, Virginia, USA. 
Copyright 2006 ACM 1-59593-433-2/06/0011...$5.00.

575
Given an unspecified attribute A
i
, the correlation between A
i
and the
user query q is evaluated by the difference between the distribution 
of A
i
's values over the query results and their distribution over the
whole database D. The bigger the difference, the more A
i
correlates
to the specified attribute value(s). Consequently, we assign a bigger 
attribute weight to A
i
. Example 3 explains our intuition.
Example 3: Suppose a used car database D contains car instances 
for which the Year has values 1975 and onwards and D returns a 
subset d containing the tuples that satisfy the query with condition 
"Year &gt; 2005". Intuitively, Mileage values of the tuples in d 
distribute in a small and dense range with a relatively low average, 
while the Mileage values of tuples in D distribute in a large range 
with a relatively high average. The distribution difference shows a 
close correlation between the unspecified attribute, namely, 
Mileage, and the query "Year &gt; 2005".
Besides the attribute weight, we also assign a preference score to 
each attribute value, including the values of both specified and 
unspecified attributes. In the E-commerce context, we first assume 
that an expensive product is less preferred than a cheap product if 
other attribute values are equal. Hence, we assign a small preference 
score for a high Price value and a large preference score for a low 
Price value. We further assume that a non-Price attribute value with 
high desirableness, such as a luxury car or a new car, correlates 
positively with a high Price value. Thus, a luxury car is more 
expensive than a standard car and a new car is usually more 
expensive than an old car. Based on this assumption, we convert a 
value a
i
of a non-Price attribute A
i
to a Price value p'
I
where p'
I
is
the average price of the product for A
i
= a
i
in the database.
Consequently, the preference score for a
i
will be large if p'
I
is large
because a large Price value denotes a high desirableness for the user. 
Finally, the attribute weight and the value preference score are 
combined to get the final ranking score for each tuple. The tuples 
with the largest ranking scores are presented to the user first. 
The contributions of this paper include the following: 
1.  We present a novel approach to rank the tuples in the query
results returned by E-commerce Web databases.
2.  We propose a new attribute importance learning approach,
which is domain independent and query adaptive.
3.  We also propose a new attribute-value preference score
assignment approach for E-commerce Web databases.
In the entire ranking process, no user feedback is required. 
The rest of the paper is organized as follows. Section 2 reviews 
some related work. Section 3 gives a formal definition of the many-query
-result problem and presents an overview of QRRE. Section 4 
proposes our attribute importance learning approach while Section 5 
presents our attribute preference score assignment approach. 
Experimental results are reported in Section 6. The paper is 
concluded in Section 7.
RELATED WORK
Query result ranking has been investigated in information retrieval 
for a long time. Cosine Similarity with TF-IDF weighting of the 
vector space model [2] and [26], the probabilistic ranking model 
[30] and [31] and the statistical language model [12] have been 
successfully used for ranking purposes. In addition, [10], [11], [14]
and [15] explore the integration of database and information 
retrieval techniques to rank tuples with text attributes. [1], [5] and 
[17] propose some keyword-query based retrieval techniques for 
databases. However, most of these techniques focus on text 
attributes and it is very difficult to apply these techniques to rank 
tuples with categorical or numerical attributes.
Some recent research addresses the problem of relational query 
result ranking. In [9], [26], [28] and [33], user relevance feedback is 
employed to learn the similarity between a result record and the 
query, which is used to rank the query results in relational 
multimedia databases. In [21] and [26], the SQL query language is 
extended to allow the user to specify the ranking function according 
to their preference for the attributes. In [18] and [19], users are 
required to build profiles so that the query result is ranked according 
to their profile. Compared with the above work, our approach is 
fully automatic and does not require user feedback or other human 
involvement.
In [1] and [12], two ranking methods have been proposed that take 
advantage of the links (i.e., associations) among records, such as the 
citation information between papers. Unfortunately, linking 
information among records does not exist for most domains.
The work that is most similar to ours is the probabilistic information 
retrieval (PIR) model in [8], which addresses the many-query-result 
problem in a probabilistic framework. In PIR, the ranking score is 
composed of two factors: global score, which captures the global 
importance of unspecified values, and conditional score, which 
captures the strength of the dependencies between specified and 
unspecified attribute values. The two scores are combined using a 
probabilistic approach. Our approach differs from that in [8] in the 
following aspects: 
1.  PIR only focuses on point queries, such as "A
i
= a
i
". Hence, both
a query with condition "Mileage &lt; 5000" and a query with 
condition "Mileage &lt; 2500" may have to be converted to a 
query with condition "Mileage = small" to be a valid query in 
PIR, which is not reasonable for many cases. In contrast, QRRE 
can handle both point and range queries.
2.  PIR focuses on the unspecified attributes during query result
ranking while QRRE deals with both specified and unspecified 
attributes. For example, suppose a car with price less than 
$10,000 is labeled as a "cheap" car. For a query "Price &lt; 10000", 
PIR will only consider the value difference for non-Price 
attributes among tuples and ignore the price difference, which is 
usually important for a poor buyer. On the contrary, QRRE will 
consider the value difference for all attributes.
3.  A workload containing past user queries is required by PIR in
order to learn the dependency between the specified and 
unspecified attribute values, which is unavailable for new online 
databases, while QRRE does not require such a workload.
The experimental results in Section 6 show that QRRE produces a 
better quality ranking than does PIR.
The attribute-importance learning problem was studied in [23] and 
[24], in which attribute importance is learned according to the 
attribute dependencies. In [23], a Bayesian network is built to 
discover the dependencies among attributes. The root attribute is the 
most important while the leaf attributes are less important.
576
In [24], an attribute dependency graph is built to discover the 
attribute dependencies. Both of these methods learn the attribute 
importance based on some pre-extracted data and their result is 
invariant to the user queries. Furthermore, both methods can only 
determine the attribute importance sequence. They are incapable of 
giving a specific value to show how important each attribute is. In 
contrast, the attribute-importance learning method presented in this 
paper can be adapted to the user's query and thus can be tailored to 
take into account the desire of different users, since each attribute is 
assigned a weight that denotes its importance for the user. To our 
knowledge, this is the first work that generates attribute weights that 
are adaptive to the query the user submitted.

QUERY RESULT RANKING
In this section, we first define the many-query-result problem and 
then present an overview of QRRE.
3.1  Problem Formulation
Consider an autonomous Web database D with attributes A={A
1
, A
2
,
..., A
m
} and a selection query q over D with a conjunctive selection
condition that may include point queries, such as "A
i
= a
i
", or range
queries, such as  "a
i1
&lt; A
i
&lt; a
i2
". Let T={t
1
, t
2
, ..., t
n
} be the set of
result tuples returned by D for the query q. In many cases, if q is not 
a selective query, it will produce a large number of query results 
(i.e., a large T). The goal is to develop a ranking function to rank the 
tuples in T that captures the user's preference, is domain-independent
and does not require any user feedback.
3.2  QRRE
Initially, we focus on E-commerce Web databases because E-commerce
Web databases comprise a large proportion of the 
databases on the Web. We further assume that each E-commerce 
Web database has a Price attribute, which we always assume to be 
A
1
. The Price attribute A
1
plays an intermediate role for all attributes
during the attribute preference score assignment.
Example 4:  Consider the tuples in Table 1 that represent an 
example query result set T. It can be seen that most tuples have their 
own advantages when compared with other tuples. For example, t
1

is a relatively new car while t
2
is a luxury car and t
3
is the cheapest
among all cars. Hence, depending on a user's preferences, different 
rankings may be needed for different users. Assuming that a user 
would prefer to pay the smallest amount for a car and that all other 
attribute values are equal, then the only certainty is that t
4
should
always be ranked after t
3
because its mileage is higher than t
3
while
it is more expensive than t
3
.
Table 1. Examples of used car tuples.
Year  Make  Model Mileage  Price  Location
t
1

2005 Toyota Corolla 16995  26700  Seattle
t
2

2002  Mercedes-Benz
G500 47900  39825  Seattle
t
3

2002 Nissan  350Z  26850  17448  Seattle
t
4

2002 Nissan  350Z  26985  18128  Seattle
According to Example 4, two problems need to be solved when we 
assign a ranking score for a tuple t
i
={t
i1
,  t
i2
, ...,  t
im
} in the query
result T: 
1.  How can we surmise how much a user cares about an attribute
A
j
and how should we assign a suitable weight w
j
for the
attribute(s) A
j
to reflect its (their) importance to the user?
2.  How do we assign a preference score v
ij
for an attribute value t
ij
?
For example, when assigning the score for the attribute value "Year 
= 2005" in t
1
, should the score be larger than the score assigned for
attribute value "Year = 2002" in t
2
and how much larger is
reasonable? The first problem will be discussed in Section 4. The 
second problem will be discussed in Section 5.
Having assigned a preference score v
ij
(1jm) to each attribute-value
of t
i
and a weight w
j
to the attribute A
j
, the value preference
scores v
ij
are summed to obtain the ranking score s
i
for t
i
to reflect
the attribute importance for the user. That is:

=
=
m
j
ij
j
i
v
w
s
1

The overall architecture of a system employing QRRE is shown in 
Figure 1. Such a system includes two components: pre-processing 
component and online processing component. The pre-processing 
component collects statistics about the Web database D using a set

Figure 1: Architecture of a system employing Query Result Ranking for E-commerce (QRRE).
577
of selected queries. Two kinds of histograms are built in the preprocessing
step: single-attribute histograms and bi-attribute 
histograms. A single-attribute histogram is built for each attribute A
j
.
A bi-attribute histogram is built for each non-Price attribute (i.e., A
j

in which i&gt;1) using the Price attribute A
1
.
The online-processing component ranks the query results given the 
user query q. After getting the query results T from the Web 
database  D for q, a weight is assigned for each attribute by 
comparing its data distribution in D and in the query results T. At 
the same time, the preference score for each attribute value in the 
query result is determined using the information from the bi-attribute
histograms. The attribute weights and preference scores are 
combined to calculate the ranking score for each tuple in the query 
result. The tuples' ranking scores are sorted and the top K tuples 
with the largest ranking scores are presented to the user first.
ATTRIBUTE WEIGHT ASSIGNMENT
In the real world, different users have different preferences. Some 
people prefer luxury cars while some people care more about price 
than anything else. Hence, we need to surmise the user's preference 
when we make recommendations to the user as shown by Example 
4 in Section 3. The difficulty of this problem lies in trying to 
determine what a user`s preference is (i.e., which attributes are more 
important) when no user feedback is provided. To address this 
problem, we start from the query the user submitted. We assume 
that the user's preference is reflected in the submitted query and, 
hence, we use the query as a hint for assigning weights to attributes. 
The following example provides the intuition for our attribute 
weight assignment approach.
Example 5:  Consider the query q with condition "Year &gt; 2005", 
which denotes that the user prefers a relatively new car. It is 
obvious that the specified attribute Year is important for the user.  
However, all the tuples in the query result T satisfy the query 
condition. Hence, we need to look beyond the specified attribute and 
speculate further about what the user's preferences may be from the 
specified attribute. Since the user is interested in cars that are made 
after 2005, we may speculate that the user cares about the Mileage 
of the car. Considering the distribution of Mileage values in the 
database, cars whose model year is greater than 2005 usually have 
a lower mileage when compared to all other cars. In contrast, 
attribute Location is less important for the user and its distribution 
in cars whose model year is greater than 2005 may be similar to the 
distribution in the entire database.
According to this intuition, an attribute A
j
that correlates closely
with the query will be assigned a large weight and vice verse. 
Furthermore, as Example 3 in Section 1 shows, the correlation of A
j

and the query can be measured by the data distribution difference of 
A
j
in D and in T.
It should be noted that the specified attribute is not always important, 
especially when the condition for the specified attribute is not 
selective. For example, for a query with condition "Year &gt; 1995 and 
Make = BMW", the specified attribute Year is not important 
because almost all tuples in the database satisfy the condition 
"Year &gt; 1995" and the Year distribution in D and in T is similar.
A natural measure of the distribution difference of A
j
in D and in T
is the Kullback-Leibler distance or Kullback-Leibler (KL) 
divergence [13]. Suppose that A
j
is a categorical attribute with value
set {a
j1
, a
j2
, ..., a
jk
}. Then the KL-divergence of A
j
from D to T is:

=
=
=
=
=
k
l
jl
j
jl
j
jl
j
KL
T
a
A
prob
D
a
A
prob
D
a
A
prob
T
D
D
1
)
|
(
)
|
(
log
)
|
(
)
||
(
(1)
in which prob(A
j
=a
jl
| D) refers to the probability that A
j
= a
jl
in D
and prob(A
j
=a
jl
| T) refers to the probability that A
j
= a
jl
in T. If A
j
is
a numerical attribute, its value range is first discretized into a few 
value sets, where each set refers to a category, and then the KL-divergence
of A
j
is calculated as in (1).
4.1  Histogram Construction
To calculate the KL-divergence in equation (1) we need to obtain 
the distribution of attribute values over D. The difficulty here is that 
we are dealing with an autonomous database and we do not have 
full access to all the data. In [24], the attribute value distribution 
over a collection of data crawled from D is used to estimate the 
actual attribute value distribution over D. However, it is highly 
likely that the distribution of the crawled data can be different from 
that of D because the crawled data may be biased to the submitted 
queries. 
In this paper, we propose a probing-and-count based method to 
build a histogram for an attribute over a Web database
1
. We assume
that the number of query results is available in D for a given query. 
After submitting a set of selected queries to D, we can extract the 
number of query results, instead of the actual query results, to get 
the attribute value distribution of A
i
. An equi-depth histogram [27] is
used to represent the attribute value distribution, from which we will 
get the probability required in Equation (1). The key problem in our 
histogram construction for A
i
is how to generate a set of suitable
queries to probe D.
Figure 2 shows the algorithm for building a histogram for attribute 
A
i
. For each attribute  A
i
, a histogram is built in the preprocessing
stage. We assume that one attribute value of  A
i
is enough to be a
query for D. If A
i
is a categorical attribute, each category of A
i
is
used as a query to get its occurrence count (Lines 2-3). If A
i
is a
numerical attribute, an equal-depth histogram is built for A
i
. We first
decide the occurrence frequency

threshold  t for each bucket by
dividing |D|, namely, the number of tuples in D, with the minimum 
bucket number n that will be created for a numerical attribute A
i
. In
our experiments, n is empirically set to be 20. Then we probe D 
using a query with condition on A
i
such that low A
i
&lt;up and get c,
the number of instances in that range (Line 8). If c is smaller than t, 
a bucket is added for it in H
Di
(Line 10) and another query probe is
prepared (Line 11). Otherwise, we update the query probe condition 
on  A
i
by reducing the size of the bucket (Line 13) and a new
iteration begins. The iteration continues until each value in the value 
range is in a bucket. It is obvious that there are some improvements 
that can be made to the algorithm to accelerate the histogram 
construction. The improvements are not described here because 
histogram construction is not the major focus of this paper. 
Considering that only a single-attribute histogram is constructed, the 
process should complete quickly.

1
Although both our histogram construction method and the
histogram construction methods in [1] and [5] are probing-based
, they have different goals. The goal in [1] and [5] is to 
build a histogram that precisely describes the regions on which 
the queries concentrate, while our purpose is to build a 
histogram that summarizes the data distribution of D as 
precisely as possible with a number of query probes.
578
A histogram H
Ti
also needs to be built for A
i
over T (the result set) to
get its probability distribution over T. For each bucket of H
Di
, a
bucket with the same bucket boundary is built in H
Ti
while its
frequency is counted in T.
4.2  Attribute Weight Setting
After getting the histogram of A
i
over D and T, the histograms are
converted to a probability distribution by dividing the frequency in 
each bucket of the histogram by the bucket frequency sum of the 
histogram. That is, the probability distribution of A
i
for D, P
Di
, is
|
|
|
|
D
c
p
Dk
Di
=

in which c
Dk
is the frequency of the k
th
bucket in H
Di
. The
probability distribution of A
i
for T, P
Ti
, is
|
|
|
|
T
c
p
Tk
Ti
=

in which c
Tk
is the frequency of the k
th
bucket in H
Ti
.
Next, for the i
th
attribute A
i
, we assign its importance w
i
as

=
=
m
j
Tj
Dj
Ti
Di
i
P
P
KL
P
P
KL
w
1
)
,
(
)
,
(

The attribute weight assignment is performed not only on the 
unspecified attributes, but also on the specified attributes. If a 
specified attribute is a point condition, its attribute weight will be 
the same for all tuples in the query result. If a specified attribute is a 
range condition, its attribute weight will be different for the tuples in 
the query result. Example 6 illustrates this point.
Example 6: Consider a query q with condition "Make = 2004 and 
Price &lt; 10000". In q, since the specified attribute Make is a point 
attribute, the attribute weight assigned to it is useless because all 
the query results have the same value for Make. On the other hand, 
since the attribute Price is a range attribute, the price of different 
tuples is an important factor to consider during ranking.
4.3  Examples of Attribute Weight Assignment
In our experiments, we found that the attribute weight assignment 
was intuitive and reasonable for the given queries. Table 2 shows 
the attribute weight assigned to different attributes corresponding to 
different queries in our experiments for the carDB. Given a query 
with condition "Mileage &lt; 20000", which means that the user 
prefers a new car, as expected the attribute "Mileage" is assigned a 
large weight because it is a specified attribute and the attribute 
"Year" is assigned a large weight too. The attribute "Model" is 
assigned a large weight because a new car usually has a model that 
appears recently. In contrast, Consider the query with condition 
"Make = BMW & Mileage &lt; 100000".
The
sub-condition
"Mileage &lt; 100000" possesses a very weak selective capability 
because almost all tuples in the database satisfy it. The buyer is 
actually just concerned about the Make and the Model of the car. As 
expected, the attribute Make and Model are assigned large weights, 
while Year and Mileage are no longer assigned large weights.

Table 2: Attribute weight assignments for two queries.

Mileage &lt; 20000
Make = BMW & Mileage &lt; 100000
Year 0.222
0.015
Make 0.017
0.408
Model 0.181
0.408
Price 0.045
0.120
Mileage 0.533
0.04
Location 0.0003
0.002
ATTRIBUTE PREFERENCE SCORE ASSIGNMENT
In addition to the attributes themselves, different values of an 
attribute may have different attractions for the buyer. For example, a 
car with a low price is obviously more attractive than a more 
expensive one if other attribute values are the same. Similarly, a car 
with low mileage is also obviously more desirable. Given an 
attribute value, the goal of the attribute preference score assignment 
module is to assign a preference score to it that reflects its 
desirableness for the buyer. To facilitate the combination of scores 
of different attribute values, all scores assigned for different attribute 
values are in [0, 1].
Instead of requiring human involvement for attribute value 
assignment, given a normal E-commerce context, we make the 
following two intuitive assumptions: 
1.  Price assumption: A product with a lower price is always more
desired by buyers than a product with a higher price if the other 
attributes of the two products have the same values. For 
example, if all other attribute values are the same, a cheaper car 
is preferred over a more expensive car.
2.  Non-Price assumption: A non-Price attribute value with higher
desirableness for the user corresponds to a higher price. For 
example, a new car, which most buyers prefer, is usually more
Input: Attribute A
i
and its value range
Web database D with the total number of tuples | D |
Minimum bucket number n
Output:  A single-attribute histogram H
Di
for A
i

Method:
1.  If A
i
is a categorical attribute
2.      For  each  category  a
ij
of A
i
, probe D using a query with
condition "A
i
=a
ij
" and get its occurrence count c
3.              Add a bucket (a
ij
, c) into H
D
i

4.  If A
i
is a numerical value attribute with value range [a
low
, a
up
)
5.           t = |D| / n 
6.          low = a
low
, up = a
up

7.          Do      
8.               probe D with a query with condition  "low A
i
&lt;up"
and get its occurrence count c
9.
if c  t
10.
Add a bucket (low, up, c) into H
D
i

11.
low = up, up = a
up

12.               else  
13.                  up = low + (up - low) / 2  
14.         While low &lt;  a
up

15.
Return H
D
i

Figure 2: Probing-based histogram construction algorithm.
579
expensive than an old car. Likewise, a luxury car is usually 
more expensive than an ordinary car.
With the above two assumptions, we divide the attributes into two 
sets: Price attribute set, which only includes the attribute Price, and 
non-Price attribute set, which includes all attributes except Price. 
The two sets of attributes are handled in different ways.
According to the Price assumption, we assign a large score for a low 
price and a small score for a high price. To avoid requiring human 
involvement to assign a suitable score for a Price value, the Price 
distribution in D is used to assign the scores. Given a Price value t, a 
score  v
t
is assigned to it as the percentage of tuples whose Price
value is bigger than t
i
in D:
|
| D
S
v
t
t
=

in which S
t
denotes the number of tuples whose Price value is bigger
than t. In our experiments, the histogram for the attribute Price A
1
,
whose construction method is described in Section 4, is used for the 
Price preference score assignment.
Figure 3 shows the algorithm used to assign a score v for a Price 
value  t using the Price histogram. Given the Price histogram H
D1
,
the frequency sum is fist calculated (Line 1). Then we count the 
number  S
t
of tuples whose Price value is bigger than t. For each
bucket in H
D1
, if the lower boundary of the bucket is bigger than t, it
means that all the tuples for this bucket have a Price value bigger 
than t and the frequency of this bucket is added to S
t
(Line 4). If t is
within the boundary of the bucket, we assume that the Price has a 
uniform distribution in the bucket and a fraction of the frequency in 
this bucket is added to S
t
(Line 5). If the upper boundary of the
bucket is smaller than t, it means that all the tuples for this bucket 
have a Price value lower than t and the bucket is ignored. Finally the 
ratio is generated by dividing S
t
with the frequency sum.
For a value a
i
of a non-Price attribute A
i
, the difficulty of assigning
it a score is two fold:
1.  How to make the attribute preference score assignment adaptive
for different attributes? Our goal is to have an intuitive 
assignment for each attribute without human involvement. The 
difficulty is that different attributes can have totally different 
attribute values.
2.  How to establish the correspondence between different
attributes? For example, how can we know that the
desirableness of "Year = 2005" is the same as the desirableness 
of "Mileage = 5000" for most users?
We solve the problem in two steps. First, based on the non-Price 
assumption, we can convert a non-Price value a
i
to a Price attribute
value t
i
:
·  If A
i
is a categorical attribute, t
i
is the average price for all tuples
in D such that A
i
=a
i
.
·  If A
i
is a numerical attribute, v
i
is the average price for all tuples
in D such that a
i
-d &lt; A
i
&lt; a
i
+d where d is used to prevent too
few tuples or no tuple being collected if we just simply set A
i

=a
i
.
In our experiments, a bi-attribute histogram (A
1
, A
i
) is used when a
i

is converted to a Price value. The bi-attribute histograms are built in 
the pre-processing step in a way similar to the histogram 
construction described in Section 4.
Second, after converting all non-Price attribute values to Price 
values, we use a uniform mechanism to assign them a preference 
score. We assign a large score for a large Price value according to 
the non-Price assumption. That is, given a converted Price value t
i
, a
preference score v
i
is assigned to it as the percentage of Price values
that is smaller than t
i
in D. The algorithm for the converted Price
preference score assignment can be easily adapted from the 
algorithm in Figure 3.
5.1  Examples of Attribute Preference Score