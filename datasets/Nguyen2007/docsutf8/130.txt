Low Latency Photon Mapping Using Block Hashing
Abstract
For hardware accelerated rendering, photon mapping is especially useful for simulating caustic lighting effects on
non-Lambertian surfaces. However, an efficient hardware algorithm for the computation of the k nearest neighbours
to a sample point is required.
Existing algorithms are often based on recursive spatial subdivision techniques, such as kd-trees. However, hardware
implementation of a tree-based algorithm would have a high latency, or would require a large cache to avoid
this latency on average.
We present a neighbourhood-preserving hashing algorithm that is low-latency and has sub-linear access time.
This algorithm is more amenable to fine-scale parallelism than tree-based recursive spatial subdivision, and maps
well onto coherent block-oriented pipelined memory access. These properties make the algorithm suitable for
implementation using future programmable fragment shaders with only one stage of dependent texturing.
Categories and Subject Descriptors
(according to ACM CCS)
: I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism, Color, Shading, Shadowing, and Texture.

Introduction
Photon mapping, as described by Jensen
25
, is a technique
for reconstructing the incoming light field at surfaces everywhere
in a scene from sparse samples generated by forward
light path tracing. In conjunction with path tracing, photon
mapping can be used to accelerate the computation of both
diffuse and specular global illumination . It is most effective
for specular or glossy reflectance effects, such as caustics
24
.
The benefits of migrating photo-realistic rendering techniques
towards a real-time, hardware-assisted implementation
are obvious. Recent work has shown that it is possible
to implement complex algorithms, such as ray-tracing,
using the programmable features of general-purpose hardware
accelerators
35
and/or specialised hardware
41
. We are
interested in hardware support for photon mapping: specifically
, the application of photon maps to the direct visualisation
of caustics on non-Lambertian surfaces, since diffuse
global illumination effects are probably best handled in a
real-time renderer using alternative techniques such as irradiance
volumes
18
.
Central to photon mapping is the search for the set of photons
nearest to the point being shaded. This is part of the interpolation
step that joins light paths propagated from light
sources with rays traced from the camera during rendering,
and it is but one application of the well-studied k nearest
neighbours (kNN) problem.
Jensen uses the kd-tree
5
,
6
data structure to find these nearest
photons. However, solving the kNN problem via kd-trees
requires a search that traverses the tree. Even if the tree is
stored as a heap, traversal still requires random-order memory
access and memory to store a stack. More importantly,
a search-path pruning algorithm, based on the data already
examined, is required to avoid accessing all data in the tree.
This introduces serial dependencies between one memory
lookup and the next. Consequently, a hardware implementation
of a kd-tree-based kNN solution would either have high
latency, or would require a large cache to avoid such latency.
In either case a custom hardware implementation would be
required. These properties motivated us to look at alternatives
to tree search.
Since photon mapping is already making an approximation
by using kNN interpolation, we conjectured that an
approximate kNN (AkNN) solution should suffice so long
as visual quality is maintained. In this paper we investigate
a hashing-based AkNN solution in the context of high-c
The Eurographics Association 2002.
89
Ma and McCool / Low Latency Photon Mapping Using Block Hashing
performance hardware-based (specifically, programmable
shader-based) photon mapping. Our major contribution is
an AkNN algorithm that has bounded query time, bounded
memory usage, and high potential for fine-scale parallelism.
Moreover, our algorithm results in coherent, non-redundant
accesses to block-oriented memory. The results of one memory
lookup do not affect subsequent memory lookups, so accesses
can take place in parallel within a pipelined memory
system. Our algorithm is based on array access, and is
more compatible with current texture-mapping capabilities
than tree-based algorithms. Furthermore, any photon mapping
acceleration technique that continues to rely on a form
of kNN (such as irradiance caching
7
) can still benefit from
our technique.
In Section
2
, we first review previous work on the kNN
and the approximate k-nearest neighbour (AkNN) problems.
Section
3
describes the context and assumptions of our research
and illustrates the basic hashing technique used in
our algorithm. Sections
4
and
5
describe the details of our
algorithm. Section
6
presents numerical, visual quality, and
performance results. Section
7
discusses the mapping of the
algorithm onto a shader-based implementation. Finally, we
conclude in Section
8
.
Previous Work
Jensen's book
25
covers essentially all relevant previous work
leading up to photon mapping. Due to space limitations, we
will refer the reader to that book and focus our literature review
on previous approaches to the kNN and AkNN problems
.
Any non-trivial algorithm that claims to be able to solve
the kNN problem faster than brute-force does so by reducing
the number of candidates that have to be examined when
computing the solution set. Algorithms fall into the following
categories: recursive spatial subdivision, point location,
neighbourhood graphs, and hashing.
Amongst algorithms based on recursive spatial subdivision
, the kd-tree
5
method is the approach commonly used
to solve the kNN problem
14
. An advantage of the kd-tree is
that if the tree is balanced it can be stored as a heap in a
single array. While it has been shown that kd-trees have optimal
expected-time complexity
6
, in the worst case finding
the k nearest neighbours may require an exhaustive search of
the entire data structure via recursive decent. This requires a
stack the same size as the depth of the tree. During the recursion
, a choice is made of which subtree to search next based
on a test at each internal node. This introduces a dependency
between one memory access and the next and makes it hard
to map this algorithm into high-latency pipelined memory
accesses.
Much work has been done to find methods to optimise
the kd-tree method of solving the kNN and AkNN problems.
See Christensen
26
, Vanco et al.
44
, Havran
19
, and Sample
et al.
39
. Many other recursive subdivision-based techniques
have also been proposed for the kNN and AkNN problems,
including kd-B-trees
36
, BBD-trees
4
, BAR-trees
9
, Principal-Axis
Trees
33
, the R-tree family of data structures
27
, and
ANN-trees
30
. Unfortunately, all schemes based on recursive
search over a tree share the same memory dependency problem
as the kd-tree.
The second category of techniques are based on building
and searching graphs that encode sample-adjacency information
. The randomised neighbourhood graph approach
3
builds and searches an approximate local neighbourhood
graph. Eppstein et al.
11
investigated the fundamental properties
of a nearest neighbour graph. Jaromczyk and Toussaint
surveyed data structures and techniques based on Relative
Neighbourhood Graphs
23
. Graph-based techniques tend to
have the same difficulties as tree-based approaches: searching
a graph also involves stacks or queues, dependent memory
accesses, and pointer-chasing unsuited to high-latency
pipelined memory access.
Voronoi diagrams can be used for optimal 1-nearest
neighbour searches in 2D and 3D
10
. This and other point-location
based techniques
17
for solving nearest neighbour
problems do not need to calculate distances between the
query point and the candidates, but do need another data
structure (like a BSP tree) to test a query point for inclusion
in a region.
Hashing approaches to the kNN and AkNN problems
have recently been proposed by Indyk et al.
20
,
21
and Gionis
et al.
16
. These techniques have the useful property that
multi-level dependent memory lookups are not required. The
heart of these algorithms are simple hash functions that preserve
spatial locality, such as the one proposed by Linial
and Sasson
31
, and Gionis et al.
16
. We base our technique on
the latter. The authors also recognise recent work by Wald
et al.
45
on real-time global illumination techniques where
a hashing-based photon mapping technique was apparently
used (but not described in detail).
Numerous surveys and books
1
,
2
,
15
,
42
,
43
,
17
provide further
information on this family of problems and data structures
developed to solve them.
Context
We have developed a novel technique called Block Hashing
(BH) to solve the approximate kNN (AkNN) problem in the
context of, but not limited to, photon mapping.
Our algorithm uses hash functions to categorise photons
by their positions. Then, a kNN query proceeds by deciding
which hash bucket is matched to the query point and retrieving
the photons contained inside the hash bucket for analysis
. One attraction of the hashing approach is that evaluation
of hash functions takes constant time. In addition, once we
have the hash value, accessing data we want in the hash table
takes only a single access. These advantages permit us to
c The Eurographics Association 2002.
90
Ma and McCool / Low Latency Photon Mapping Using Block Hashing
avoid operations that are serially dependent on one another,
such as those required by kd-trees, and are major stepping
stones towards a low-latency shader-based implementation.
Our technique is designed under two assumptions on the
behaviour of memory systems in (future) accelerators. First,
we assume that memory is allocated in fixed-sized blocks .
Second, we assume that access to memory is via burst transfer
of blocks that are then cached. Under this assumption,
if any part of a fixed-sized memory block is "touched", access
to the rest of this block will be virtually zero-cost. This
is typical even of software implementations on modern machines
which rely heavily on caching and burst-mode transfers
from SDRAM or RDRAM. In a hardware implementation
with a greater disparity between processing power and
memory speed, using fast block transfers and caching is even
more important. Due to these benefits, in BH all memory
used to store photon data is broken into fixed-sized blocks.
3.1. Locality-Sensitive Hashing
Since our goal is to solve the kNN problem as efficiently as
possible in a block-oriented cache-based context, our hashing
technique requires hash functions that preserve spatial
neighbourhoods. These hash functions take points that are
close to each other in the domain space and hash them close
to each other in hash space. By using such hash functions,
photons within the same hash bucket as a query point can be
assumed to be close to the query point in the original domain
space. Consequently, these photons are good candidates for
the kNN search. More than one such scheme is available; we
chose to base our technique on the Locality-Sensitive Hashing
(LSH) algorithm proposed by Gionis et al.
16
, but have
added several refinements (which we describe in Section
4
).
The hash function in LSH groups one-dimensional real
numbers in hash space by their spatial location. It does so
by partitioning the domain space and assigning a unique
hash value to each partition. Mathematically, let
T =
{t
i
| 0  i  P} be a monotonically increasing sequence
of P + 1 thresholds between 0 and 1. Assume t
0
= 0 and
t
P
= 1, so there are P
- 1 degrees of freedom in this sequence
. Define a one-dimensional Locality-Sensitive Hash
Function h
T
: [0, 1]
{0 . . . P - 1} to be h
T
(t) = i, where
t
i
t &lt; t
i+1
. In other words, the hash value i can take
on P different values, one for each "bucket" defined by the
threshold pair [t
i
,t
i+1
). An example is shown in Figure
1
.
t
1
t
2
t
3
t
4
t
0
Figure 1: An example of h
T
. The circles and boxes represent
values to be hashed, while the vertical lines are the thresholds
in
T . The boxes lie between t
1
and t
2
, thus they are given
a hash value of 1.
The function h
T
can be interpreted as a monotonic non-uniform
quantisation of spatial position, and is characterised
by P and the sequence
T . It is important to note that h
T
gives
each partition of the domain space delineated by
T equal
representation in hash space. Depending on the location of
the thresholds, h
T
will contract some parts of the domain
space and expand other parts. If we rely on only a single
hash table to classify a data set, a query point will only hash
to a single bucket within this table, and the bucket may represent
only a subset of the true neighbourhood we sought.
Therefore, multiple hash tables with different thresholds are
necessary for the retrieval of a more complete neighbourhood
around the query point (See Figure
2
.)
h
T1
h
T3
h
T2
t
1
t
2
t
3
t
4
t
0
t
4
t
0
t
4
t
0
t
1
t
1
t
2
t
2
t
3
t
3
Figure 2: An example of multiple hash functions classifying
a dataset. The long vertical line represents the query
value. The union of results multiple hash tables with different
thresholds represents a more complete neighbourhood.
To deal with n-dimensional points, each hash table will
have one hash function per dimension. Each hash function
generates one hash value per coordinate of the point (See
Figure
3
.) The final hash value is calculated by

n
-1
i=0
h
i
P
i
,
where h
i
are the hash values and P is the number of thresholds
. If P were a power of two, then this amounts to concatenating
the bits. The only reason we use the same number of
thresholds for each dimension is simplicity. It is conceivable
that a different number of thresholds could be used for
each dimension to better adapt to the data. We defer the discussion
of threshold generation and query procedures until
Sections
4.2
and
4.4
, respectively.
h
x
h
y
P
Figure 3: Using two hash functions to handle
a 2D point. Each hash function will be
used to hash one coordinate.
LSH is very similar to grid files
34
. However, the grid file
was specifically designed to handle dynamic data. Here, we
assume that the data is static during the rendering pass. Also,
the grid file is more suitable for range searches than it is for
solving the kNN problem.
3.2. Block-Oriented Memory Model
It has been our philosophy that hardware implementations of
algorithms should treat off-chip memory the same way software
implementations treat disk: as a relatively slow, "out-of
-core", high-latency, block-oriented storage device. This
c The Eurographics Association 2002.
91
Ma and McCool / Low Latency Photon Mapping Using Block Hashing
analogy implies that algorithms and data structures designed
to optimise for disk access are potentially applicable to hardware
design. It also drove us to employ fixed-sized blocks to
store the data involved in the kNN search algorithm, which
are photons in the context of this application.
In our prototype software implementation of BH, each
photon is stored in a structure similar to Jensen's "extended"
photon representation
25
. As shown in Figure
4
, each component
of the 3D photon location is represented by a 32-bit
fixed-point number. The unit vectors representing incoming
direction ^d and surface normal ^n are quantised to 16-bit
values using Jensen's polar representation. Photon power is
stored in four channels using sixteen-bit floating point numbers
. This medium-precision signed representation permits
other AkNN applications beyond that of photon mapping.
Four colour channels are also included to better match the
four-vectors supported in fragment shaders. For the photon
mapping application specifically, our technique is compatible
with the replacement of the four colour channels with a
Ward RGBE colour representation
46
. Likewise, another implementation
might use a different representation for the normal
and incident direction unit vectors.
|
32
|
x
y
z
^
d
^
n
c
1
c
2
c
3
c
4
| 16 |
Figure 4: Representation of a photon record.
The 32-bit values (x, y, z) denote the position of
a photon and are used as keys. Two quantised
16-bit unit vectors ^d, ^n and four 16-bit floating
point values are carried as data.
All photon records are stored in fixed-sized memory
blocks. BH uses a 64 32-bit-word block size, chosen to
permit efficient burst-mode transfers over a wide bus to
transaction-oriented DRAM. Using a 128-bit wide path to
DDR SDRAM, for instance, transfer of this block would
take eight cycles, not counting the overhead of command
cycles to specify the operation and the address. Using next-generation
QDR SDRAM this transfer would take only four
cycles (or eight on a 64-bit bus, etc.)
Our photon representation occupies six 32-bit words.
Since photon records are not permitted to span block boundaries
, ten photons will fit into a 64-word block with four
words left over. Some of this extra space is used in our implementation
to record how many photons are actually stored in
each block. For some variants of the data structures we describe
, this extra space could also be used for flags or pointers
to other blocks. It might be possible or desirable in other
implementations to support more or fewer colour channels,
or channels with greater or lesser precision, in which case
some of our numerical results would change.
Block Hashing
Block Hashing (BH) contains a preprocessing phase and a
query phase. The preprocessing phase consists of three steps.
After photons have been traced into the scene, the algorithm
organises the photons into fixed-sized memory blocks, creates
a set of hash tables, and inserts photon blocks into the
hash tables.
In the second phase, the hash tables will be queried for a
set of candidate photons from which the k nearest photons
will be selected for each point in space to be shaded by the
renderer.
4.1. Organizing Photons into Blocks
Due to the coherence benefits associated with block-oriented
memory access, BH starts by grouping photons and storing
them into fixed-sized memory blocks. However, these benefits
are maximised when the photons within a group are close
together spatially.
We chose to use the Hilbert curve
13
to help group photons
together. The advantage of the Hilbert curve encoding of position
is that points mapped near each other on the Hilbert
curve are guaranteed to be within a certain distance of each
other in the original domain
22
. Points nearby in the original
domain space have a high probability of being nearby on the
curve, although there is a non-zero probability of them being
far apart on the curve. If we sort photons by their Hilbert
curve order before packing them into blocks, then the photons
in each block will have a high probability of being spatially
coherent. Each block then corresponds to an interval of
the Hilbert curve, which in turn covers some compact region
of the domain (see Figure
7
a). Each region of domain space
represented by the blocks is independent, and regions do not
overlap.
BH sorts photons and inserts them into a B
+
-tree
8
using
the Hilbert curve encoding of the position of each photon as
the key. This method of spatially grouping points was first
proposed by Faloutsos and Rong
12
for a different purpose.
Since a B
+
-tree stores photon records only at leaves, with
a compatible construction the leaf nodes of the B
+
-tree can
serve as the photon blocks used in the later stages of BH.
One advantage of using a B
+
-tree for sorting is that insertion
cost is bounded: the tree is always balanced, and in the
worst case we may have to split h nodes in the tree, when
the height of the tree is h. Also, B
+
-trees are optimised for
block-oriented storage, as we have assumed.
The B
+
-tree used by BH has index and leaf nodes that
are between half full to completely full. To minimise the final
number of blocks required to store the photons, the leaf
nodes can be compacted (see Figure
5
.) After the photons
are sorted and compacted, the resulting photon blocks are
ready to be used by BH, and the B
+
-tree index and any leaf
nodes that are made empty by compaction are discarded. If
the complete set of photons is known a priori, the compact
B
+
-tree
37
for static data can be used instead. This data structure
maintains full nodes and avoids the extra compaction
step.
c The Eurographics Association 2002.
92
Ma and McCool / Low Latency Photon Mapping Using Block Hashing
(b)
Index node
Empty cell in leaf node
Occupied cell in leaf node
(a)
Figure 5: Compaction of photon blocks. (a) B
+
-tree after inserting
all photons. Many leaf nodes have empty cells. (b) All
photon records are compacted in the leaf nodes.
Regardless, observe that each photon block contains a
spatially clustered set of photons disjoint from those contained
in other blocks. This is the main result we are after
; any other data structures that can group photons into
spatially-coherent groups, such as grid files
34
, can be used
in place of the B
+
-tree and space-filling curve.
4.2. Creating the Hash Tables
The hash tables used in BH are based on the LSH scheme described
in Section
3.1
. BH generates L tables in total, serving
as parallel and complementary indices to the photon data.
Each table has three hash functions (since photons are classified
by their 3D positions), and each hash function has P + 1
thresholds.
BH employs an adaptive method that generates the thresholds
based on the photon positions. For each dimension, a
histogram of photon positions is built. Then, the histogram is
integrated to obtain a cumulative distribution function (cdf ).
Lastly, stratified samples are taken from the inverse of the cdf
to obtain threshold locations. The resulting thresholds will
be far apart where there are few photons, and close together
where photons are numerous. Ultimately this method attempts
to have a similar number of photons into each bucket.
Hash tables are stored as a one-dimensional array structure
, shown in Figure
6
. The hash key selects a bucket out of
the P
n
available buckets in the hash table. Each bucket refers
up to B blocks of photons, and has space for a validity flag
per reference, and storage for a priority value. We defer the
discussion on the choice of P, L and B until Section
5
.
B
V V V
V
Priority
Figure 6: Hash table bucket
layout
4.3. Inserting Photon Blocks
In BH, references to entire photon blocks, rather than individual
photons, are inserted into the hash tables. One reason
for doing so is to reduce the memory required per bucket.
Another, more important, reason is that when merging results
from multiple hash tables (Section
3.1
), BH needs
to compare only block addresses instead of photons when
weeding out duplicates as each block contains a unique set of
photons. This means fewer comparisons have to be made and
the individual photons are only accessed once per query, during
post-processing of the merged candidate set to find the
k nearest photons. Consequently, the transfer of each photon
block through the memory system happens at most once per
query. All photons accessed in a block are potentially useful
contributions to the candidate set, since photons within a single
block are spatially coherent. Due to our memory model
assumptions, once we have looked at one photon in a block
it should be relatively inexpensive to look at the rest.
(a)
(b)
(c)
Figure 7: Block hashing illustrated. (a) Each block corresponds
to an interval of the Hilbert curve, which in turn covers
some compact region of the domain. Consequently, each
bucket (b) represents all photons (highlighted with squares)
in each block with at least one photon hashed into it (c).
Each bucket in a hash table corresponds to a rectangular
region in a non-uniform grid as shown in Figure
7
b. Each
block is inserted into the hash tables once for each photon
within that block, using the position of these photons to create
the keys. Each bucket of the hash table refers to not only
the photons that have been hashed into that bucket, but also
all the other photons that belong to the same blocks as the
hashed photons (see Figure
7
c.)
Since each photon block is inserted into each hash table
multiple times, using different photons as keys, a block
may be hashed into multiple buckets in the same hash table
. Of course, a block should not be inserted into a bucket
more than once. More importantly, our technique ensures
that each block is inserted into at least one hash table. Orphaned
blocks are very undesirable since the photons within
will never be considered in the subsequent AkNN evaluation
and will cause a constant error overhead. Hence, our technique
does not na√Øvely drop a block that causes a bucket to
overflow.
However, there may be collisions that cause buckets to
overflow, especially when a large bucket load factor is chosen
to give a compact hash table size, or there exists a large
variation in photon density (which, of course, is typical in
this application). Our algorithm uses two techniques to address
this problem. The first technique attempts to insert every
block into every hash table, but in different orders on
different hash tables, such that blocks that appear earlier in
the ordering are not favoured for insertion in all tables. BH
uses a technique similar to disk-striping
38
, illustrated by the
c The Eurographics Association 2002.
93
Ma and McCool / Low Latency Photon Mapping Using Block Hashing
pseudo code in Figure
8
. An example is given in the diagram
in the same figure.
for h from 0 to (number_of_hash_tables-1)
for b from 0 to (number_of_blocks-1)
idx = (h+b) modulo L
insert block[b] into hashtable[idx]
endfor
endfor
0
1
2
0
1
2
1
2
0
1
2
0
1
2
0
1
2
0
1
2
0
1
2
0
3
4
5
3
4
5
3
4
5
Photon Block
Hash Table Bucket
1st iteration
2nd iteration
3rd iteration
Figure 8: Striping insertion strategy
The second technique involves a strategy to deal with
overflow in a bucket. For each photon block, BH keeps the
count of buckets that the block has been hashed into so far.
When a block causes overflow in a bucket, the block in the
bucket that has the maximum count will be bumped if that
count is larger than one, and larger than that of the incoming
block. This way we ensure that all blocks are inserted into
at least one bucket, given adequate hash table sizes, and no
block is hashed into an excessive number of buckets.
4.4. Querying
A query into the BH data structure proceeds by delegating
the query to each of the L hash tables. These parallel accesses
will yield as candidates all photon blocks represented
by buckets that matched the query. The final approximate
nearest neighbour set comes from scanning the unified candidate
set for the nearest neighbours to the query point (see
Figure
9
.) Note that unlike kNN algorithms based on hier-archical
data structures, where candidates for the kNN set
trickle in as the traversal progresses, in BH all candidates are
available once the parallel queries are completed. Therefore,
BH can use algorithms like selection
29
(instead of a priority
queue) when selecting the k nearest photons.
Each query will retrieve one bucket from each of the L
hash tables. If the application can tolerate elevated inaccuracy
in return for increased speed of query (for example, to
pre-visualise a software rendering), it may be worthwhile to
consider using only a subset of the L candidate sets. Block
hashing is equipped with a user-specified accuracy setting:
Let A
IN be an integer multiplier. The block hashing algorithm
will only consider Ak candidate photons in the final
scan to determine the k nearest photons to a query. Obviously
the smaller A is, the fewer photons will be processed
in the final step; as such, query time is significantly reduced,
but with an accuracy cost. Conversely, a higher A will lead
to a more accurate result, but it will take more time. Experimental
results that demonstrate the impact of the choice of
A will be explored in Section
6
.
Query point
Matched point
Data point
(a)
(b)
(c)
Figure 9: Merging the results from multiple hash tables.
(a) the query point retrieves different candidates sets from
different hash tables, (b) the union set of candidates after
merging, and (c) the two closest neighbours selected.
There needs to be a way to select the buckets from which
the Ak candidate photons are obtained. Obviously, we want
to devise a heuristic to pick the "best" candidates. Suppose
every bucket in every hash table has a priority given by

=
|bucket_capacity - #entries - #overflows|
where "#overflows" is the number of insertion attempts after
the bucket became full. The priority can be pre-computed
and stored in each bucket of each hash table during the insertion
phase. The priority of a bucket is smallest when the
bucket is full but never overflowed. Conversely, when the
hash bucket is underutilised or overflow has occurred,

will
be larger. If a bucket is underutilised, it is probably too small
spatially (relative to local sample density). If it has experienced
overflow, it is likely too large spatially, and covers too
many photon block regions.
During each query, BH will sort the L buckets returned
from the hash tables by their priority values, smallest values
of

first. Subsequently, buckets are considered in this order,
one by one, until the required Ak photons are found. In this
way the more "useful" buckets will be considered first.
Choice of Parameter Values
Block Hashing is a scheme that requires several parameters:
B, the bucket capacity; L, the number of hash tables whose
results are merged; and P, the number of thresholds per dimension
. We would like to determine reasonable values for
these parameters as functions of k, the number of nearest
neighbours sought, and N, the number of photons involved.
It is important to realize the implications of these parameters
. The total number of 32-bit pointers to photon blocks
is given by LP
3
B. Along with the number of thresholds 3LP,
this gives the memory overhead required for BH. The upper
bound for this value is 6N, the number of photons multiplied
by the six 32-bit words each photon takes up in our
implementation. If we allow B to be a fixed constant for now,
c The Eurographics Association 2002.
94
Ma and McCool / Low Latency Photon Mapping Using Block Hashing
the constraint LP
3
+ 3LP
N arises from the reasonable assumption
that we do not want to have more references to
blocks than there are photons, or more memory used in the
index than in the data.
Empirically, L = P = ln N has turned out to be a good
choice. The value ln N remains sub-linear as N increases,
and this value gives a satisfactory index memory overhead
ratio: There are a total of B(ln N)
4
block references. Being
four bytes each, the references require 4B(ln N)
4
bytes. With
each hash table, there needs to be 3LP = 3(ln N)
2
thresholds
. Represented by a 4-byte value each, the thresholds take
another 12(ln N)
2
bytes. Next, assuming one photon block
can hold ten photons, N photons requires N/10 blocks; each
block requires 64 words, so the blocks require 25.6N bytes
in total. The total memory required for N photons, each occupying
6 words, is 24N bytes. This gives an overhead ratio
of
(4B(ln N)
4
+ 12(ln N)
2
+ 25.6N
- 24N)/24N.
(1)
The choice of B is also dependent on the value of k specified
by the situation or the user. However, since it is usual
in photon mapping that k is known ahead of time, B can be
set accordingly. B should be set such that the total number of
photons retrieved from the L buckets for each query will be
larger than k. Mathematically speaking, each photon block
in our algorithm has ten photons, hence 10LB
k. In particular
, 10LB &gt; Ak should also be satisfied. Since we choose
L = ln N, rearranging the equation yields: B &gt; Ak/(10 ln N)
For example, assuming A = 16, N = 2000000, k = 50, then
B = 6.
If we substitute B back into Equation
1
, we obtain the final
overhead equation
(4(Ak/10)(ln N)
3
+ 12(ln N)
2
+ 1.6N)/24N.
(2)
Figure
10
plots the number of photons versus the memory
overhead. For the usual range of photon count in a photon
mapping application, we see that the memory overhead,
while relative large for small numbers of photons, becomes
reasonable for larger numbers of photons, and has an asymptote
of about 6%. Of course, if we assumed different block
size (cache line size), these results would vary, but the analysis
is the same.
Memory Overhead
Ratio (%)
5
15
20
25
30
0.50
0.75
1.00
1.25
1.50
1.75
A=16
A=8
A=4
10
0.25
2.00
Number of Photons
Figure 10: Plot of photon count vs. memory overhead in-curred
by BH, assuming k = 50.
Results
For BH to be a useful AkNN algorithm, it must have satisfactory
algorithmic accuracy. Moreover, in the context of
photon m