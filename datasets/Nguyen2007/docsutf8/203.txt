UML-Based Service Robot Software Development: A Case Study
ABSTRACT
The research field of Intelligent Service Robots, which has 
become more and more popular over the last years, covers a wide 
range of applications from climbing machines for cleaning large 
storefronts to robotic assistance for disabled or elderly people. 
When developing service robot software, it is a challenging 
problem to design the robot architecture by carefully considering 
user needs and requirements, implement robot application 
components based on the architecture, and integrate these 
components in a systematic and comprehensive way for 
maintainability and reusability. Furthermore, it becomes more 
difficult to communicate among development teams and with 
others when many engineers from different teams participate in 
developing the service robot.

To solve these problems, we applied
the COMET design method, which uses the industry-standard 
UML notation, to developing the software of an intelligent service 
robot for the elderly, called T-Rot, under development at Center 
for Intelligent Robotics (CIR). In this paper, we discuss our 
experiences with the project in which we successfully addressed 
these problems and developed the autonomous navigation system 
of the robot with the COMET/UML method.

Categories and Subject Descriptors
D.2.2 [Software Engineering]: Design Tools and Techniques ­ 
object-oriented design methods

General Terms
Design

INTRODUCTION
Robots have been used in several new applications. In recent 
years, both academic and commercial research has been focusing
on the development of a new generation of robots in the emerging 
field of service robots. Service robots are individually designed to 
perform tasks in a specific environment for working with or 
assisting humans and must be able to perform services semi- or 
fully automatically [1]. Examples of service robots are those used 
for inspection, maintenance, housekeeping, office automation and 
aiding senior citizens or physically challenged individuals [2]. A 
number of commercialized service robots have recently been 
introduced such as vacuum cleaning robots, home security robots, 
robots for lawn mowing, entertainment robots, and guide robots 
[3, 4].
In this context, Public Service Robot (PSR) systems have been 
developed for indoor service tasks at Korea Institute of Science 
and Technology (KIST) [5, 6]. The PSR is an intelligent service 
robot, which has various capabilities such as navigation, 
manipulation, etc. Up to now, three versions of the PSR systems, 
that is, PSR-1, PSR-2, and a guide robot Jinny have been built.
The worldwide aging population and health care costs of aged 
people are rapidly growing and are set to become a major problem 
in the coming decades. This phenomenon could lead to a huge 
market for service robots assisting with the care and support of 
the disabled and elderly in the future [8]. As a result, a new 
project is under development at Center for Intelligent Robotics 
(CIR) at KIST, i.e. the intelligent service robot for the elderly, 
called T-Rot.
In our service robot applications, it is essential to not only 
consider and develop a well-defined robot software architecture, 
but also to develop and integrate robot application components in 
a systematic and comprehensive manner. There are several 
reasons for this:

First, service robots interact closely with humans in a wide 
range of situations for providing services through robot 
application components such as vision recognition, speech 
recognition, navigation, etc. Thus, a well-defined robot 
control architecture is required for coherently and 
systematically combining these services into an integrated 
system.

Second, in robot systems, there are many-to-many relations 
among software components as well as hardware 
components. For instance, a local map module requires 
range data from a laser scanner, ultrasonic sensors, and 
infrared sensors, as well as prior geometrical descriptions of 
the environment. On the other hand, the laser scanner should 
provide its data to a path planner, localizer, and a local map

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
ICSE'06, May 20-28, 2006, Shanghai, China. 
Copyright 2006 ACM 1-59593-085-X/06/0005...$5.00.

534
building module. These relationships, as well as interactions 
among software or hardware modules, must be carefully 
analyzed and systematically managed from an early stage of 
development in order to understand the big picture.

Third, the functional performance of each software and 
hardware module becomes highly dependent on the 
architecture, as the number of robot platforms increases [6], 
and new services are added, or existing services are removed 
or updated to address changes in user needs.

Fourth, previously developed software modules like maps, 
localization, and path planners can be directly reused for 
new tasks or services by service robot developers. Thus, a 
robot architecture, as well as systematic processes or 
methods, are required to support the implementation of the 
system, to ensure modularity and reusability.
As a consequence, in the previous work [5,6], the Tripodal 
schematic control architecture was proposed to tackle the 
problems. Many related research activities have been done. 
However, it is still a challenging problem to develop the robot 
architecture by carefully taking into account user needs and 
requirements, implement robot application components based on 
the architecture, and integrate these components in a systematic 
and comprehensive way. The reason is that the developers of 
service robots generally tend to be immersed in technology 
specific components, e.g. vision recognizer, localizer and path 
planner, at an early stage of product development without 
carefully considering architecture to integrate those components 
for various services [9]. Moreover, engineers and developers are 
often grouped into separate teams in accordance with the specific 
technologies (e.g., speech processing, vision processing), which 
makes integration of these components more difficult [7, 9]. In 
such a project like T-Rot, particularly, several engineers and 
developers (i.e., approximately, more than 150 engineers) from 
different organizations and teams participate in the 
implementation of the service robot. Each separate team tends to 
address the specific technologies such as object recognition, 
manipulation, and navigation and so on. Engineers who come 
from different teams are concerned with different characteristics 
of the system. Thus, a common medium is required to create 
mutual understanding, form consensus, and communicate with 
each other for successfully constructing the service robot. Without 
such a medium or language, it is difficult to sufficiently 
understand the service robot system and interact between teams to 
integrate components for services.
Within the domain of software engineering, many approaches 
have been suggested for a systematic and complete system 
analysis and design, and for the capture of specifications. The 
object-oriented paradigm [10,11] is a widely-accepted approach 
to not only cover the external and declarative view of a system, 
but also at the same time bridge seamlessly with the internal 
implementation view of a system [13]. Object-oriented concepts 
are crucial in software analysis and design because they focus on 
fundamental issues of adaptation and evolution [14]. Therefore, 
compared with the traditional structured software development 
methods, object-oriented methods are a more modular approach 
for analysis, design, and implementation of complex software 
systems, which leads to more self-contained and hence modifiable 
and maintainable systems. More recently, the Unified Modeling 
Language (UML) [15,16] has captured industry-wide attention for
its role as a general-purpose language for modeling software 
systems, especially for describing object-oriented models. The 
UML notation is useful to specify the requirements, document the 
structure, decompose into objects, and define relationships 
between objects in a software system. Certain notations in the 
UML have particular importance for modeling embedded systems 
[17,18], like robot systems. By adopting the UML notation, 
development teams thus can communicate among themselves and 
with others using a defined standard [14,17,18]. More importantly, 
it is essential for the UML notation to be used with a systematic 
object-oriented analysis and design method in order to be 
effectively applied [14].
As a result, our aim is to develop the intelligent service robot 
based on the systematic software engineering method, especially 
for real-time, embedded and distributed systems with UML. To 
do so, we applied the COMET method, which is a UML based 
method for the development of concurrent applications, 
specifically distributed and real-time applications [14]. By using 
the COMET method, it is possible to reconcile specific 
engineering techniques with the industry-standard UML and 
furthermore to fit such techniques into a fully defined 
development process towards developing the service robot 
systems.
In this paper, we describe our experience of applying the COMET 
/UML method into developing the intelligent service robot for the 
elderly, called T-Rot, developed at CIR. In particular, we focused 
on designing an autonomous navigation system for the service 
robot, which is one of the most challenging issues for the 
development of service robots.
Section 2 describes the hardware configuration and services of the 
T-Rot, and discusses the related work. Section 3 illustrates how to 
apply the COMET method into designing and developing the 
autonomous navigation system for the service robot, and 
discusses the results of experiments. The lessons learned from the 
project are summarized in section 4, and section 5 concludes the 
paper with some words on further work.

BACKGROUD ON T-Rot
Fig. 1. KIST service robots
At KIST, intelligent service robots have been developed in large-scale
indoor environments since 1998. So far, PSR-1 and PSR-2, 
which performs delivery, patrol, and floor cleaning jobs, and a 
guide robot Jinny, which provides services like exhibition guide 
and guidance of the road at a museum, have been built [5,6] (see 
Fig. 1). The service robot T-Rot is the next model of the PSR 
system under development for assisting aged persons. 
Development of T-Rot, in which our role is developing and 
integrating robot software, started in 2003 by mainly CIR with
535
more than 10 groups consisting of more than 150 researchers and 
engineers from academia and industry. This project is based on 
the needs and requirements of elderly people through the studies 
and analysis of the commercial health-care market for providing 
useful services to them. Thus, the aim of this project is to develop 
the intelligent service robot for the elderly by cooperating and 
integrating the results of different research groups. This project 
that is divided into three stages will continue until 2013 and we 
are now in the first stage for developing the service robot 
incrementally to provide various services.
2.2

Hardware of T-Rot
The initial version of T-Rot, as shown in Fig. 2, has three single 
board computer (SBC), that is, mobile Pentium 4 (2.2GHz) and 
1GB SDRAM on each SBC. In terms of software environment, 
Linux Red hat 9.0 and RTAI (Real-Time Application Interface) 
[12] are used as operating system. Fig. 3 shows hardware 
configuration as a whole. As mentioned earlier, development of 
T-Rot is conducted incrementally for various services and thus the 
platform will be extended with manipulators and robot hands later. 
In our project, we developed the robot software based on the 
initial version of the platform. The details of the hardware 
platform are described in Table 1.

Fig. 2. T-Rot robot hardware platform

Fig. 3. T-Rot robot hardware platform configuration
Table 1. T-Rot hardware platform devices
Intel Mobile Pentium 4 (2.2 GHz)
1GB SDRAM
SBC
30GB Hard Disk
16 microphones for speaker localization
1 microphone for speech recognition
Voice
1 speaker for speech generation
Vision
2 stereo vision cameras for recognizing users and object
s (1288 H x 1032 V maximum resolution and 7Hz fram
e rates)
Pan/Tilt for controlling the vision part
2 laser scanners (front and back)
2 IR scanners (front and back)
12 Ultrasonic sensors
Sensor
1 Gyroscope sensor for measuring balance
2 actuators for two drive wheels (right and left)
2 free wheels (the support wheels)
2 Servo Motors (100 [w])
2 encoders (2048 ppr)
Actuator
2 bumpers
1 TFT LCD & Touch (10.4" 1024x768, 26000 colors)
KVM (Keyboard/Mouse)
Interface
Wireless LAN for communications
2.3

Robot Services
Some of the primary services under-developed that the initial 
version for T-Rot provides for the elderly are described as below.

Voice-based Information Services: The robot T-Rot can 
recognize voice commands from a user (i.e., an aged person) 
via microphones equipped with the robot and can synthesize 
voices for services. While a user is watching TV, the user 
can ask some questions about the specific TV program or 
request a task to open an Internet homepage by speaking the 
TV program name.

Sound Localization and Voice Recognition: A user can call 
a robot's predefined name, to let the robot recognize the call 
while the robot knows the direction to move to the user. This 
service analyzes audio data from 3 microphones on the 
shoulder for sound localization and 16 mic array on the head 
for speech recognition to recognize the command from the 
user.

Autonomous navigation: A user can command the robot to 
move to a specific position in the map to perform some task. 
For instance, the robot can navigate to its destination in the 
home environment via its sensors, which include laser 
scanners and ultrasonic sensors. The robot plans a path to 
the specified position, executes this plan, and modifies it as 
necessary for avoiding unexpected obstacles. While the 
robot is moving, it constantly checks sensor data from its 
sensors every 200 ms.

An errand service: The robot can carry objects that a user 
(i.e., an aged person) usually uses, like a plate, books, a cane 
a cup of tea, beverages, etc according to the user's 
instructions. For instance, the user can order the robot to 
bring a cup of tea or beverage by speaking the name of the 
drink.
Of these T-Rot services, our emphasis was on the autonomous 
navigation service, which is one of the most challenging issues 
and is essential in developing service robots, particularly mobile 
service robots to assist elderly people. It includes hardware 
integration for various sensors and actuators, and the development 
of crucial navigation algorithms like maps, path planners, and
536
localizers as well as software integration of software modules like 
a path planner, a localizer, and a map building module.
2.4

Control Architecture of PSR
Up to now, there have been many related research activities to 
develop efficient and well-defined control architectures and 
system integration strategies for constructing service robots. A 
recent trend is that many control architectures are converging to a 
similar structure based on a hybrid approach that integrates 
reactive control and deliberation [6]. At KIST, for developing 
service robots, that is PSR-1,  PSR-2, and Jinny  in the previous 
work [5,6], the Tripodal schematic control architecture was 
proposed as the solution to the problem.  
One important point of Tripodal schematic design is to integrate 
robot systems by using a layered functionality diagram. The 
layered functionality diagram is a conceptual diagram of three 
layers for arrangement of various hardware and software modules 
and functions. It also shows the connectivity and the information 
flow between components. Those layers are composed of 
deliberate, sequencing, and reactive layers based on the hybrid 
approach. The purposes of the deliberate layer are to interface 
with a user and to execute a planning process. The sequencing 
layer is classified into two groups, that is, the controlling part that 
executes the process by managing the components in the reactive 
layer and the information part that extracts highly advanced 
information from sensor data. The reactive layer controls the real-time
command and hardware-related modules for sensors and 
actuators. The detailed description of whole control architecture 
of the PSR is introduced in [5]. 
However, as described earlier, in order to effectively apply this 
approach and the UML notation to developing service robots, it is 
essential to use a systematic software engineering process or 
methods like object-oriented analysis and design methods, 
especially for real-time and embedded systems. We believe that 
only a systematic and comprehensive software development 
process and method will be able to resolve the issues discussed 
before and will be vital for success in developing service robots.
2.5

The COMET method
COMET [14] is a method for designing real-time and distributed 
applications, which integrates object-oriented and concurrent 
processing concepts and uses the UML notation [15,16]. The 
COMET object- oriented software life cycle model is a highly 
iterative software development process based around the use case 
concept. Therefore, in this project, the COMET method with 
UML was used to develop a system for autonomous navigation by 
the intelligent service robot, T-Rot. The method separates 
requirements activities, analysis activities and design activities, 
and these activities are briefly described as below. The details are 
described in section 3 with the case study.

Requirements modeling - A use case model is developed in 
which the functional requirements of the system are defined 
in terms of actors and use cases.

Analysis modeling - Static and dynamic models of the 
system are developed. The static model defines the 
structural relationships among problem domain classes. A 
dynamic model is then developed in which the use cases 
from the requirements model are refined to show the objects 
that participate in each use case and how they interact with 
each other.

Design modeling ­ The software architecture of the system 
is designed, in which the analysis model is mapped to an 
operational environment. For distributed applications, a 
component based development approach is taken, in which 
each subsystem is designed as a distributed self-contained 
component.
APPLYING THE COMET/UML METHOD TO T-ROT
In this section, we explain how to develop robot software for the 
autonomous navigation system with the COMET/UML method. 
In our project, the UML notation conforms to UML 1.3 and the 
Rational Rose tool is used.
3.1

Requirements Modeling
Capturing the functional requirements of the system is the first 
phase in software development, which defines what the system 
should do or provide for the user. In our approach, developers can 
catch the functional requirements or services by using the use 
case model in terms of use cases and actors (see Fig. 4). To 
identify and define the requirements of the system more clearly, 
the system has to be considered like a black box. In the service 
robot, the actor can be usually a human user as well as external 
I/O devices and external timer.
Navigation
Commander
(from 1.0 Actors)
Clock
(from 1.0 Actors)
Obstacle Avoidance
&lt;&lt;extend&gt;&gt;

Fig. 4. Use case diagram for Navigation
Table 2 shows a specification for Navigation  use case. In our 
navigation system, we identified a Commander and a Clock as an 
actor. While the robot is moving, if the robot recognizes obstacles, 
it should avoid them for continuing to go to the destination. Even 
when humans or objects suddenly appear, the robot must be able 
to stop to avoid crashing into those. However, in order to do this, 
the robot has to check that there are obstacles by using sensor data 
more often (e.g., every 50 ms) than the normal navigation system 
does (e.g., every 200 ms). As a result, the Obstacle Avoidance use 
case is extended from the Navigation use case. During the 
Navigation use case is executing, if the obstacles are recognized, 
then the Obstacle Avoidance use case is triggered to perform the 
emergency stop of the robot. If the obstacles disappear, the robot 
moves again to the destination.
Table 2. Navigation use case
Summary
The Commander enters a destination and the robot 
system moves to the destination.
Actor Commander
Precondition
The robot system has the grid map and the current 
position is known
Description
1. The use case begins when the commander enters a 
destination.
2. The system calculates an optimal path to the 
destination.
3. The system commands the wheel actuator to start
537
moving to the destination.
4. The wheel actuator notifies the system that it has 
started moving
5. The system periodically reads sensor data and 
calculates the current position.
6. The system determines that it arrives at the destination 
and commands the wheel actuator to stop.
7. The wheel actuator notifies the system that it has 
stopped moving and the use case is finished.
Alternative
6.1. If the system doesn't arrive at the destination, it 
keeps moving.
Postcondition  The robot system is at the destination and waiting for the
next destination.
3.2

Analysis Modeling
3.2.1

Static Modeling
The objective of static modeling is to understand the interface 
between the system and the external environment and to describe 
the static structure of the system under development by 
developing a system context class diagram. It is specifically 
important for real-time and embedded systems like robot systems 
[14]. The system context class diagram can be determined by 
static modeling of the external classes that connect to the system.
Commander
(from 1.0 Actors)
Sensor
&lt;&lt;external input device&gt;&gt;
WheelActuator
&lt;&lt;external output device&gt;&gt;
CommandLine
&lt;&lt;external user&gt;&gt;
1
1
1
1
Robot Navigation System
&lt;&lt;System&gt;&gt;
1..*
1
1..*
1
Inputs To

1
1
1
1
Outputs to

1
1 1
1
interacts with

Clock
(from 1.0 Actors)
Clock
&lt;&lt;external timer&gt;&gt;
1
1
1
1
Awakens


Fig. 5. Robot Navigation System context class diagram
The system context class diagram of the Robot Navigation System 
is shown in Fig. 5, which illustrates the external classes to which 
the system has to interface. In our navigation system, a 
commander enters a destination via a command line, to which the 
robot should move. The system uses sensor data via various 
sensors such as laser scanners, IR scanners, ultrasonic sensors, etc 
and it controls the wheels of the robot via the wheel actuator. 
Therefore, the external classes correspond to the users (i.e., a 
Commander who interacts with the system via a Command Line), 
and I/O devices (i.e., a Sensor and Wheel Actuator). A Clock actor 
needs an external timer class called Clock to provide timer events 
to the system. This external timer class is needed to periodically 
check sensor data via those sensors for avoiding obstacles (i.e., 
doing the emergency stop) while the robot is moving. 
Next, to structure the Robot Navigation System into objects, 
object structuring needs to be considered in preparation for 
dynamic modeling. The objective of the object structuring is to 
decompose the problem into objects within the system. We 
identified the internal objects according to the object structuring 
criteria in COMET (see Fig. 6). In our system, interface objects, 
i.e. a Command Line Interface,  Sensor Interface and Wheel 
Actuator Interface are identified by identifying the external 
classes that interface to the system, i.e. the Command Line, 
Sensor, and Wheel Actuator, respectively. There are four entity 
objects identified, that is, a Current Position,  Destination, 
Navigation Path and Navigation Map, which are usually long-living
object that stores information. In addition to those objects,
there is a need for control objects, which provide the overall 
coordination for objects in a use case and may be coordinator, 
state-dependent control, or timer objects. The Navigation System 
has a state-dependent control object called Navigation Control 
that controls the wheel actuator and sensors. The states of the 
Navigation Control object are shown on a Navigation Control 
statechart (this will be discussed in the dynamic modeling). There 
are two timer objects, i.e. a Navigation Timer and an Obstacle 
Avoidance Timer. The Obstacle Avoidance Timer is activated by a 
timer event from an external timer to periodically check that there 
is any obstacle around the robot. On the other hand, the 
Navigation Timer is started by the Navigation Control object and 
generates a timer event for navigation. Also, a Localizer 
algorithm object and Path Planner algorithm object are identified, 
which encapsulate an algorithm used in the problem domain, 
namely the autonomous navigation.
&lt;&lt; Robot Navigation System &gt;&gt;
Commander
(from 1.0 Actors)
CommandLineInterface
&lt;&lt;user interface&gt;&gt;
CommandLine
&lt;&lt;external user&gt;&gt;
1
1
1
1
1
1
1
1
SensorInterface
&lt;&lt;input device interface&gt;&gt;
Sensor
&lt;&lt;external input device&gt;&gt;
1
1..*
1
1..*
WheelActuator
&lt;&lt;external output device&gt;&gt;
WheelActuatorInterface
&lt;&lt;output device interface&gt;&gt;
1
1
1
1
Destination
&lt;&lt;entity&gt;&gt;
Navigation Path
&lt;&lt;entity&gt;&gt;
Navigation Map
&lt;&lt;entity&gt;&gt;
Current Position
&lt;&lt;entity&gt;&gt;
Navigation Control
&lt;&lt;state dependent&gt;&gt;
Navigation Timer
&lt;&lt;timer&gt;&gt;
ObstacleAvoidanceTimer
&lt;&lt;timer&gt;&gt;
Clock
&lt;&lt;external timer&gt;&gt;
1
1
1
1
Localizer
&lt;&lt;algorithm&gt;&gt;
PathPlanner
&lt;&lt;algorithm&gt;&gt;

Fig. 6. Object structuring class diagram for Navigation
System
3.2.2

Dynamic Modeling
Dynamic modeling emphasizes the dynamic behavior of the 
system and plays an important role for distributed, concurrent and 
real-time system analysis. The dynamic model defines the object 
interactions that correspond to each use case and thus is based on 
the use cases and the objects identified during object structuring. 
In our case, collaboration diagrams are developed to show the 
sequence of object interactions for each use case. Additionally, if 
the collaboration involves the state-dependent object, which 
executes a statechart, the event sequence is shown on a statechart.
: Navigation
Control
: CommandLine
: Sensor
: WheelActuator
: WheelActuatorInterface
: SensorInterface
: Destination
: Navigation
Path
: Navigation Map
: Current
Position
: CommandLineInterface
: Navigation
Timer
Path
Planner
Localizer
Sequencing
Layer
&lt;&lt;external user&gt;&gt;
&lt;&lt;user interface&gt;&gt;
&lt;&lt;state dependent control&gt;&gt;
&lt;&lt;timer&gt;&gt;
&lt;&lt;entity&gt;&gt;
&lt;&lt;algorithm&gt;&gt;
&lt;&lt;entity&gt;&gt;
&lt;&lt;entity&gt;&gt;
&lt;&lt;entity&gt;&gt;
&lt;&lt;algorithm&gt;&gt;
&lt;&lt;external input device&gt;&gt;
&lt;&lt;input device interface&gt;&gt;
&lt;&lt;output device interface&gt;&gt;
&lt;&lt;external output device&gt;&gt;
Deliberate 
Layer
Reactive 
Layer
1.2a: Store Destination
2.11, 3.11 : Check Destination
2.12 : No , 3.12: Yes
1.13, 2.18: Planned Path
1.10, 2.15: Read a Path
1.14: Start
2.19: Move
3.13: Stop
1.17: Started
3.16: Stopped
1.4, 2.7, 3.7: Read Current Position
1.7, 2.10, 3.10: Current Position
1.2, 2.5, 3.5: Read Map
1.8, 2.13: Update Map
1.3, 2.6, 3.6 : Map
1.9, 2.14: Updated Map
1: Enter Destination
1.1: Destination Entered
2.1, 3.1: Read Sensors
2.4, 3.4: Sensor Data
2.2, 3.2: Read
2.3, 3.3: Data
1.15: Start WheelActuator Output
2.20:Move WheelActuator Output
3.14: Stop WheelActuator Output
1.16, 5.8: Started Ack
3.15: Stopped Ack
1.5, 2.8, 3.8: Localize
1.6, 2.9, 3.9: Current Position
2, 3: After(Elapsed Time)
1.18, 5.10: Start Timer
3.17, 4.10: Stop Timer
1.12, 2.17: Path
1.11, 2.16: Plan a path

Fig. 7. Collaboration diagram for Navigation use case
538
In the navigation system, the Localizer has the algorithm which 
can calculate the current position based on sensor data via sensors. 
So, the role of the Localizer is to update the current position of 
the service robot. In the Path Planner object, there is a method for 
calculating a path to arrive at the destination based on both sensor 
information and the current position that is calculated at the 
Localizer. The Navigation Timer is an internal timer that is 
controlled by the Navigation Control. After the destination is 
entered from the external user, the Navigation Control starts the 
Navigation Timer, then the timer generates a timer event 
periodically (i.e., every 200ms) until the Navigation Control stops 
the timer.  
The Navigation  use case starts with the commander entering the 
destination into the navigation system. The message sequence 
number starts at 1, which is the first external event initiated by the 
actor. Subsequence numbering in sequence is from 1.1 to 1.18 as 
shown in Fig. 7. The next message sequence activated by the 
Navigation Timer is numbered 2, followed by the events 2.1, 2.2, 
and so forth. The following message sequences are illustrated in 
the collaboration diagram (see Fig. 7).  
The collaboration diagram for the Obstacle Avoidance use case is 
shown in Fig. 8. When activated by the Obstacle Avoidance 
Timer every 50 ms, the Sensor Interface object reads sensor data 
via various sensors (Events 4.1, 5.1, 6.1). If an obstacle is 
recognized, the Obstacle Avoidance Timer sends the emergency 
stop message to the Wheel Actuator Interface (Event 4.5). 
Afterwards, the timer also sends a suspend event to the 
Navigation Control. If the obstacle disappears, the timer sends a 
restart event to the Navigation Control for the robot to move 
again.
: Sensor
: WheelActuator
: WheelActuatorInterface
: SensorInterface
: Clock
: ObstacleAvoidanceTimer
&lt;&lt;state dependent control&gt;&gt;
&lt;&lt;external timer&gt;&gt;
&lt;&lt;external input device&gt;&gt;
&lt;&lt;timer&gt;&gt;
&lt;&lt;input device interface&gt;&gt;
&lt;&lt;output device interface&gt;&gt;
&lt;&lt;external output device&gt;&gt;
: Navigation
Control
Sequencing
Layer
Reactive 
Layer
5.6: Start
5.9: Started
4, 5, 6: Timer Event
4.2, 5.2, 6.2: Read
4.3, 5.3, 6.3: Data
4.1, 5.1, 6.1: Read Sensors
4.4, 5.4, 6.4: Sensor Data
4.5: Stop
4.8: Stopped
4.9: Suspend
5.5: Restart
6.5: Time Expired
5.7: Start WheelActuator Output
4.6 : Stop WheelActuator Output
5.8: Started Ack
4.7: Stopped Ack

Fig. 8. Collaboration diagram for Obstacle Avoidance use case 
With COMET, the software architecture can be based on a 
software architectural style (pattern) such as client/server or 
layers of abstraction. In our project, the layered strategy of the 
Tripodal schematic design described in section 2 is applied for 
design and modeling of the robot system, which provides a 
conceptual diagram of three layers (i.e., deliberate, sequencing, 
and reactive layers) for arrangement of various hardware and 
software modules and functions. Therefore, in the collaboration 
diagrams (see Fig. 7 and 8), the Command Line Interface is 
located in the deliberate layer and the Sensor Interface,  Wheel 
Actuator Interface, and Obstacle Avoidance Timer are in the 
reactive layer. The others are positioned in the sequencing layer.  
In our navigation system, after drawing the collaboration 
diagrams for the Navigation and Obstacle Avoidance use cases 
which include the Navigation Control state-dependent object, we 
develop a Navigation Control statechart, which is executed by the 
Navigation Control object. The statechart needs to be considered
in connection with the collaboration diagram. Specifically, it is 
required to take into account the messages that are received and 
sent by the control object, which executes the statechart [14]. An 
input event (e.g., 1.1: destination entered) into the Navigation 
Control object on the collaboration diagram should be consistent 
with the same event shown on the statechart. The output event, 
which causes an action, enable or disable activity, like 1.2: Read 
Map (which cases an action) on the statechart must be consistent 
with the output event depi