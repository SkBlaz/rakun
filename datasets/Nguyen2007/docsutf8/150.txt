Preventing Attribute Information Leakage in Automated Trust Negotiation
ABSTRACT
Automated trust negotiation is an approach which establishes
trust between strangers through the bilateral, iterative
disclosure of digital credentials. Sensitive credentials
are protected by access control policies which may also be
communicated to the other party. Ideally, sensitive information
should not be known by others unless its access control
policy has been satisfied. However, due to bilateral information
exchange, information may flow to others in a variety
of forms, many of which cannot be protected by access control
policies alone. In particular, sensitive information may
be inferred by observing negotiation participants' behavior
even when access control policies are strictly enforced.
In this paper, we propose a general framework for the
safety of trust negotiation systems. Compared to the existing
safety model, our framework focuses on the actual
information gain during trust negotiation instead of the exchanged
messages. Thus, it directly reflects the essence of
safety in sensitive information protection. Based on the proposed
framework, we develop policy databases as a mechanism
to help prevent unauthorized information inferences
during trust negotiation. We show that policy databases
achieve the same protection of sensitive information as existing
solutions without imposing additional complications
to the interaction between negotiation participants or restricting
users' autonomy in defining their own policies.
Categories and Subject Descriptors
K.6.5 [Management
of Computing and Information Systems]: Security and
Protection
General Terms
Security, Theory

INTRODUCTION
Automated trust negotiation (ATN) is an approach to
access control and authentication in open, flexible systems
such as the Internet. ATN enables open computing by as-Permission
to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CCS'05, November 7­11, 2005, Alexandria, Virginia, USA.
Copyright 2005 ACM 1-59593-226-7/05/0011 ...
$
5.00.
signing an access control policy to each resource that is to
be made available to entities from different domains. An
access control policy describes the attributes of the entities
allowed to access that resource, in contrast to the traditional
approach of listing their identities. To satisfy an access control
policy, a user has to demonstrate that they have the
attributes named in the policy through the use of digital
credentials. Since one's attributes may also be sensitive, the
disclosure of digital credentials is also protected by access
control policies.
A trust negotiation is triggered when one party requests
access to a resource owned by another party. Since each
party may have policies that the other needs to satisfy, trust
is established incrementally through bilateral disclosures of
credentials and requests for credentials, a characteristic that
distinguishes trust negotiation from other trust establishment
approaches [2, 11].
Access control policies play a central role in protecting
privacy during trust negotiation. Ideally, an entity's sensitive
information should not be known by others unless they
have satisfied the corresponding access control policy. However
, depending on the way two parties interact with each
other, one's private information may flow to others in various
forms, which are not always controlled by access control
policies. In particular, the different behaviors of a negotiation
participant may be exploited to infer sensitive information
, even if credentials containing that information are
never directly disclosed.
For example, suppose a resource's policy requires Alice to
prove a sensitive attribute such as employment by the CIA.
If Alice has this attribute, then she likely protects it with an
access control policy. Thus, as a response, Alice will ask the
resource provider to satisfy her policy. On the other hand,
if Alice does not have the attribute, then a natural response
would be for her to terminate the negotiation since there is
no way that she can access the resource. Thus, merely from
Alice's response, the resource provider may infer with high
confidence whether or not Alice is working for the CIA, even
though her access control policy is strictly enforced.
The problem of unauthorized information flow in ATN has
been noted by several groups of researchers [20, 22, 27]. A
variety of approaches have been proposed, which mainly fall
into two categories. Approaches in the first category try to
"break" the correlation between different information. Intuitively
, if the disclosed policy for an attribute is independent
from the possession of the attribute, then the above inference
is impossible. A representative approach in this category is
by Seamons et al. [20], where an entity possessing a sensi-36
tive credential always responds with a cover policy of f alse
to pretend the opposite. Only when the actual policy is satisfied
by the credentials disclosed by the opponent will the
entity disclose the credential. Clearly, since the disclosed
policy is always f alse, it is not correlated to the possession
of the credential. One obvious problem with this approach,
however, is that a potentially successful negotiation may fail
because an entity pretends to not have the credential.
Approaches in the second category aim to make the correlation
between different information "safe", i.e., when an
opponent is able to infer some sensitive information through
the correlation, it is already entitled to know that information
. For example, Winsborough and Li [23] proposed the
use of acknowledgement policies ("Ack policies" for short) as
a solution. Their approach is based on the principle "discuss
sensitive topics only with appropriate parties". Therefore,
besides an access control policy P , Alice also associates an
Ack policy P
Ack
with a sensitive attribute A. Intuitively,
P
Ack
determines when Alice can tell others whether or not
she has attribute A. During a negotiation, when the attribute
is requested, the Ack policy P
Ack
is first sent back
as a reply. Only when P
Ack
is satisfied by the other party,
will Alice disclose whether or not she has A and may then
ask the other party to satisfy the access control policy P .
In order to prevent additional correlation introduced by Ack
policies, it is required that all entities use the same Ack policy
to protect a given attribute regardless of whether or not
they have A. In [23], Winsborough and Li also formally defined
the safety requirements in trust negotiation based on
Ack policies.
Though the approach of Ack policies can provide protection
against unauthorized inferences, it has a significant disadvantage
. One benefit of automated trust negotiation is
that it gives each entity the autonomy to determine the appropriate
protection for its own resources and credentials.
The perceived sensitivity of possessing an attribute may be
very different for different entities. For example, some may
consider the possession of a certificate showing eligibility for
food stamps highly sensitive, and thus would like to have a
very strict Ack policy for it. Some others may not care as
much, and have a less strict Ack policy, because they are
more concerned with their ability to get services than their
privacy. The Ack Policy system, however, requires that all
entities use the same Ack policy for a given attribute, which.
deprives entities of the autonomy to make their own decisions
. This will inevitably be over-protective for some and
under-protective for others. And either situation will result
in users preferring not to participate in the system.
In this paper, we first propose a general framework for safe
information flow in automated trust negotiation. Compared
with that proposed by Winsborough and Li, our framework
focuses on modeling the actual information gain caused by
information flow instead of the messages exchanged. Therefore
it directly reflects the essence of safety in sensitive information
protection. Based on this framework, we propose
policy databases as a solution to the above problem. Policy
databases not only prevent unauthorized inferences as described
above but also preserve users' autonomy in deciding
their own policies. In order to do this, we focus on severing
the correlation between attributes and policies by introducing
randomness, rather than adding additional layers or
fixed policies as in the Ack Policy system. In our approach,
there is a central database of policies for each possession
sensitive attribute. Users who possess the attribute submit
their policies to the database anonymously. Users who do
not possess the attribute can then draw a policy at random
from the database. The result of this process is that the
distributions of policies for a given possession sensitive attribute
are identical for users who have the attribute and
users who do not. Thus, an opponent cannot infer whether
or not users possess the attribute by looking at their policies.
The rest of the paper is organized as follows. In section
2, we propose a formal definition of safety for automated
trust negotiation. In section 3, we discuss the specifics of
our approach, including what assumptions underlie it, how
well it satisfies our safety principle, both theoretically and
in practical situations, and what practical concerns to implementing
it exist. Closely related work to this paper is
reported in section 4. We conclude this paper in section 5
SAFETY IN TRUST NEGOTIATION
In [23], Winsborough and Li put forth several definitions
of safety in trust negotiation based on an underlying notion
of indistinguishability. The essence of indistinguishability is
that if an opponent is given the opportunity to interact with
a user in two states corresponding to two different potential
sets of attributes, the opponent cannot detect a difference
in those sets of attributes based on the messages sent. In
the definition of deterministic indistinguishability, the messages
sent in the two states must be precisely the same. In
the definition of probabilistic indistinguishability, they must
have precisely the same distribution.
These definitions, however, are overly strict. To determine
whether or not a given user has a credential, it is not sufficient
for an opponent to know that the user acts differently
depending on whether or not that user has the credential:
the opponent also has to be able to figure out which behavior
corresponds to having the credential and which corresponds
to lacking the credential. Otherwise, the opponent has not
actually gained any information about the user.
Example 1. Suppose we have a system in which there
is only one attribute and two policies, p
1
and p
2
. Half of
the users use p
1
when they have the attribute and p
2
when
they do not. The other half of the users use p
2
when they
have the attribute and p
1
when they do not. Every user's
messages would be distinguishable under the definition of indistinguishability
presented in [23] because for each user the
distribution of messages is different. However, if a fraction
r of the users have the attribute and a fraction 1 - r do not,
then
1
2
· r +
1
2
· (1 - r) =
1
2
of the users display policy p
1
and
the other half of the users display policy p
2
. As such the
number of users displaying p
1
or p
2
does not change as r
changes. Hence, they are independent. Since the policy displayed
is independent of the attribute when users are viewed
as a whole, seeing either policy does not reveal any information
about whether or not the user in question has the
attribute.
As such, Winsborough and Li's definitions of indistinguishability
restrict a number of valid systems where a given
user will act differently in the two cases, but an opponent
cannot actually distinguish which case is which. In fact,
their definitions allow only systems greatly similar to the
Ack Policy system that they proposed in [22]. Instead we
propose a definition of safety based directly on information
37
gain instead of the message exchange sequences between the
two parties.
Before we formally define safety, we first discuss what
safety means informally. In any trust negotiation system,
there is some set of objects which are protected by policies
. Usually this includes credentials, information about
attribute possession, and sometimes even some of the policies
in the system. All of these can be structured as digital
information, and the aim of the system is to disclose that
information only to appropriate parties.
The straight-forward part of the idea of safety is that an
object's value should not be revealed unless its policy has
been satisfied. However, we do not want to simply avoid
an object's value being known with complete certainty, but
also the value being guessed with significant likelihood.
As such, we can define the change in safety as the change
in the probability of guessing the value of an object.
If
there are two secrets, s
1
and s
2
, we can define the conditional
safety of s
1
upon the disclosure of s
2
as the conditional
probability of guessing s
1
given s
2
. Thus, we define
absolute safety in a system as being the property that no
disclosure of objects whose policies have been satisfied results
in any change in the probability of guessing the value
of any object whose policy has not been satisfied regardless
of what inferences might be possible.
There exists a simple system which can satisfy this level of
safety, which is the all-or-nothing system, a system in which
all of every user's objects are required to be protected by
a single policy which is the same for all users. Clearly in
such a system there are only two states, all objects revealed
or no objects revealed. As such, there can be no inferences
between objects which are revealed and objects which are
not. This system, however, has undesirable properties which
outweigh its safety guarantees, namely the lack of autonomy,
flexibility, and fine-grained access control. Because of the
necessity of protecting against every possible inference which
could occur, it is like that any system which achieves ideal
safety would be similarly inflexible.
Since there have been no practical systems proposed which
meet the ideal safety condition, describing ideal safety is not
sufficient unto itself. We wish to explore not just ideal safety,
but also safety relative to certain types of attacks. This will
help us develop a more complete view of safety in the likely
event that no useful system which is ideally safe is found.
If a system does not have ideal safety, then there must
be some inferences which can cause a leakage of information
between revealed objects and protected objects. But
this does not mean that every single object revealed leaks
information about every protected object. As such, we can
potentially describe what sort of inferences a system does
protect against. For example, Ack Policy systems are moti-vated
by a desire to prevent inferences from a policy to the
possession of the attribute that it protects. Inferences from
one attribute to another are not prevented by such a system
(for example, users who are AARP members are more likely
to be retired than ones who are not). Hence, it is desirable
to describe what it means for a system to be safe relative to
certain types of inferences.
Next we present a formal framework to model safety in
trust negotiation. The formalism which we are using in this
paper is based on that used by Winsborough and Li, but is
substantially revised.
2.0.1
Trust Negotiation Systems
A Trust Negotiation System is comprised of the following
elements:
· A finite set, K, of principals, uniquely identified by a randomly
chosen public key, P ub
k
. Each principal knows the
associated private key, and can produce a proof of identity.
· A finite set, T , of attributes. An attribute is something
which each user either possesses or lacks. An example would
be status as a licensed driver or enrollment at a university.
· A set, G, of configurations, each of which is a subset of T .
If a principal k is in a configuration g  G, then k possesses
the attributes in g and no other attributes.
· A set, P, of possible policies, each of which is a logical proposition
comprised of a combination of and, or, and attributes
in T . We define an attribute in a policy to be true with respect
to a principal k if k has that attribute. We consider
all logically equivalent policies to be the same policy.
· Objects. Every principal k has objects which may be protected
which include the following:
- A set, S, of services provided by a principal. Every principal
offers some set of services to all other principals. These
services are each protected by some policy, as we will describe
later. A simple service which each principal offers is
a proof of attribute possession. If another principal satisfies
the appropriate policy, the principal will offer some proof
that he holds the attribute. This service is denoted s
t
for
any attribute t  T .
- A set, A, of attribute status objects. Since the set of all
attributes is already known, we want to protect the information
about whether or not a given user has an attribute.
As such we formally define A as a set of boolean valued random
variables, a
t
. The value of a
t
for a principal k, which
we denote a
t
(k) is defined to be true if k possesses t  T
and false otherwise. Thus A = {a
t
|t  T }.
- A set, Q of policy mapping objects. A system may desire
to protect an object's policy either because of correlations
between policies and sensitive attributes or because in some
systems the policies themselves may be considered sensitive.
Similar to attribute status objects, we do not protect a policy
, per se, but instead the pairing of a policy with what
it is protecting. As such, each policy mapping object is a
random variable q
o
with range P where o is an object. The
value of q
o
for a given principal k, denoted q
o
(k) is the policy
that k has chosen to protect object o.
Every system should define which objects are protected. It
is expected that all systems should protect the services, S,
and the attribute status objects, A. In some systems, there
will also be policies which protect policies. Thus protected
objects may also include a subset of Q. We call the set of
protected objects O, where O  S  A  Q. If an object is
not protected, this is equivalent to it having a policy equal
to true.
For convenience, we define Q
X
to be the members of Q
which are policies protecting members of X , where X is a
set of objects. Formally, Q
X
= {q
o
Q|o  X }.
Some subset of the information objects are considered to
be sensitive objects. These are the objects about which we
want an opponent to gain no information unless they have
satisfied that object's policy. Full information about any
object, sensitive or insensitive, is not released by the system
38
until its policy has been satisfied, but it is acceptable for
inferences to cause the leakage of information which is not
considered sensitive.
· A set, N , of negotiation strategies. A negotiation strategy
is the means that a principal uses to interact with other
principals. Established strategies include the eager strategy
[24] and the trust-target graph strategy [22].
A negotiation
strategy, n, is defined as an interactive, deterministic,
Turing-equivalent computational machine augmented by a
random tape. The random tape serves as a random oracle
which allows us to discuss randomized strategies.
A negotiation strategy takes as initial input the public knowledge
needed to operate in a system, the principal's attributes,
its services, and the policies which protect its objects. It
then produces additional inputs and outputs by interacting
with other strategies. It can output policies, credentials,
and any additional information which is useful. We do not
further define the specifics of the information communicated
between strategies except to note that all the strategies in a
system should have compatible input and output protocols.
We refrain from further specifics of strategies since they are
not required in our later discussion.
· An adversary, M , is defined as a set of principals coordinating
to discover the value of sensitive information objects
belonging to some k  M .
Preventing this discovery is
the security goal of a trust negotiation system. We assume
that adversaries may only interact with principals through
trust negotiation and are limited to proving possession of
attributes which they actually possess. In other words, the
trust negotiation system provides a means of proof which is
resistant to attempts at forgery.
· A set, I, of all inferences. Each inference is a minimal subset
of information objects such that the joint distribution of the
set differs from the product of the individual distributions
of the items in the set.
1
These then allow a partitioning, C, of the information objects
into inference components. We define a relation  such
that o
1
o
2
iff i  I|o
1
, o
2
i. C is the transitive closure
of .
In general, we assume that all of the information objects
in our framework are static. We do not model changes in a
principal's attribute status or policies. If such is necessary,
the model would need to be adapted.
It should also be noted that there is an additional constraint
on policies that protect policies which we have not
described.
This is because in most systems there is a way
to gain information about what a policy is, which is to satisfy
it. When a policy is satisfied, this generally results in
some service being rendered or information being released.
As such, this will let the other party know that they have
satisfied the policy for that object. Therefore, the effective
policy protecting a policy status object must be the logical
or of the policy in the policy status object and the policy
which protects it.
2.0.2
The Ack Policy System
To help illustrate the model, let us describe how the Ack
Policy system maps onto the model. The mapping of oppo-1
A system need not define the particulars of inferences, but
should discuss what sort of inferences it can deal with, and
hence what sort of inferences are assumed to exist.
nents, the sets of principals, attributes, configurations, and
policies in the Ack Policy system is straightforward.
In an Ack Policy system, any mutually compatible set of
negotiation strategies is acceptable. There are policies for
protecting services, protecting attribute status objects, and
protecting policies which protect attribute proving services.
As such, the set of protected objects, O = S  A  Q
S
.
According to the definition of the Ack Policy system, for
a given attribute, the policy that protects the proof service
for that attribute is protected by the same policy that protects
the attribute status object. Formally, t  T , k 
K, q
a
t
(k) = q
q
st
(k). Further, the Ack policy for an attribute
is required to be the same for all principals. Thus we know
t  T p  Pk  K|q
a
t
(k) = p.
Two basic assumptions about the set of inferences, I, exist
in Ack Policy systems, which also lead us to conclusions
about the inference components, C. It is assumed that inferences
between the policy which protects the attribute proving
service, q
s
t
(k), and the attribute status object, a
t
(k),
exist. As such, those two objects should always be in the
same inference component. Because Ack Policies are uniform
for all principals, they are uncorrelated to any other
information object and they cannot be part of any inference.
Hence, each Ack Policy is in an inference component of its
own.
2.0.3
Safety in Trust Negotiation Systems
In order to formally define safety in trust negotiation, we
need to define the specifics of the opponent. We need to
model the potential capabilities of an opponent and the information
initially available to the opponent. Obviously, no
system is safe against an opponent with unlimited capabilities
or unlimited knowledge.
As such, we restrict the opponent to having some tactic,
for forming trust negotiation messages, processing responses
to those messages, and, finally, forming a guess about the
values of unrevealed information objects. We model the tactic
as an interactive, deterministic, Turing-equivalent computational
machine. This model is a very powerful model,
and we argue that it describes any reasonable opponent.
This model, however, restricts the opponent to calculating
things which are computable from its input and implies that
the opponent behaves in a deterministic fashion.
The input available to the machine at the start is the
knowledge available to the opponent before any trust negotiation
has taken place. What this knowledge is varies
depending on the particulars of a trust negotiation system.
However, in every system this should include the knowledge
available to the principals who are a part of the opponent
, such as their public and private keys and their credentials
. And it should also include public information such
as how the system works, the public keys of the attribute
authorities, and other information that every user knows.
In most systems, information about the distribution of attributes
and credentials and knowledge of inference rules
should also be considered as public information.
All responses
from principals in different configurations become
available as input to the tactic as they are made. The tactic
must output both a sequence of responses and, at the end,
guesses about the unknown objects of all users.
We observe that an opponent will have probabilistic knowledge
about information objects in a system. Initially, the
probabilities will be based only on publicly available knowl-39
edge, so we can use the publicly available knowledge to describe
the a priori probabilities.
For instance, in most systems, it would be reasonable to
assume that the opponent will have knowledge of the odds
that any particular member of the population has a given attribute
. Thus, if a fraction h
t
of the population is expected
to possess attribute t  T , the opponent should begin with
an assumption that some given principal has a h
t
chance of
having attribute t. Hence, h
t
represents the a priori probability
of any given principal possessing t. Note that we
assume that the opponent only knows the odds of a given
principal having an attribute, but does not know for certain
that a fixed percentage of the users have a given attribute.
As such, knowledge about the value of an object belonging
to some set of users does not imply any knowledge about
the value of objects belonging to some other user.
Definition 1. A trust negotiation system is safe relative
to a set of possible inferences if for all allowed mappings between
principals and configurations there exists no opponent
which can guess the value of sensitive information objects
whose security policies have not been met with odds better
than the a priori odds over all principals which are not in
the opponent, over all values of all random tapes, and over
all mappings between public key values and principals.
Definition 1 differs from Winsborough and Li's definitions
in several ways. The first is that it is concerned with multiple
users. It both requires that the opponent form guesses
over all users and allows the opponent to interact with all
users. Instead of simply having a sequence of messages sent
to a single principal, the tactic we have defined may interact
with a variety of users, analyzing incoming messages, and
then use them to form new messages. It is allowed to talk
to the users in any order and to interleave communications
with multiple users, thus it is more general than those in [23].
The second is that we are concerned only with the information
which the opponent can glean from the communication,
not the distribution of the communication itself. As such,
our definition more clearly reflects the fundamental idea of
safety.
We next introduce a theorem which will be helpful in proving
the safety of systems.
Theorem 1. There exists no opponent which can beat the
a priori odds of guessing the value of an object, o, given
only information about objects which are not in the same
inference component as o, over all principals not in M and
whose policy for o M cannot satisfy, over all random tapes,
and over all mappings between public keys and principals.
The formal proof for this theorem can be found in Appendix
A. Intuitively, since the opponent only gains information
about objects not correlated to o, its guess of the
value of o is not affected.
With theorem 1, let us take a brief moment to prove
the safety of the Ack Policy systems under our framework.
Specifically, we examine Ack Policy systems in which the
distribution of strategies is independent of the distributions
of attributes, an assumption implicitly made in [23]. In Ack
Policy systems the Ack Policy is a policy which protects
two objects in our model: an attribute's status object and
its policy for that attribute's proof service. Ack Policies are
required to be uniform for all users, which ensures that they
are independent of all objects.
Ack Policy systems are designed to prevent inferences
from an attribute's policy to an attribute's status for attributes
which are sensitive. So, let us assume an appropriate
set of inference components in order to prove that Ack
Policy systems are safe relative to that form of inference.
As we described earlier, each attribute status object should
be in the same inference component with the policy which
protects that attribute's proof service, and the Ack policy
for each attribute should be in its own inference component.
The Ack Policy system also assumes that different attributes
are independent of each other. As such, each attribute status
object should be in a different inference group.
This set of inference components excludes all other possible
types of inferences. The set of sensitive objects is the
set of attribute status objects whose value is true. Due to
Theorem 1, we know then that no opponent will be able to
gain any information based on objects in different inference
components. So the only potential source of inference for
whether or not a given attribute's status object, a
t
, has a
value of true or f alse is the policy protecting the attribute
proof service, s
t
.
However, we know that the same policy, P , protects both
of these objects. As such unauthorized inference between
them is impossible without satisfying P .
2
Thus, the odds
for a
t
do not change. Therefore, the Ack Policy system is
secure against inferences from an attribute's policy to its
attribute status.
POLICY DATABASE
We propose a new trust negotiation system designed to
be safe under the definition we proposed, but to also allow
the users who have sensitive attributes complete freedom to
determine their own policies. It also does not rely on any
particular strategy being used. Potentially, a combination of
strategies could even be used so long as the strategy chosen
is not in any way correlated to the attributes possessed.
This system is based on the observation that there is more
than one way to deal with a correlation. A simple ideal system
which prevents the inference from policies to attribute
possession information is to have each user draw a random
policy. This system obviously does not allow users the freedom
to create their own policies. Instead we propose a system
which looks like the policies are random even though
they are not.
This system is similar to the existing trust negotiation
systems except for the addition of a new element: the policy
database. The policy database is a database run by a trusted
third party which collects anonymized information about the
policies which are in use. In the policy database system, a
2
Except that one of these is a policy mapping object which
is being protected by a policy. As such, we have to keep
in mind that there exists a possibility that the opponent
could gain information about the policy without satisfying
it. Specifically, the opponent can figure out what attributes
do not satisfy it by proving that he possesses those
attributes. However, in an Ack Policy system, the policy
protecting an attribute proof object of an attribute which a
user does not hold is always f alse. No opponent can distinguish
between two policies which they cannot satisfy since
all they know is that they have failed to satisfy 