Clustering din√°mico con hormigas artificiales
Ôªø
La tarea de clustering consiste en la clasificaci√≥n no supervisada de patrones (observaciones,
datos, vectores, etc.) en grupos. Este problema ha sido analizado en varios contextos y por investigadores de distintas disciplinas, reflejando su amplia utilidad. Si bien se han propuesto distintas
alternativas para abordar las tareas de clustering, existe un √°rea particularmente interesante y novedosa que ha planteado distintos enfoques bio-inspirados que incluyen los algoritmos gen√©ticos
y algoritmos basados en la met√°fora del comportamiento de las hormigas. En este trabajo, analizamos la utilizaci√≥n de algoritmos basados en el comportamiento de hormigas en la Miner√≠a de
Datos, y m√°s espec√≠ficamente, en la tarea de clustering. Entre estos algoritmos podemos mencionar al AntTree, inspirado en las posibilidades de auto-ensamblaje de las hormigas reales. Tambi√©n
se propone una extensi√≥n a este algoritmo que incluye la capacidad de desconexi√≥n del √°rbol por
parte de las hormigas con el objeto de posicionarse en otro grupo m√°s adecuado. √âsto permite
flexibilizar el proceso del descubrimiento de clusters dentro de los datos a analizar. La factibilidad del enfoque propuesto es analizada experimentalmente considerando distintas instancias del
problema de clustering. Los resultados obtenidos son comparados con los del algoritmo AntTree
original y los de K-means, uno de los algoritmos de clustering tradicional m√°s utilizados.
Palabras claves: Sistemas Inteligentes, T√©cnicas bio-inspiradas, Clustering, Miner√≠a de Datos.
1. Introducci√≥n
La tarea de clustering [4] es la clasificaci√≥n no supervisada de patrones (observaciones, datos,
vectores, etc.) en grupos. Este problema ha sido analizado en varios contextos y por investigadores de
distintas disciplinas, reflejando su amplia utilidad. Es un problema de gran dificultad combinatoria y,
dado a que se ha utilizado en diferentes √°reas, los m√©todos obtenidos carecen de generalidad.
La tarea de clustering consiste en la organizaci√≥n de una colecci√≥n de patrones (usualmente representados como vectores de atributos o puntos en un espacio multidimensional) en clusters (o grupos)
bas√°ndose en la similitud que existe entre los mismos. Intuitivamente, patrones de un mismo cluster
son m√°s similares a patrones que se encuentran fuera del mismo. Los humanos resuelven de manera
competitiva problemas de clustering en dos dimensiones, pero la mayor√≠a de los problemas reales
implican dimensiones m√°s grandes. Adem√°s, la distribuci√≥n de los datos muy dif√≠cilmente siga una
forma definida. Por ello, encontramos una gran cantidad de algoritmos que se comportan de mejor o
peor manera dependiendo de la distribuci√≥n del conjunto de datos.
La variedad de t√©cnicas que existen difieren en la representaci√≥n de los datos, en la medida de
proximidad (o similitud) entre elementos, y en la manera que agrupan los elementos. El m√©todo m√°s
simple que resuelve el problema de clustering se denomina K-means, en donde debemos definir la
cantidad de grupos (llamados centroides) que existen en los datos. Cada centroide define un grupo
de datos y se asocia cada dato al centroide m√°s cercano. Luego, iterativamente se recalculan estos
centroides de tal forma que en cada iteraci√≥n se minimiza la funci√≥n SSE (suma de los errores al
cuadrado). Cuando no existe mejora, el algoritmo finaliza su ejecuci√≥n. Este m√©todo posee desventajas
muy importantes. Una de las principales es que se debe especificar el n√∫mero de clusters desde el
principio, dato que generalmente no se conoce. Esto significa que para aplicar dicho algoritmo, se
presupone un conocimiento previo de la distribuci√≥n de los datos.
Debido a la gran importancia del problema de clustering en diferentes campos, en la literatura se
han propuesto distintos m√©todos para resolverlo. Recientemente, enfoques bio-inspirados tales como
los algoritmos gen√©ticos y metaheur√≠sticas como tabu search, Ant Colony Optimization (ACO) y
simulated annealing han sido aplicados exitosamente a este problema. Sin embargo, en la actualidad
ha surgido un importante grupo de algoritmos basados en la met√°fora del comportamiento de las
hormigas reales los cuales son aplicados a clustering. Entre dichos algoritmos podemos citar a AntClass, Ant-Tree y Ant-Clust.
En este trabajo investigamos la aplicaci√≥n de algoritmos basados en el comportamiento de las
colonias de hormigas (BCH) a problemas de clustering. En particular, la propuesta se centra en una
nueva versi√≥n del algoritmo Ant-Tree a la que denominamos DAntTree. El algoritmo DAntTree incorpora aspectos din√°micos de las hormigas en el proceso de construcci√≥n de los clusters, permitiendo
que las hormigas se desconecten del √°rbol y se posicionen en otro grupo m√°s adecuado. Esto flexibiliza el descubrimiento de clusters dentro de los datos a analizar y permite incrementar el grado de
precisi√≥n del algoritmo en cuanto al n√∫mero de clusters descubiertos. El trabajo incluye un estudio
comparativo entre DAntTree, el AntTree original y el algoritmo K-means a los efectos de analizar sus
potenciales ventajas y desventajas.
El resto del trabajo se organiza de la siguiente manera. En la secci√≥n 2 realizamos una breve rese√±a
de la aplicaci√≥n de t√©cnicas bio-inspiradas al problema de clustering. En la secci√≥n 3 describimos el
algoritmo AntTree que sirve de base para nuestra propuesta que presentamos luego en la secci√≥n 4.
En la secci√≥n 5 se describe el trabajo experimental realizado. En la secci√≥n 6 discutimos fortalezas y
limitaciones del enfoque propuesto y posibles extensiones futuras.
2. Clustering mediante t√©cnicas bio-inspiradas
Recientemente se han aplicado al problema de clustering distintos enfoques bio-inspirados tales
como los algoritmos gen√©ticos [12, 6] y metaheur√≠sticas como tabu search y simulated annealing [1].
Otra de las metaheur√≠sticas utilizada es el enfoque Ant Colony Optimization (ACO) [5], adaptado al
problema de clustering como se describe en [11].
Dentro de estas propuestas bio-inspiradas se destacan un grupo de algoritmos novedosos basados en la met√°fora del comportamiento de las colonias de hormigas reales (BCH). Estas t√©cnicas
demostraron ser muy efectivas, en comparaci√≥n con m√©todos tradicionales, debido a su comportamiento probabil√≠stico (que le permite evitar m√≠nimos locales) y la obtenci√≥n de buenos resultados sin
conocimiento previo del problema.
Como ejemplos de m√©todos BCH podemos nombrar el algoritmo Ant-Class [9] que utiliza los
principios exploratorios y estoc√°sticos del enfoque ACO combinados con los principios determin√≠sticos y heur√≠sticos del K-means. El entorno simulado es una grilla bidimensional en la cual las hormigas
recogen o depositan objetos en una parva de acuerdo a su similitud. El algoritmo Ant-Clust [8] inspirado en el reconocimiento qu√≠mico de las hormigas para formar grupos o clusters diferenciados por
sus respectivas propiedades qu√≠micas u olores. El algoritmo ACODF [12] un novedoso m√©todo que
presenta una adaptaci√≥n del enfoque ACO, para resolver la tarea de agrupaci√≥n.
Uno de los m√©todos recientemente desarrollado se denomina AntTree y se basa en el comportamiento de auto-ensamblaje detectado en cierto tipo de hormigas. En este caso las hormigas construyen
una estructura ‚Äúviviente‚Äù con sus cuerpos con el objeto de solucionar distintos problemas que deben
afrontar. En el contexto de clustering este m√©todo realiza un proceso en donde las hormigas (que
representan datos) se conectan entre s√≠ de acuerdo a su similitud. De esta manera se forma una estructura de √°rbol, que determina una partici√≥n del conjunto de datos. Este algoritmo ha sido comparado
previamente [2] con el m√©todo tradicional K-means y el algoritmo bio-inspirado AntClass, donde
demuestra ser un m√©todo efectivo y prometedor para su investigaci√≥n. En la siguiente secci√≥n detallaremos este m√©todo y posteriormente describiremos nuestra propuesta basada en el algoritmo AntTree
y que hemos denominado DAntTree.
3. El algoritmo AntTree
El algoritmo AntTree [2] se basa en el comportamiento de auto-ensamblaje encontrado en ciertos tipos de hormigas1, que se sujetan de manera incremental a un soporte fijo o a otras hormigas.
Construye una estructura en forma de √°rbol cuya organizaci√≥n puede ser interpretada como un agrupamiento jer√°rquico o como un agrupamiento particionado. Cada hormiga es un dato del conjunto de
datos y la forma en que se mueve por la estructura depende de su similitud con otras hormigas (datos).
AntTree est√° fundamentado en una serie de caracter√≠sticas reales observadas en los comportamientos
de auto-ensamblaje de las hormigas como por ejemplo la construcci√≥n de una estructura ‚Äúviviente‚Äù a
partir de un soporte fijo, su capacidad para moverse libremente por la estructura y otras2.
Para particionar los datos, se construye un √°rbol cuyos nodos corresponden a hormigas y cada
hormiga representa un dato del conjunto de datos a particionar. La tarea de clustering en este contexto
se reduce a determinar la disposici√≥n de las conexiones (arcos del √°rbol) que sujetan las hormigas al
soporte o a otra hormiga.
El supuesto b√°sico para trabajar con AntTree es que los datos est√°n expresados en alg√∫n lenguaje
que permite definir una funci√≥n de similitud Sim entre cualquier par de datos del conjunto bajo consideraci√≥n. De esta manera, si N es la cantidad total de datos, y (di, dj) es un par de datos arbitrario,
i ‚àà [1, N ], j ‚àà [1, N ], el valor Sim(i, j) ‚àà [0, 1] representar√° el grado de similitud entre di y dj . Un
valor de 0 significa que di y dj son completamente diferentes mientras que 1 significa que los datos
son iguales.
Los principios generales del algoritmo son los siguientes: cada hormiga representa un nodo a ser
conectado al √°rbol, es decir, un dato a ser clasificado. Partiendo de un soporte ficticio que denotaremos
a0, las hormigas gradualmente se conectar√°n al mismo y luego se conectar√°n a otras hormigas ya
conectadas y as√≠ sucesivamente hasta que no queden hormigas sin conectar al √°rbol. La organizaci√≥n
de la estructura resultante depender√° directamente de la medida de similitud definida por Sim(i, j) y
del vecindario local de las hormigas en movimiento.
A cada hormiga ai asociaremos los siguientes t√©rminos:
1. La conexi√≥n saliente de ai, que denotaremosO(ai) y que representa la conexi√≥n que ai mantiene
con el soporte u otra hormiga.
2. Las conexiones entrantes de ai, que denotaremos I(ai) y que representa el conjunto de conexiones que otras hormigas mantienen hacia ai (es decir sus hijos).
3. El dato di representado por ai.
4. Dos medidas denominadas el umbral (threshold) de similitud TSim(ai), y el umbral de diferencia TDissim(ai) que ser√°n actualizadas durante el proceso de construcci√≥n del √°rbol.
1Como ejemplos podemos citar a la hormiga argentina Linepithema humiles y a la hormiga africana Oecophylla
longinoda. En √©stas √∫ltimas, el proceso de auto-ensamblaje se produce generalmente para cruzar espacios vac√≠os y para
construir sus nidos.
2Ver [2] para una discusi√≥n m√°s detallada de los fundamentos biol√≥gicos de AntTree.
Figura 1: Proceso general de construcci√≥n del √°rbol con hormigas artificiales (adaptado de [2])
En la figura 1 se muestra el proceso general de ensamblaje de las hormigas artificiales. Se puede
observar que cada cada hormiga ai podr√° encontrarse en una de las siguientes situaciones:
1. Movi√©ndose en el √°rbol: una hormiga ai en movimiento (resaltadas con sombreado en la figura 1) puede encontrarse sobre el soporte del √°rbol (a0), o sobre otra hormiga ya conectada
(denotada por apos). En estos casos, ai no se encuentra conectada a la estructura por lo que ser√°
libre de moverse a los vecinos conectados a a0 o a apos. En la figura 2 se grafica el vecindario
de una hormiga arbitraria apos.
2. Conectada al √°rbol: en este caso ai ya tiene un valor asignado para O(ai) y no puede moverse
m√°s sobre la estructura. Por otra parte, una hormiga no podr√° tener m√°s de Lmax conexiones
entrantes (|I(ai)| ‚â§ Lmax). De esta manera podemos garantizar que la m√°xima apertura del
√°rbol no ser√° mayor que Lmax.
Figura 2: Vecindario de una hormiga arbitraria apos (adaptado de [2])
3.1. Detalles del algoritmo
El ciclo principal llevado a cabo por el algoritmo AntTree se muestra en la figura 3. Luego de
que todas las hormigas son ubicadas sobre el soporte con sus umbrales de similitud y diferencia
inicializados, en cada paso del ciclo una hormiga ai es seleccionada de una lista de hormigas sin
conectar L. M√°s adelante realizaremos algunas consideraciones sobre el ordenamiento de esta lista.
La hormiga ai seleccionada podr√° conectarse a otra hormiga (o al soporte) o moverse de acuerdo a su
similitud con sus vecinos. Por lo tanto, mientras existan hormigas movi√©ndose sobre la estructura, se
simular√° la realizaci√≥n de ciertas acciones por parte de las hormigas que depender√°n de su ubicaci√≥n
actual sobre el soporte (figura 4), o sobre otra hormiga (figura 5).
Sea L una lista (posiblemente ordenada) de hormigas sin conectar
Inicializaci√≥n: Ubicar todas las hormigas sobre el soporte.
TSim(aj)‚Üê 1 y TDissim(aj)‚Üê 0, para toda hormiga aj
Repetir
1. Escoger una hormiga ai de la lista L
2. Si ai est√° sobre el soporte (a0)
entonces caso soporte (ver figura 4)
sino caso hormiga (ver figura 5)
Hasta que todas las hormigas est√©n conectadas al √°rbol (L est√° vac√≠a)
Figura 3: Algoritmo Principal del AntTree
Para el caso en que ai se encuentra sobre el soporte, la situaci√≥n m√°s directa es cuando ai es la
primera hormiga, en cuyo caso se conecta directamente al soporte. En otro caso, ai es comparada con
a+, la hormiga m√°s similar a ai entre todas las que est√°n conectadas al soporte. Si son lo suficientemente similares (de acuerdo al umbral de similitud de ai), entonces ai se mover√° por el sub√°rbol
correspondiente a a+. En otro caso, se chequea si a+ y ai son lo suficientemente diferentes (de acuerdo al umbral de diferencia), en cuyo caso ai es conectada al soporte, creando un nuevo sub√°rbol dado
que es lo suficientemente diferente de las dem√°s hormigas conectadas al soporte. Finalmente, si ai no
es lo suficientemente similar ni lo suficientemente diferente, se actualizan ambos umbrales (diferencia
y similitud) de ai de la siguiente manera:
TSim(ai)‚Üê‚àí TSim(ai) * 0. 9
TDissim(ai)‚Üê‚àí TDissim(ai) + 0. 01
con el objeto de que ai sea m√°s ‚Äútolerante‚Äù la pr√≥xima vez que sea considerada y aumentar as√≠ su
probabilidad de conectarse (o moverse).
El segundo caso es aquel en que ai se encuentra sobre otra hormiga arbitraria apos (ver figura 5). Si
ai es a) lo suficientemente similar a apos, b) lo suficientemente diferente de las hormigas conectadas a
apos y c) existe una conexi√≥n entrante disponible (|I(ai)| < Lmax) entonces ai es conectada a apos. En
este caso ai constituir√° la ra√≠z de un nuevo sub√°rbol debajo de apos. La diferencia que existe con las
otras hormigas conectadas a apos ser√° tal que el nuevo sub-cluster estar√° bien separado de los dem√°s,
pero lo suficientemente parecido a apos. En caso contrario, ai es movida de manera aleatoria sobre
alg√∫n vecino de apos y sus umbrales son actualizados como en el caso anterior. El algoritmo finaliza
cuando todas las hormigas est√°n conectadas.
Antes de concluir la descripci√≥n del algoritmo cabe aclarar un punto respecto al orden inicial de
las hormigas en la lista L. Este paso influye significativamente en el algoritmo en general ya que
define cual ser√° la primer hormiga en conectarse al soporte. La estrategia que ha mostrado mejores
Si ninguna hormiga se encuentra conectada al soporte entonces conectar ai a a0
sino
Sea a+ la hormiga conectada a a0 m√°s similar a ai
(a) Si Sim(ai, a+) ‚â• TSim(ai) entonces mover ai sobre a+
(b) sino
i. Si Sim(ai, a+) < TDissim(ai) entonces /* ai es bastante diferente a a+*/
conectar ai al soporte a0 (si no quedan m√°s conexiones de entrada
en a0 entonces mover ai sobre a+ y decrementar TSim(ai))
ii. sino decrementar TSim(ai) y aumentar TDissim(ai) /* ai es mas tolerante */
Figura 4: Caso Soporte
resultados [2] es ordenar las hormigas de forma creciente en funci√≥n de su similitud con las dem√°s
hormigas (en promedio). De esta forma la primer hormiga en conectarse al soporte es aquella que es
m√°s distinta a todas las dem√°s y cercana a su propio grupo.
Debemos tener en cuenta que el √°rbol obtenido mediante esta t√©cnica no es equivalente a los
dendogramas encontrados por otras t√©cnicas tradicionales de clustering. En nuestro caso, cada nodo
corresponde a un dato del conjunto de datos, mientras que en los dendogramas s√≥lo las hojas se corresponden a datos del conjunto. En este sentido, la interpretaci√≥n del √°rbol obtenido por el algoritmo
AntTree (ver figura 6) puede ser la de una partici√≥n del conjunto de datos, como cualquier algoritmo
de agrupamiento particionado (considerando cada hormiga conectadas a a0 como un grupo distinto).
Tambi√©n puede ser interpretado como un √°rbol tipo dendograma, dejando que las hormigas que se
encuentran en los nodos internos bajen hacia las hojas siguiendo los nodos m√°s similares al mismo y
de esta forma ser comparado con cualquier algoritmo de agrupamiento jer√°rquico.
Debemos resaltar que existen caracter√≠sticas muy interesantes de AntTree como puede ser el hecho de que evita m√≠nimos locales gracias a su comportamiento probabil√≠stico y produce resultados
precisos sin conocimiento previo de la distribuci√≥n de los datos (no es necesario conocer de antemano
la cantidad de grupos existentes).
Sea apos la hormiga en que se encuentra ubicada ai, y sea ak un vecino aleatorio de apos
1. Si Sim(ai, apos) ‚â• TSim(ai) entonces
Sea a+ la hormiga conectada a apos m√°s similar a ai
(a) Si Sim(ai, a+) ‚â§ TDissim(ai) entonces conectar ai a apos /* Si no quedan m√°s
conexiones entrantes entonces mover ai hacia ak */
(b) sino decrementar TDissim(ai), incrementar TSim(ai) y mover ai hacia ak
2. sino mover ai hacia ak
Figura 5: Caso Hormiga
4. El algoritmo DAntTree
El m√©todo AntTree din√°mico o DAntTree que se propone en esta secci√≥n puede ser considerado
una extensi√≥n del algoritmo AntTree, que incluye como aspecto principal la posibilidad de que las
hormigas se desconecten del √°rbol para reposicionarse en otro grupo (ramificaci√≥n del √°rbol) m√°s
Figura 6: Interpretando el √°rbol como una partici√≥n sin jerarqu√≠a (adaptado de [2])
adecuado. Permite adem√°s explorar soluciones alternativas y quedarse con aquella que mejor se ajusta
al conjunto de datos.
En DAntTree la obtenci√≥n del modelo est√° dividido en 3 etapas principales: 1) la ejecuci√≥n del
AntTree (con par√°metros modificados), 2) la reubicaci√≥n de hormigas y 3) la uni√≥n de grupos.
Cada una de estas etapas es resumida en la figura 7.
1. Ejecuci√≥n del algoritmo AntTree (con par√°metros modificados).
2. Iterar hasta que todas las hormigas est√©n en un grupo adecuado
a. Selecci√≥n de las hormigas a desconectar del √°rbol
b. Ordenamiento de la lista de hormigas (en forma decreciente)
c. Reubicar las hormigas (AntTree con pre-asignaci√≥n de la rama a seguir)
3. Iterar Lmax veces o hasta que no se puedan realizar uniones
a. Selecci√≥n de 2 grupos a combinar
b. Proceso de uni√≥n
c. Evaluaci√≥n del modelo
Figura 7: Algoritmo Principal del AntTree Din√°mico
4.1. Primera etapa: Ejecuci√≥n del AntTree
El primer paso del m√©todo es la ejecuci√≥n del algoritmo b√°sico AntTree con sus par√°metros modificados de manera tal de obtener un n√∫mero considerable de grupos peque√±os y altamente acoplados
(muy similares entre s√≠) que est√©n suficientemente distanciados de los dem√°s grupos. Para lograr este efecto, s√≥lo fue necesario modificar la actualizaci√≥n del umbral TDissim de cada hormiga de la
siguiente manera3:
TDissim(ai)‚Üê‚àí TDissim(ai) + 0. 2
Tambi√©n se increment√≥ la m√°xima apertura del arbol, llev√°ndola a Lmax = 20, y de esta manera poder
obtener una mayor cantidad de grupos.
3La actualizaci√≥n de TSim se mantuvo como antes.
4.2. Segunda etapa: Reubicaci√≥n de las hormigas
Este proceso iterativo desconecta las hormigas que no se adec√∫an al grupo al que pertenecen
actualmente, y las reposiciona en otro grupo m√°s adecuado. El proceso se realiza hasta que cada
hormiga haya encontrado un grupo adecuado. Como se puede observar en la figura 7, esta etapa
involucra los siguientes pasos: a) selecci√≥n de las hormigas a desconectar, b) ordenamiento de las
hormigas seleccionadas y c) la reubicaci√≥n de las hormigas.
La selecci√≥n de las hormigas a desconectar (paso a) consiste en un recorrido del √°rbol para
determinar cu√°les son las hormigas que deber√≠an ser reubicadas en un lugar m√°s adecuado. Para
decidir si una hormiga se deb√≠a desconectar o no, se utiliz√≥ una funci√≥n basada en el m√©todo de
Silhouette [10] que determina el grado de pertenencia de una hormiga a su grupo actual, y que se
define como:
s(i) = (b(i)‚àí a(i))/max(a(i), b(i)) (1)
En este caso, a(i) es la distancia que existe entre la hormiga ai con respecto a la media del grupo al
que pertenece y b(i) es la distancia m√≠nima entre ai y las medias de los restantes grupos. Los valores
devueltos por esta funci√≥n se encuentran en el rango [‚àí1, 1]. En este caso, se defini√≥ un umbral
t = ‚àí0,2 para decidir si una cierta hormiga deb√≠a separarse de su estructura o no. El objetivo de este
umbral es separar a aquellas hormigas que realmente est√°n en un grupo equivocado. De esta forma,
si s(i) < t, podemos decir que hemos encontrado un grupo ‚Äúmejor‚Äù4 para la hormiga ai y por lo tanto
debe ser seleccionada para su desconexi√≥n.
Si L es la lista de hormigas seleccionadas para desconectar, otro paso importante es el ordenamiento de las hormigas seleccionadas L de acuerdo a su similitud pero en orden decreciente (paso
b). Observar que en este caso la lista L resultante, estar√° ordenada siguiendo el criterio opuesto al
adoptado por AntTree, ubicando a la hormiga m√°s similar a las dem√°s primero. La raz√≥n para ello,
es que a consecuencia del proceso de selecci√≥n previo, algunos grupos se quedar√°n sin hormigas y
desaparecer√°n. Por lo tanto, las hormigas que han sido asignadas a un grupo desaparecido deber√°n
ser reasignadas a otro grupo. Este proceso de reasignaci√≥n consiste simplemente en asignar el grupo
cuya media se encuentre m√°s cercana a la hormiga a reasignar. Por ello se ordena la lista en forma
decreciente con el objeto de que la hormiga m√°s similar de la lista influya en la media del grupo que
se le ha asignado y las hormigas similares a ella tomen el mismo grupo. Si hubi√©ramos ordenado
la lista de forma creciente, la hormiga m√°s lejana a las dem√°s se hubiera posicionado en primer lugar, influyendo de manera considerable en la media del grupo asignado y perjudicando a las dem√°s
reasignaciones.
Con respecto a la reubicaci√≥n de las hormigas (paso c), este paso involucra dos procesos. El
primero es la verificaci√≥n de los grupos preasignados, con el objeto de chequear si alguno de estos
grupos ha quedado vac√≠o. Si as√≠ fuera, entonces se le asigna un nuevo grupo de los que todav√≠a existen.
El segundo paso es el de reasignar la hormiga a su nuevo grupo. Esto se realiza moviendo la hormiga
a la primera de su grupo conectada al soporte. Luego repetimos los mismos pasos que en el algoritmo
b√°sico del AntTree, con el objeto de que se mantengan las distintas interpretaciones de la estructura.
4.3. Tercera etapa: Uni√≥n de grupos
En esta √∫ltima etapa se realiza un proceso de uni√≥n entre grupos con el objetivo de encontrar
los verdaderos grupos existentes dentro de los datos. La idea general consiste en combinar aquellos
grupos muy similares y evaluar el modelo obtenido, retornando aquel que mejor se haya adaptado al
conjunto de datos analizados.
4Un grupo cuya media se encuentra m√°s cercana a la hormiga ai que la media del grupo al que pertenece actualmente.
Para seleccionar los 2 grupos a combinar, se toman los m√°s similares de todos los existentes en
la estructura, de acuerdo a la similitud de sus medias. El proceso de uni√≥n es similar al paso de
reacomodar hormigas de la etapa anterior. Se unen las hormigas de uno de los grupos seleccionados
al otro grupo seleccionado. Cabe destacar que en este caso no existe un ordenamiento previo de las
hormigas, ya que todas han sido asignadas al mismo grupo. Por cada iteraci√≥n, se eval√∫a el modelo
obtenido con el mejor encontrado hasta ese momento. Si el nuevo modelo es m√°s preciso que el
mejor encontrado, entonces se almacena para devolver como resultado. La cantidad de iteraciones
depende de la cantidad de grupos encontrados y no supera las Lmax iteraciones, independientemente
del tama√±o de nuestro conjunto de datos.
Un aspecto que a√∫n no hemos explicado, es la forma en que se eval√∫a cada nuevo modelo, para
determinar si esta nueva organizaci√≥n de los datos es m√°s adecuada que el mejor modelo obtenido
hasta el momento. Para ello utilizamos una funci√≥n basada en el √≠ndice Davies-Boulding [7] que se
define tomando en cuenta una medida de similitud Rij entre 2 grupos Ci y Cj. La medida Rij se
define en base a una medida de dispersi√≥n dentro de cada grupo y un factor de similitud entre ambos
grupos. Rij debe ser posit√≠va y sim√©trica 5, y una simple elecci√≥n que satisface estas condiciones es:
Rij = (si + sj)/dij (2)
donde la medida de dispersi√≥n si se define como la m√°xima distancia que existe entre la media del
grupo Ci con un elemento dm ‚àà Ci y dij es la distancia que existe entre la media del grupo Ci con la
media del grupo Cj. Finalmente, si nc es el n√∫mero de clusters, se define el √≠ndice Davies-Boulding
(DB) como:
DBnc =
1
nc
nc‚àë
i=1
Ri (3)
con Ri = m√°xj=1...k, i6=jRij. Esta funci√≥n define, intuitivamente, la similitud promedio entre cada grupo
con su m√°s parecido. Uno desear√≠a encontrar una agrupaci√≥n en donde cada grupo sea lo m√°s diferente
posible con los dem√°s. Por ello, mientras menor sea el valor devuelto por esta funci√≥n, mejor es
nuestro modelo.
5. Resultados experimentales
Para llevar a cabo los experimentos se implementaron los algoritmos AntTree y DAntTree en el
lenguaje JAVA y se utiliz√≥ el paquete WEKA [13], que provee utilidades espec√≠ficas para clustering y
permite una f√°cil realizaci√≥n del estudio comparativo propuesto.
Como funci√≥n de similitud, se utiliz√≥ la funci√≥n de similitud de Gower:
Sim(i, j) =
‚àëp
k=1 wijksijk‚àëp
k=1wijk
(4)
donde sijk es la similitud que existe entre el elemento i y el j para el k-√©simo atributo. Cuando los
atributos son categ√≥ricos, se comparan por igualdad (sijk = 1 si son iguales y 0 en caso contrario). Para
datos continuos, sijk se define como sijk = 1‚àí|xik‚àíxjk|/Rk donde Rk es el rango de observaciones
del k-√©simo atributo. El t√©rminowijk ser√° 1 o 0, dependiendo de si la comparaci√≥n se considera v√°lida
o no. Por ejemplo, cuando falta el valor de uno de los 2 atributos, wijk ser√° igual a 0.
Para comparar los algoritmos se utilizaron cuatro bases de datos reales: breast-cancer-wisconsin
(bcw), eucalyptus (euca), heart-disease (heart) y thyroid-disease (thyroid), obtenidas del repositorio
del UCI 6. Tambi√©n se generaron tres bases de datos artificiales: art1, art2 y art3 que var√≠an en el
5Se debe cumplir que a) Rij ‚â• 0, b) Rij = Rji y c) si si = 0 y sj = 0 entonces Rij = 0
6Repositorio de bases de datos para Aprendizaje de M√°quina de la Universidad de California en Irvine (UCI) [3].
grado de proximidad que tienen los grupos dentro de los datos: art1 grupos bien separados, art2 clara
separaci√≥n pero menor que en art1 y art3 con grupos muy cercanos.
Las principales caracter√≠sticas de estas bases de datos se resumen en la tabla 1, especific√°ndose
para cada una de ellas el n√∫mero de instancias (NI), n√∫mero de atributos (NA), tipo de los atributos
(TA, categ√≥ricos o num√©ricos), n√∫mero de registros con valores faltantes (VF), n√∫mero de clases (K)
y distribuci√≥n de las instancias para cada una de las clases (DI).
Base de Datos NI NA TA VF K DI
bcw 699 9 cat. 16 2 „Äà458, 241„Äâ
euca 736 19 cat. y num. 141 5 „Äà180, 107, 130, 214, 105„Äâ
heart 303 13 cat. y num. 0 5 „Äà164, 55, 36, 35, 13„Äâ
thyroid 1505 5 num. 0 3 „Äà1050, 245, 210„Äâ
arti, i = 1, 2, 3 1002 31 num. 0 3 „Äà334, 334, 334„Äâ
Tabla 1: Caracter√≠sticas de las bases de datos utilizadas en la experimentaci√≥n
Para todas las bases de datos presentadas previamente, se conoce el cluster al que pertenece cada
instancia 7. Por lo tanto, se pueden evaluar los distintos algoritmos respecto a sus clases verdaderas.
Una medida de este tipo, es el error de clasificaci√≥n Ec utilizado en [2]. En este caso, asumamos
que ki es la clase verdadera del objeto di, k‚Ä≤i es la clase encontrada por los m√©todos para el objeto di.
K es la cantidad de grupos verdaderos y K ‚Ä≤ es la cantidad de grupos encontrados por los m√©todos.
Podemos entonces definir a Ec como:
Ec = 2N(N ‚àí 1)
‚àë
(i,j)‚àà{1,.......N}2, i<j
ij (5)
donde:
ij =
{
0 si (ki = kj ‚àß k‚Ä≤i = k‚Ä≤j) ‚à® (ki 6= kj ‚àß k‚Ä≤i 6= k‚Ä≤j),
1 en otro caso.
Debemos notar que el valor de K (n√∫mero real de clusters) debe ser especificado como par√°metro
para el caso del m√©todoK-means utilizado en los experimentos. Los restantes algoritmos desconocen
este valor.
En la tabla 2 se muestran los resultados comparativos obtenidos con K-means, AntTree y DAntTree. Se incluye en la misma el error de clasificaci√≥n Ec definido de acuerdo a la ecuaci√≥n 5, el
n√∫mero de clusters encontrados K ‚Ä≤ y la funci√≥n de Davies-Boulding (DB) correspondiente a la ecuaci√≥n 3. Tambi√©n se muestra la funci√≥n Global Silhouette promedio definida en base a la ecuaci√≥n 1
como: GS = (‚àëNi=1 s(i))/N . Debemos destacar que en todos los casos, el valor K ‚Ä≤ del m√©todo
K-means corresponde al valor de K que es especificado como par√°metro.
Del an√°lisis de estos resultados podemos destacar los siguientes puntos:
Respecto al error de clasificaci√≥n Ec, DAntTree produce mejores resultados que AntTree para
todas las bases de datos. En particular, DAntTree no es afectado tan severamente como AntTree
cuando los l√≠mites de los clusters no est√°n claramente delimitados (observar las bases de datos
euca, heart, art2 y art3). En general, los resultados obtenidos con DAntTree son comparables
a los de K-means y en algunos casos mejor como es el caso de las bases de datos euca, art1 y
art3.
Para distribuciones de datos en donde existe un limite demarcado entre grupos (como es el caso
de las bases de datos artificiales), el m√©todo DAntTree encuentra soluciones √≥ptimas.
7Obviamente esta informaci√≥n no es utilizada por los algoritmos.
Base de Datos Algoritmo Ec K ‚Ä≤ DB GS
K-mean 0.04 2 1.89 0.58
bcw AntTree 0.21 10 4.85 0.4
DAntTree 0.11 2 1.56 0.47
K-mean 0.31 5 2.38 0.27
euca AntTree 0.78 1 0 0
DAntTree 0.27 13 1.02 0.64
K-mean 0.33 5 2.38 0.24
heart AntTree 0.65 1 0 0
DAntTree 0.37 2 1.87 0.29
K-mean 0 3 1.85 0.62
thyroid AntTree 0.47 1 0 0
DAntTree 0.31 2 1.63 0.68
K-mean 0. 28 3 3.37 0.41
art1 AntTree 0. 22 2 0.48 0.72
DAntTree 0 3 0. 31 0.87
K-mean 0 3 0.42 0.83
art2 AntTree 0. 67 1 0 0
DAntTree 0 3 0.42 0.83
K-mean 0.28 3 4.07 0.1
art3 AntTree 0. 67 1 0 0
DAntTree 0 3 0. 38 0.83
Tabla 2: Resultados obtenidos con las distintas bases de datos
DAntTree tiende a encontrar un n√∫mero de clusters K ‚Ä≤ m√°s cercano al n√∫mero real de clusters
K que en el caso del AntTree. En particular, para 4 de las 7 bases de datos, el valor de K ‚Ä≤ para
DAntTree coincidi√≥ con el de K.
Con respecto a las medidas DB y GS que intentan estimar la calidad del clustering, debemos
recordar que valores peque√±os de DB y altos de GS son deseables. En este sentido, el m√©todo
propuesto ha encontrado modelos con valores m√≠nimos de DB y m√°ximos de GS en relaci√≥n a
los dem√°s m√©todos en comparaci√≥n (AntTree yK-means), para la mayor√≠a de las bases de datos
consideradas. Si consideramos la medidaDB por ejemplo, DAntTree arroja mejores resultados
que K-means en todos los casos y que AntTree cuando este √∫ltimo arroja resultados no nulos.
Con respecto a la medida GS, DAntTree es superior a AntTree en todos los casos, y supera a
K-means en todas las bases de datos excepto bcw.
Los resultados obtenidos con DAntTree son comparables e incluso mejores para algunas de las
m√©tricas utilizadas en comparaci√≥n a los obtenidos por K-means. Este aspecto no deja de ser
relevante si se considera que K-means se ve beneficiado de conocer a priori el n√∫mero exacto
de clusters reales y que tanto AntTree como DAntTree no requieren de esta informaci√≥n.
Debemos notar que los tiempos de ejecuci√≥n que pudimos observar del m√©todo propuesto no difieren en general en forma muy significativa de los tiempos de ejecuci√≥n de su predecesor, el algoritmo
AntTree. Sin embargo, este aspecto requiere de un estudio m√°s detallado utilizando otras bases de
datos con grandes vol√∫menes de informaci√≥n.
6. Conclusiones y trabajo futuro
En este trabajo se present√≥ un nuevo algoritmo de clustering basado en el m√©todo AntTree, un
enfoque bio-inspirado cuyos fundamentos se encuentran en el comportamiento de auto-ensamblaje
observado en ciertas clases de hormigas. El algoritmo propuesto utiliza al AntTree b√°sico con sus par√°metros de tolerancia por similitud y diferencia modificados, posteriormente reacomoda las hormigas
y finalmente une los grupos resultantes seleccionando el mejor modelo obtenido.
La principal conclusi√≥n obtenida mediante este nuevo m√©todo es que, dada la capacidad de desconectarse de las hormigas de la estructura, hemos obtenido resultados m√°s precisos que el algoritmo
AntTree y comparables con otros m√©todos que requieren de mayor informaci√≥n previa.
Tambi√©n se ha mantenido una caracter√≠stica fundamental del AntTree ya que podemos interpretar
de distintas formas el modelo obtenido, y de esta manera visualizarlo como un agrupamiento particionado o un agrupamiento jer√°rquico. Este nuevo m√©todo tiene la propiedad fundamental de adaptarse a
distintas distribuciones de los datos, dado que partimos de peque√±os grupos y realizamos un proceso
de uni√≥n en base a la similitud entre los mismos.
Posibles extensiones a este trabajo incluyen la experimentaci√≥n del algoritmo DAntTree con bases
de datos con mayores vol√∫menes de informaci√≥n, incluyendo bases de datos documentales.
