Desempe√±o de tr√°fico tipo streaming en una red de datos simulada
ÔªøEste art√≠culo presenta un estudio e investigaci√≥n realizados,
en vinculaci√≥n al √°rea de redes de datos, en particular referido al desempe√±o en la transferencia de informaci√≥n. Describe el proceso sobre c√≥mo
es dicha transferencia sobre una red simulada por software. Realiza una
comparaci√≥n de los desempe√±os alcanzados para distintas velocidades de
transferencia de datos tipo streaming. Se analizan los resultados, en base
a una serie de par√°metros considerados como son el throughput, el delay
(retardo), el packet loss (p√©rdida de paquetes) y el jitter (variaci√≥n del
retardo). Se concluye con una reflexi√≥n sobre qu√© par√°metros afectan en
mayor o menor medida las transferencias realizadas.
Key words: CBR, throughput, delay, packet loss, jitter, P2P, tr√°fico
autosimilar, streaming, TCP
1. Introducci√≥n
Este trabajo est√° enmarcado en el contexto del desarrollo de una Tesis de
Maestr√≠a en Redes de Datos, de la Facultad de Inform√°tica de la UNLP. En
dicha tesis se analizan las herramientas para poder determinar cu√°l o cu√°les son
los mejores recursos a utilizar en una red P2P1 , en funci√≥n de su conectividad.
Se tiene como objetivo llegar a armar un ranking que ordene los nodos (que
tienen el recurso de streaming deseado), de acuerdo al nivel de conectividad que
se tiene con cada uno de estos y al tipo de recurso (voz, m√∫sica, video, video
en alta definici√≥n, etc.). Al mismo tiempo, para llegar a esta meta es necesario
analizar cu√°les son los par√°metros que influyen en el proceso transferencia de
datos, y de qu√© manera lo hacen.
1 Las redes P2P o peer-to-peer son sistemas distribuidos consistentes de nodos de
borde interconectados capaces de autoorganizarse dentro de topolog√≠as de redes con
el prop√≥sito de compartir recursos tales como contenidos, ciclos de CPU, almacenaje
y/o ancho de banda, capaces de adaptarse a las fallas y acomodarse a poblaciones
transitorias de nodos mientras mantienen una aceptable conectividad y rendimiento,
sin requerir intermediaci√≥n o soporte de un servidor o autoridad global centralizada.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1086
La velocidad de transferencia sobre un enlace de datos est√°, principalmente,
determinada por las caracter√≠sticas de dicho enlace y las capacidades de c√≥mputo
de los sistemas intermedios, que llevan a cabo la comunicaci√≥n. El throughput, el
delay, el packet loss y el jitter son par√°metros que cuantifican dicha transferencia
[1].
El tr√°fico de datos es la secuencia de movimientos de elementos de datos a
trav√©s de diversos dispositivos f√≠sicos. El t√≠pico elemento de datos es una secuencia continua de bits que conforman un paquete de datos. Cuando estos paquetes
pasan a trav√©s de un dispositivo f√≠sico, se produce habitualmente una demora provocada por la recepci√≥n (generalmente acompa√±ada del encolamiento del
paquete), luego por el procesamiento (donde el paquete es analizado), y por
√∫ltimo, por el despacho (donde finalmente el paquete abandona el dispositivo).
Todo este procedimiento causa demoras. Generalmente, los arribos se encolan
sucesivamente como entradas de los clientes y all√≠, experimentan una posible
espera, para luego ser servidos antes de que se produzca su despacho exitoso
por la salida. Para poder construir un modelo del arribo y posterior servicio
se utilizan variables aleatorias. La naturaleza estad√≠stica de los arribos puede
ser expresada de diferentes formas. Por ejemplo, si los sucesivos tiempos entre
arribos (interarrival times ‚Äì IATs) son independientes, una especificaci√≥n de las
condiciones iniciales (que determine el instante de tiempo en el cual la operaci√≥n
de encolamiento comienza), en conjunto con la Funci√≥n de Densidad de Probabilidad (probability density function ‚Äì pdf) de los IATs, son suficientes para
describir completamente la naturaleza de los arribos. La variable aleatoria con
distribuci√≥n de Pareto para los IATs permite armar uno de estos modelos. Esta
variable aleatoria exhibe algunas caracter√≠sticas importantes, seg√∫n los valores
de los par√°metros de su pdf. Su varianza puede ser finita o infinita. Las variables
aleatorias con varianza infinita encuentran aplicaci√≥n en la caracterizaci√≥n de
tr√°fico de datos a r√°fagas [2].
Estos aspectos te√≥ricos se exponen aqui con el fin de dar contexto a los
experimentos que luego se describen en este trabajo, y que retomar√°n este marco.
A continuaci√≥n se hace una breve revisi√≥n de algunos enfoques que justifican la
utilizaci√≥n de este mecanismo para modelizar el tr√°fico.
2. Internet y el tr√°fico autosimilar
A partir de 1993, aparecieron varios estudios en la literatura que documentaban que el patr√≥n del tr√°fico de datos se puede modelar de manera correcta
por un proceso autosimilar, en una amplia variedad de situaciones relacionadas
con redes de datos del mundo real. Existen muchos ejemplos de tr√°fico que se
adecuan mejor a un modelo autosimilar que a uno de Poisson (que ven√≠a siendo
utilizado), entre los que se encuentra el tr√°fico Ethernet, el HTTP, TCP sobre
enlaces WAN, la parte interactiva de una conexi√≥n TELNET, la etapa de transferencia de datos sobre una conexi√≥n FTP. A partir de todos estos acontecimientos
es que cobra sentido armar un escenario de simulaci√≥n con tr√°fico autosimilar
para trazar un paralelismo con lo que pasa hoy en Internet [3].
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1087
El fen√≥meno de la autosimilaridad tiene un profundo impacto en el desempe√±o de las redes de datos. Estudios basados en la teor√≠a tradicional de colas
(que utilizan la distribuci√≥n de probabilidad de Poisson) muestran que el crecimiento brusco del delay en un sistema, comienza reci√©n cuando se alcanza una
utilizaci√≥n del 80 %, mientras que capturas de tr√°fico Ethernet e ISDN muestran que ese crecimiento brusco empieza antes, entre el 50 y el 60 %. Diferentes
investigaciones revelan que esos problemas de desempe√±o se deben a la autosimilaridad [3]. Uno de los descubrimientos m√°s importantes que se realiz√≥ sobre
Ethernet, es que a medida que la carga aumenta, el grado de autosimilaridad
aumenta. Este resultado es sin duda muy importante, ya que es justamente con
mucha carga, donde las cuestiones relacionadas al desempe√±o toman un papel
relevante. A partir de estos resultados, se empez√≥ a abandonar la teor√≠a de que
la agregaci√≥n de tr√°fico de datos generado a partir de la multiplexaci√≥n un gran
n√∫mero de fuentes independientes, resulta en un proceso de Poisson.
La existencia de tr√°fico autosimilar en los backbones, producto de la agregaci√≥n de flujos de m√∫ltiples fuentes, incide en el desempe√±o de los routers, ya que
estos perciben que los paquetes llegan de a r√°fagas. Dichas r√°fagas degradan el
desempe√±o porque deben procesar mucha informaci√≥n de golpe, lo que agrega
demoras al procesamiento de los paquetes y termina incrementando el delay, o
debido a este incremento se produce el llenado de las colas en los routers lo
que produce el descarte de paquetes y su consecuente degradaci√≥n en los flujos
individuales. Todo este fen√≥meno tiene un impacto directo en el tama√±o de los
buffers, con los que los routers deben contar, como para poder hacer frente a los
requerimientos del tr√°fico en situaciones de alta carga. No es la intenci√≥n de este
trabajo determinar cu√°l es el tama√±o que esos buffers deben tener [4], pero s√≠ ver
c√≥mo influye la existencia del tr√°fico autosimilar en un flujo individual entre dos
hosts en los bordes de la red.
3. Metodolog√≠a para el desarrollo de la investigaci√≥n
Luego de abordar el estudio te√≥rico de la tem√°tica planteada, se dise√±o√≥ un
trabajo experimental con el fin de analizar los diferentes par√°metros que intervienen en la transmisi√≥n de informaci√≥n en una red de datos, y de esta manera,
generar una estrategia adecuada para el armado del ranking de nodos, que se
mencion√≥ inicialmente en este art√≠culo.
Para el an√°lisis emp√≠rico de las transferencias se utiliz√≥ un simulador de
redes de datos discreto llamado Network Simulator [5] en su versi√≥n 2, que se
nombrar√° a partir de ahora como ns2. El tr√°fico que se us√≥ para la simulaci√≥n es
del tipo CBR (Constant Bit Rate), ya que es acorde al tipo de flujo que generan
las aplicaciones de streaming.
En lo que resta de este art√≠culo se describir√°: c√≥mo se dise√±o√≥ y planific√≥ el
escenario de simulaci√≥n y c√≥mo se llevaron a cabo las mismas. Se presentar√°n
los resultados que se obtuvieron de manera gr√°fica para poder verlos comparativamente. Finalmente, se realizar√° una interpretaci√≥n de dichos resultados.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1088
4. Escenario de simulaci√≥n
La generaci√≥n del core de la red (un grafo de 200 nodos) se llev√≥ a cabo usando
un software que de manera aleatoria dispuso los nodos y los enlaces entre ellos, de
forma que el grafo quede totalmente conexo. Los routers (nodos) tuvieron todos
un grado mayor o igual a uno lo que garantiz√≥ que todos los nodos estuvieran
conectados con al menos uno de los otros nodos. Los enlaces (aristas) tuvieron
asignados aleatoriamente la velocidad de conexi√≥n full-duplex entre 3 y 4 Mbps,
y el delay menor a 3 milisegundos. Por otro lado, se conectaron dos host (nodos
con un √∫nico v√≠nculo a la nube de routers), que fueron los encargados de realizar
las transferencias de streaming, uno como emisor y el otro como receptor. Estos
hosts se conectaron v√≠a un enlace con una velocidad de conexi√≥n full-duplex de
2 Mbps y un delay de 10 milisegundos. El tr√°fico entre el emisor y el receptor
fue encaminado a trav√©s de siete saltos, de entre los routers de la red.
5. Generaci√≥n sint√©tica de tr√°fico autosimilar
El tr√°fico autosimilar fue modelizado mediante la agregaci√≥n de fuentes ONOFF que utilizan la distribuci√≥n de Pareto, mencionada en la introducci√≥n de
este trabajo. Los paquetes son enviados a velocidad fija durante los per√≠odos
ON, y no se env√≠an paquetes durante los per√≠odos de OFF. Ambos per√≠odos son
tomados de dos variables aleatorias independientes con distribuci√≥n de Pareto,
teniendo un tama√±o de paquete fijo. Estas fuentes pueden ser usadas para generar tr√°fico agregado que exhiba Dependencia de Largo Alcance (Long Range
Dependency - LRD).
En la simulaci√≥n sobre ns2 se utiliz√≥ un tiempo de r√°faga inicial (ON) de 12
segundo, un tiempo ocioso inicial (OFF) de 12 segundo, una velocidad constante
de transferencia de 200 Kbps, un tama√±o de paquete fijo de 210 bytes, y se
pas√≥ como par√°metro de forma de la distribuci√≥n de Pareto 1,5 [6] (lo que hace
que tenga una varianza infinita).
6. Ejecuci√≥n de la simulaci√≥n
Una vez creado el grafo se procedi√≥ a iniciar, en cada nodo, una fuente de
transmisi√≥n de datos con distribuci√≥n de Pareto comenzando en un instante
aleatorio. De esta forma, se obtiene un escenario de tr√°fico autosimilar sobre el
que se cursan los datos a medir. Para poder analizar los factores involucrados
en el tiempo de transferencia, en la investigaci√≥n, se llevaron a cabo una serie
de simulaciones con ns2. Se eligieron ciertos escenarios que a continuaci√≥n se
describen.
Para desarrollar el an√°lisis emp√≠rico a trav√©s de la simulaci√≥n se armaron
tres categor√≠as de tr√°fico, seg√∫n la velocidad a la que se inyectaron los datos en
la comunicaci√≥n. Las tres categor√≠as usaron un tr√°fico del tipo CBR (Constant
Bit Rate), a diferentes velocidades. Una funcion√≥ a 400Kbps, otra a 1Mbps y la
otra a 1,5Mbps. El prop√≥sito de tener las diferentes categor√≠as es ver qu√© ocurre
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1089
con aplicaciones que requieren esas velocidades de transferencia para funcionar
de manera correcta. Dicha simulaci√≥n se llev√≥ a cabo para distintos vol√∫menes
de datos comenzando con 1KByte hasta 600KBytes iterando con un incremento
de 1KByte por simulaci√≥n, para cada una de las tres velocidades que fueron
mencionadas antes. Por cada simulaci√≥n se gener√≥ un archivo con la traza y se
calculan las m√©tricas que se mencionan a continuaci√≥n.
7. M√©tricas o par√°metros considerados en la simulaci√≥n
A continuaci√≥n se listan las m√©tricas utilizadas y la forma en que se calcularon
para el flujo CBR:
Throughput:
T =
successful data
transmision time
. (1)
Delay Promedio extremo-a-extremo:
D = (Td ‚àí Ts);Dp = (1/pkts)
pkts‚àë
i=1
Di . (2)
donde, Td es el instante de recepci√≥n del paquete en el destino y Ts es el
instante de emisi√≥n del paquete en el origen. Si Td no existe, porque el paquete
no llega a destino, D no se puede calcular.
Jitter M√°ximo:
J = |Di+1‚ÄìDi|; Jm = MAX(Ji) i[1, pkts] . (3)
donde, Di es el delay del i-√©simo paquete y Di+1 el delay del siguiente. Tanto
Di como Di+1 deben ser calculables para que J se pueda calcular.
Packet Loss:
L = [(
drop pkts
sent pkts
)100] % . (4)
A partir de la ejecuci√≥n de la simulaci√≥n planteada, se abord√≥ el an√°lisis
de resultados con el fin de considerar c√≥mo impactan los diferentes par√°metros
tenidos en cuenta, y c√≥mo esta informaci√≥n resulta relevante a los fines planteados
de realizar el ranking de nodos que poseen el recurso deseado. A continuaci√≥n se
detallan los resultados, organiz√°ndolos a partir de las m√©tricas consideradas.
8. Resultados de la simulaci√≥n
A partir de la traza generada por la ejecuci√≥n de las sucesivas simulaciones,
se calcularon las m√©tricas antes mencionadas.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1090
Figura 1. El eje de las x denota el tama√±o de la transferencia, que va de 1 a 600 Kbytes,
mientras que el eje de las y determina el throughput de cada una de las transferencias.
Cada color indica una velocidad de transmisi√≥n distinta y tiene su referencia en la parte
superior derecha de la figura.
8.1. Throughput
La Fig. 1, permite observar el throughput. Ninguno de los 3 casos alcanza un
nivel satisfactorio, ya que est√° siempre muy por debajo del desempe√±o te√≥rico
para el enlace. Para el caso de las transferencias a 400 Kbps, parecer√≠a estabilizarse en torno a los 350 Kbps, lo que resulta en el rendimiento m√°s aceptable.
Para los casos de 1 y 1,5 Mbps, promedia en la mitad de lo pretendido, lo que es
a√∫n peor que en el caso de 400 Kbps. Se puede vislumbrar que el nivel de packet
loss tiene que ser significativo, porque los valores reales del throughput est√°n
muy lejos de los deseados. Para lograr una calidad aceptable de streaming por
estos canales se requiere un buffering importante en el receptor, meticulosamente
calculado, y que demandar√≠a una espera inicial prolongada.
8.2. Delay
Como se puede observar, a trav√©s de la Fig. 2, el delay tiene un m√°ximo
relativamente constante en todas las categor√≠as, y tiene algunos picos hacia abajo
que son especialmente significativos en la conexi√≥n de 400 Kbps. Esto permite
deducir que la espera en los routers es, usualmente, la misma para cada uno de
los paquetes, y que tiene ciertos momentos m√°s aliviados en la transferencia de
400 Kbps, que no aparecen con las otras dos velocidades de 1 y 1,5 Mbps.
8.3. Packet Loss
La Fig. 3, muestra que la p√©rdida de paquetes es muy alta en todos los
casos, y empeora a medida que aumenta la velocidad de transferencia. Esto
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1091
Figura 2. El eje de las x denota el tama√±o de la transferencia, que va de 1 a 600
Kbytes, mientras que el eje de las y determina el delay promedio de cada una de
las transferencias. Cada color indica una velocidad de transmisi√≥n distinta y tiene su
referencia en la parte superior derecha de la figura.
Figura 3. El eje de las x denota el tama√±o de la transferencia, que va de 1 a 600
Kbytes, mientras que el eje de las y determina el porcentaje de packet loss de cada una
de las transferencias. Cada color indica una velocidad de transmisi√≥n distinta y tiene
su referencia en la parte superior derecha de la figura.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1092
es coincidente con la merma producida en el throughput. Este es uno de los
principales enemigos del streaming, y requiere de un protocolo que sea resistente
a las p√©rdidas, que implemente mecanismos de retransmisi√≥n o de recuperaci√≥n
de estas situaciones.
8.4. Jitter
Figura 4. El eje de las x denota el tama√±o de la transferencia, que va de 1 a 600 Kbytes,
mientras que el eje de las y determina el jitter m√°ximo de cada una de las transferencias.
Cada color indica una velocidad de transmisi√≥n distinta y tiene su referencia en la parte
superior derecha de la figura.
El jitter, como ya se mencion√≥, se define como la variaci√≥n del retardo, por
lo que un delay relativamente constante refleja un jitter, tambi√©n, relativamente
constante y bajo. La figura 4 muestra c√≥mo se comportan los peores casos, y
siempre est√°n por debajo de los 40 ms, lo que es un resultado bueno para una
aplicaci√≥n de tipo streaming.
Este an√°lisis acerca de c√≥mo se comportan las diferentes m√©tricas en el escenario simulado, con las consideraciones enunciadas, permite arribar a las primeras
conclusiones de la investigaci√≥n, que no son m√°s que un punto inicial para continuar el camino hacia el objetivo final planteado. En el siguiente apartado, se
enuncian algunas de estas conclusiones preliminares.
9. Conclusiones
A partir de las simulaciones realizadas, se puede observar considerando los
gr√°ficos de las figuras presentadas, c√≥mo influye el tr√°fico autosimilar agregado
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1093
en cada una de las m√©tricas calculadas, siendo el packet loss y, por consiguiente,
el Throughput los dos m√°s afectados negativamente. Este an√°lisis es v√°lido debido a la amplia variedad de protocolos que tienen en el tr√°fico que generan un
fuerte componente autosimilar estad√≠stico, c√≥mo ya se mencionaron anteriormente diversos ejemplos. Sin embargo, esto no invalida otra diversidad de an√°lisis
que puedan hacerse utilizando la teor√≠a de colas tradicional que puede utilizarse
para modelar otras tantas situaciones donde el tr√°fico se adecua mejor a dicho
modelo.
Este trabajo se enfoca en ver c√≥mo es afectado un flujo de streaming por
el resto del tr√°fico. A diferencia del tr√°fico TCP, que se autoregula gracias a
sus mecanismos para evitar la congesti√≥n, el rate de inserci√≥n en el streaming
es CBR. Adem√°s, se enmarca en la influencia que tiene el tr√°fico autosimilar
que circula por la red. La elecci√≥n de llevar a cabo la simulaci√≥n en una red
cuyo tr√°fico es autosimilar, se debe al gran volumen de tr√°fico autosimilar que
hay en Internet. El tr√°fico P2P, as√≠ como la World Wide Web (HTTP), tiene
un comportamiento autosimilar. A partir de esto resulta de inter√©s, modelar el
tr√°fico de Internet (o por lo menos una parte grande de ella) como autosimilar.
A la luz de los resultados, se puede ver que existe una relaci√≥n entre el
throughpu t tan bajo que se puede observar en todos los casos, y un packet loss
alto. En los casos de 1 y 1,5 Mbps el descarte de paquetes oscila entre el 20 % y
el 45 %. Estos dos par√°metros muestran que hay un alto grado de congesti√≥n en
la red.
Los tres casos podr√≠an no ser los ideales para realizar una transferencia de
tipo streaming, ya que requerir√≠an de un alto grado de buffering para amortiguar
las malas condiciones del enlace. Al principio se plante√≥ el objetivo de armar un
ranking de nodos ordenado seg√∫n la conveniencia en utilizarlos para hacerse del
servicio de streaming deseado, pero como por ahora se realiz√≥ la simulaci√≥n con
una √∫nica fuente s√≥lo podemos adelantar, que en esta etapa de la investigaci√≥n
todav√≠a falta verificar otras fuentes para poder armar el ranking y elegir.
Como se mencion√≥ con anterioridad, estos primeros resultados y conclusiones,
constituyen elementos centrales para continuar el camino de la investigaci√≥n
planteada.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACI√ìN 1094
