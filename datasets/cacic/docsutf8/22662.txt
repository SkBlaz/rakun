Un m√©todo para la fragmentaci√≥n vertical de bases de datos y su variante como evaluador de particiones
Ôªø El dise√±o de bases de datos distribuidas es un problema de 
optimizaci√≥n que implica la soluci√≥n de problem√°ticas como la fragmentaci√≥n 
de los datos y su ubicaci√≥n. T√≠picamente, los criterios que determinan si la 
fragmentaci√≥n y la asignaci√≥n son √≥ptimas se establecen de manera 
independiente. Primero se busca la ‚Äúmejor‚Äù fragmentaci√≥n y luego la ‚Äúmejor‚Äù 
ubicaci√≥n de los fragmentos obtenidos. La fragmentaci√≥n vertical es m√°s 
complicada que la partici√≥n horizontal, debido al incremento del n√∫mero de 
posibles alternativas. En este trabajo se presenta un nuevo m√©todo para la 
fragmentaci√≥n vertical, que se basa fundamentalmente en la Matriz de 
Atracci√≥n entre Atributos, suplantando la conocida Matriz de Afinidad entre 
Atributos. Se utiliza como heur√≠stica el enfoque de agrupamientos jer√°rquicos y 
una regla de decisi√≥n basada en la homogeneidad interna y la heterogeneidad 
externa de los grupos obtenidos. Tambi√©n se presenta una variante para que 
pueda ser usado como evaluador de particiones. 
Palabras claves: Bases de datos distribuidas, evaluador de particiones, 
fragmentaci√≥n vertical, medida de afinidad, m√©todo jer√°rquico, regla de 
decisi√≥n. 
1   Introducci√≥n 
El inter√©s por el desarrollo de bases de datos distribuidas relacionales ha aumentado 
en la medida que las organizaciones y empresas han crecido y se han expandido 
geogr√°ficamente; de la mano del vertiginoso progreso en el uso de redes de 
computadores. El dise√±o de bases de datos distribuidas es un problema de 
optimizaci√≥n que implica la soluci√≥n de problem√°ticas como la fragmentaci√≥n de los 
datos, su ubicaci√≥n y replicaci√≥n. 
La fragmentaci√≥n es el proceso mediante el cual una relaci√≥n global es 
descompuesta en fragmentos horizontales y/o verticales. Un fragmento vertical 
atiende al agrupamiento de datos en funci√≥n de atributos o conjuntos de ellos, 
mientras que la fragmentaci√≥n horizontal atiende a dicho agrupamiento en funci√≥n de 
                                                          
 
 
tuplas o conjuntos de tuplas. T√≠picamente, los criterios que determinan si la 
fragmentaci√≥n y la asignaci√≥n son √≥ptimas se establecen de manera independiente, en 
dos pasos. En el primero se busca la ‚Äúmejor‚Äù fragmentaci√≥n y, en el segundo, se busca 
la ‚Äúmejor‚Äù ubicaci√≥n de los fragmentos obtenidos en el paso anterior [1]. 
Comparando las formas de fragmentaci√≥n, la partici√≥n vertical es m√°s complicada 
que la partici√≥n horizontal, debido al incremento del n√∫mero de posibles alternativas 
[2]. Como se se√±ala en [3], un objeto con m atributos puede ser particionado de B(m) 
diferentes formas, donde B(m) es el m-√©simo n√∫mero de Bell, para m suficientemente 
grandes, B(m) se aproxima a mm; para m=15 este es ‚âà 109, para m=30 este es ‚âà 1023. 
Por lo tanto, es importante contar con una estrategia que reduzca de manera 
eficiente el n√∫mero de c√°lculos. Aunque diferentes, dos enfoques secuenciales que 
conducen a agrupamientos jer√°rquicos parecen haber adquirido un inter√©s particular 
entre los tax√≥nomos. Una de las estrategias es el algoritmo propuesto por Ward 
(1963). Su idea es aglomerar los puntos o los grupos resultantes, reduciendo su 
n√∫mero en uno en cada etapa  de un procedimiento de fusi√≥n secuencial, hasta que 
todos los puntos est√©n en un √∫nico cl√∫ster. Un algoritmo contrario ha sido propuesto 
por Edwards y Cavalli-Sforza (1965). La esencia de su m√©todo es la partici√≥n 
consecutiva de un conjunto de puntos en dos subconjuntos: primero un conjunto 
inicial es dividido en dos grupos, cada uno de ellos se subdivide en dos grupos m√°s 
peque√±os por separado, y as√≠ sucesivamente, hasta que se alcancen los puntos 
individuales [4]. 
En la presente investigaci√≥n se utiliza el enfoque propuesto por Ward, aunque el 
m√©todo propuesto se puede implementar independientemente con cualquiera de los 
dos enfoques. 
En la mayor√≠a de las situaciones de agrupaci√≥n de la vida real, un investigador 
aplicado se enfrenta con el dilema de seleccionar el n√∫mero de grupos o particiones 
en la soluci√≥n final. Los procedimientos no jer√°rquicos suelen exigir que el usuario 
especifique este par√°metro antes de que la agrupaci√≥n se lleve a cabo y los m√©todos 
jer√°rquicos habitualmente producen una serie de soluciones que van desde n grupos 
hasta una soluci√≥n con un √∫nico cl√∫ster presente (asumir n objetos en el conjunto de 
datos). Cuando aplicamos para los resultados m√©todos de agrupaci√≥n jer√°rquicos, las 
t√©cnicas para la determinaci√≥n del n√∫mero de grupos en un conjunto de datos son 
muchas veces referidas como reglas de decisi√≥n [5]. 
El dilema de una regla de decisi√≥n, que en ingl√©s se denomina com√∫nmente 
‚Äústopping rule‚Äù, es clave ya que en su soluci√≥n descansa la decisi√≥n correcta sobre 
nuestra estructura grupal [6]. 
Algunos procedimientos propuestos presentan problemas, ya que sugieren reglas 
de decisi√≥n no autom√°ticas por lo que no eliminan el tema de la subjetividad humana, 
otros son m√©todos gr√°ficos que requieren el juicio del investigador y en otros casos 
son √≠ndices con par√°metros de control que no han sido totalmente definidos o 
desarrollados. Otra deficiencia presente en algunas reglas de decisi√≥n es su 
incapacidad de operar cuando el agrupamiento √≥ptimo resulta ser k=1 (los n 
elementos deben pertenecer a un √∫nico cl√∫ster) o k=n (cada uno de los n elementos 
debe permanecer en un cl√∫ster individualmente). Y por √∫ltimo, otra dificultad que 
presentan m√©todos anteriormente propuestos, es el nivel de c√≥mputo asociado al 
procedimiento. 
Por tanto, el objetivo del presente trabajo es proponer un m√©todo sencillo, que 
incluya una regla de decisi√≥n, para la partici√≥n vertical de bases de datos; que 
garantice un √≠ndice preciso, que elimine la subjetividad humana del proceso, que no 
sea susceptible a los casos extremos de k=1 y k=n, anteriormente explicados, con un 
nivel de c√≥mputo aceptable, que iguale y/o mejore los resultados obtenidos mediante 
el m√©todos anteriores. Tambi√©n se presenta una variante de dicho m√©todo para que 
pueda ser usado como evaluador de particiones. 
2   Metodolog√≠a 
Como la inmensa mayor√≠a de los algoritmos previos para la partici√≥n vertical de bases 
de datos se usar√° la Matriz de Uso de Atributos (MUA) como entrada. Esta matriz 
relaciona las transacciones con los atributos de la relaci√≥n y contendr√° un 1 en una de 
sus celdas si el atributo Ai es utilizado en la Transacci√≥n Tj o un cero en caso 
contrario. Esta matriz cuenta con una √∫ltima columna reservada para  las frecuencias 
de acceso de cada transacci√≥n, es decir, el n√∫mero de veces que la transacci√≥n se 
solicita en un intervalo de tiempo definido. Se utilizar√° un ejemplo que considera 8 
transacciones y 10 atributos que producen la MUA que se aprecia en la Tabla I, este 
ejemplo es utilizado en [7][8][9]. 
Tabla 1.  Ejemplo de Matriz de Uso de Atributos de dimensi√≥n 10x8.  
ref A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 acc 
T1 1 0 0 0 1 0 1 0 0 0 25 
T2 0 1 1 0 0 0 0 1 1 0 50 
T3 0 0 0 1 0 1 0 0 0 1 25 
T4 0 1 0 0 0 0 1 1 0 0 35 
T5 1 1 1 0 1 0 1 1 1 0 25 
T6 1 0 0 0 1 0 0 0 0 0 25 
T7 0 0 1 0 0 0 0 0 1 0 25 
T8 0 0 1 1 0 1 0 0 1 1 15 
 
Hoffer y Severance en [10] proponen el concepto de afinidad entre pares de 
atributos. Aplicando este concepto a la MUA se obtiene la Matriz de Afinidad entre 
atributos (MAA), que es lo que se propone en los 2 primeros pasos del M√©todo 
Navathe como se indica en [11]. Sin embargo, muchos autores han criticado el uso de 
esta matriz. Sharma Chakravarthy, Jaykumar Muthuraj, Ravi Varadarajan y Shamkant 
B. Navathe en [8] aseguran que, debido a que solo un par de atributos son 
involucrados, esta medida no refleja la cercan√≠a o afinidad cuando m√°s de dos 
atributos son implicados. Aunque en este trabajo se comparte el enfoque de Jun Du, 
Ken Barker y Reda Alhajj, quienes fundamentan en [7] las limitaciones de la medida 
de afinidad como una medida de afinidad local y la necesidad de una medida de 
afinidad global para lograr que todos los valores de la matriz sean comparables entre 
s√≠. No obstante, para esta investigaci√≥n, no se considera consistente la medida de 
afinidad global descrita en el trabajo de estos autores, por lo que se realiz√≥ un an√°lisis 
de varias de las medidas de afinidad existentes y que son revisadas por el Doctor en 
Ciencias Biol√≥gicas Alejandro Herrera Moreno en [6]. Se decide que el √çndice de 
Jaccard, una expresi√≥n de similitud, es una medida de afinidad global apropiada para 
el tema de la partici√≥n vertical de bases de datos, eliminando de su f√≥rmula el factor 
que representa las ausencias conjuntas o ceros compartidos, debido a que el hecho de 
que en una transacci√≥n no se usen ninguno de los dos atributos del par analizado, no 
brinda ninguna informaci√≥n para el caso que ocupa. A continuaci√≥n la ecuaci√≥n del 
√çndice de Jaccard que se sugiere: 
 
S = a/(a + b + c) . (1) 
 
donde: 
S: valor de atracci√≥n entre el atributo Ai y Aj, 
a: suma de las frecuencias de aquellas transacciones que usan tanto el atributo Ai 
como el  Aj, 
b: suma de las frecuencias de aquellas transacciones que usan el atributo Ai y no el 
Aj y 
c: suma de las frecuencias de aquellas transacciones que usan el atributo Aj y no el 
Ai. 
Al aplicar la medida de afinidad global seleccionada, se obtiene una nueva matriz, 
se nombrar√° Matriz de Atracci√≥n entre Atributos (MAA*).  
Tabla 2.  Ejemplo de Matriz de Atracci√≥n entre Atributos de dimensi√≥n 10x10.  
Sij A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 
A1 1 0,156 0,15 0 1 0 0,45 0,156 0,15 0 
A2 0,156 1 0,5 0 0,156 0 0,4 1 0,5 0 
A3 0,15 0,5 1 0,107 0,15 0,107 0,142 0,5 1 0,107 
A4 0 0 0,107 1 0 1 0 0 0,107 1 
A5 1 0,156 0,15 0 1 0 0,45 0,156 0,15 0 
A6 0 0 0,107 1 0 1 0 0 0,107 1 
A7 0,45 0,4 0,142 0 0,45 0 1 0,4 0,142 0 
A8 0,156 1 0,5 0 0,156 0 0,4 1 0,5 0 
A9 0,15 0,5 1 0,107 0,15 0,107 0,142 0,5 1 0,107 
A10 0 0 0,107 1 0 1 0 0 0,107 1 
 
Como se puede apreciar, la matriz es sim√©trica, los valores quedan normalizados 
entre cero y uno, donde uno representa el valor m√°ximo de similitud y cero el 
m√≠nimo, por lo que todos los valores son comparables entre s√≠. Es obvio que la 
diagonal principal est√© formada por unos, debido a que la atracci√≥n de un atributo 
consigo mismo es m√°xima. 
A partir de este punto se comienza a aplicar el m√©todo de agrupamiento jer√°rquico 
que propone Ward (1963). Recordemos que este procedimiento parte de que los k 
atributos est√©n individualmente en un grupo y en cada etapa reduce el n√∫mero de 
grupos en 1, mediante fusiones, hasta que todos los atributos se encuentren en un 
√∫nico cl√∫ster. Para esta altura, se habr√°n obtenido k-1 posibles agrupaciones. Tras 
cada fusi√≥n deben ser recalculados los valores de ‚Äúatracci√≥n‚Äù entre grupos-atributos o 
pares de atributos que provoca el nuevo grupo reci√©n formado. 
Lance y Williams. Boesch (1977) citan ocho estrategias aglomerativas que pueden 
ser utilizadas con este prop√≥sito, entre las que se encuentran: ligamiento simple o 
vecino m√°s cercano, ligamiento completo o vecino m√°s lejano, promedio simple, 
promedio de grupos y estrategia flexible. El ‚Äúpromedio simple‚Äù es un m√©todo que se 
considera como conservativo del espacio ya que introduce poca distorsi√≥n en las 
afinidades originales, propiedad que la hace una estrategia muy recomendada [6]. Por 
tal motivo esta es la estrategia seleccionada para el presente m√©todo. A continuaci√≥n, 
en la Tabla 3, se presenta en resumen los resultados obtenidos al aplicar los procesos 
antes mencionados al ejemplo desarrollado. 
Tabla 3.  Resultados de aplicar el m√©todo de agrupamiento de Ward con la estrategia 
aglomerativa ‚ÄúPromedio Simple‚Äù.  
#  de 
fusiones 
#  de 
grupos 
Grupos Valores de fusi√≥n 
0 10 (A1)(A2)(A3)(A4)(A5)(A6)(A7)(A8)(A9)(A10) - 
1 9 (A1, A5)(A2)(A3)(A4)(A6)(A7)(A8)(A9)(A10) 1 
2 8 (A1, A5)(A2, A8)(A3) (A4)(A6)(A7)(A9)(A10) 1 
3 7 (A1, A5)(A2, A8)(A3, A9)(A4)(A6)(A7)(A10) 1 
4 6 (A1, A5)(A2, A8)(A3, A9)(A4, A6)(A7)(A10) 1 
5 5 (A1, A5) (A2, A8) (A3, A9) (A4, A6, A10) (A7) 1 
6 4 (A1, A5)(A2, A8, A3, A9) (A4, A6, A10) (A7) 0,5 
7 3 (A1, A5, A7) (A2, A8, A3, A9) (A4, A6, A10)  0,45 
8 2 (A1, A5, A7, A2, A8, A3, A9) (A4, A6, A10) 0,212 
9 1 (A1, A5, A7, A2, A8, A3, A9, A4, A6, A10) 0,02675 
 
N√≥tese que los valores de fusi√≥n decrecen a medida que aumenta el n√∫mero de 
fusiones debido a que siempre se selecciona el mayor valor de ‚Äúatracci√≥n‚Äù para 
realizar la fusi√≥n. El principal problema a enfrentar ahora es escoger cu√°l de estos 
agrupamientos parece ser mejor, aqu√≠ es donde juega un papel fundamental la ‚Äúregla 
de decisi√≥n‚Äù. 
Muchos intentos por definir qu√© es un grupo emplean propiedades como la 
cohesi√≥n interna y el aislamiento externo lo cual est√° m√°s cerca de la definici√≥n de 
clasificaci√≥n que pretende de manera objetiva crear grupos muy homog√©neos entre s√≠ 
y bien diferentes de otros. Esto es lo que nos dicen Hair, Anderson, Tatham y Black 
en [12] cuando explican que los grupos deben poseer una homogeneidad interna muy 
alta (‚Äúwithincluster‚Äù) y una heterogeneidad externa (‚Äúbetweencluster‚Äù) tambi√©n muy 
alta [6]. 
Basado en estos principios en el presente art√≠culo se proponen varias ecuaciones 
para calcular la Homogeneidad Interna (HI) y la Heterogeneidad Externa (HE). A 
continuaci√≥n se muestra la f√≥rmula para calcular la HI de un grupo espec√≠fico: 
 
ùêªùêªùêªùêªùëôùëô = ÔøΩ
1 ùë†ùë†ùë†ùë† ùëõùëõ = 1
‚àëùê¥ùê¥ùë†ùë†ùëñùëñ
ùëõùëõ(ùëõùëõ ‚àí 1)
2ÔøΩ
