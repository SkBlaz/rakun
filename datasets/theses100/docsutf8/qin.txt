ABSTRACT
With the development of hardware, mobile phone has become a feature-rich handheld device. Built-in camera and Bluetooth technology are supported in most current mobile phones. A real-time image processing experiment was conducted with a SonyEricsson P910i smartphone and a desktop computer. This thesis describes the design and implementation of a system which uses a mobile phone as a PC mouse. The movement of the mobile phone can be detected by analyzing the images captured by the onboard camera and the mouse cursor in the PC can be controlled by the movement of the phone.

ii

ACKNOWLEDGEMENTS
I would like to thank my supervisor, Bill Rogers, for his valued ideas, helpful advices, patience and continuous support during the time of my study. Thanks also to the Waikato University TSG (Technical Support Group) people for providing the experiment environment.

iii

CONTENTS
Abstract .......................................................................................ii Acknowledgement ...........................................................................iii Contents .......................................................................................iv

Chapter 1 Introduction ......................................................................1 1.1 The mobile phone as a Swiss army knife .........................................1 1.2 The structure of the thesis ...........................................................2

Chapter 2 Literature Review ..............................................................3 2.1 Marker Tracking and HMD Calibration for a Video-based Augmented Reality Conferencing System ...........................................3 2.2 Location based Applications for Mobile Augmented Reality ...................5 2.3 Rapid and Accurate Deployment of Fiducial Markers for Augmented Reality .....................................................................7 2.4 Toolkit for Bar Code Recognition and Resolving on Camera Phones 足 Jump Starting the Internet of Things......................................8 2.5 Using camera-equipped mobile phones for interacting with real-world objects .....................................................................10 2.6 Adaptive image thresholding algorithm ............................................11 2.7 Mixed Interaction Spaces .............................................................14 2.8 Computer Vision Face Tracking For Use in a Perceptual User Interface .....16

Chapter 3 Technical Background ..........................................................17 3.1 Experiment environment 3.1.1 Hardware environment ...........................................................17 .....................................................17

3.1.2 Development environment .....................................................18 3.2 Brief introduction of Bluetooth ....................................................19
iv

3.2.1 The structure of the Bluetooth network ......................................19 3.2.2 Bluetooth Protocol...............................................................20 3.3 Brief Introduction of Camera Video Format ...................................23 3.3.1 Colour space .....................................................................24 3.3.2 The RGB colour model ........................................................24 3.3.3 YUV colour model ..........................................................26

3.3.4 P910i Video Format ............................................................26 3.3.5 RGB YUV conversions ........................................................27 3.4 Methods used ........................................................................28

Chapter 4 System Design ....................................................................30 4.1 Image recognition....................................................................31 4.1.1 Patterned surface ..............................................................31 4.1.2 Conversion to Binary Image ................................................33

4.1.2.1 Using Y plane only .......................................................34 4.1.2.2 Using a Fixed Threshold Value .......................................34 4.1.3 Intersection finding ............................................................38 4.2 Bluetooth network.....................................................................39 4.2.1 Bluetooth protocol................................................................39 4.2.2 Simple Video Transfer Protocol (SVTP) .....................................40 4.3 Revised Design.........................................................................43

Chapter 5 Implementation

................................................................47

5.1 Mobile device application implementation.........................................48 5.1.1 Symbian GUI programming....................................................50 5.1.1.1 Naming Conventions......................................................50 5.1.1.2 Exception handling........................................................50 5.1.1.3 Two-phase construction...................................................52 5.1.1.4 Symbian GUI application framework .................................53 5.1.2 Mobile Phone Class Details ...................................................53
v

5.1.2.1 Bluetooth network handler ..............................................55 5.1.2.2 Image recognition ........................................................58 5.1.2.3 SVTP (Simple Video Transfer Protocol) implementation ..........62 5.1.2.4 The control class ..........................................................63 5.1.2.5 The view class ..............................................................64 5.2 Windows application implementation................................................65 5.2.1 Windows Bluetooth programming.............................................67 5.2.2 Structure of the server application.............................................68 5.2.2.2 Protocol handler ...........................................................70 5.2.2.3 Mouse controller..........................................................71 5.2.2.4 User interface object......................................................73

Chapter 6 Conclusion...................................................................................76

Chapter 7 Further work.....................................................................78

Bibliography....................................................................................80

vi

List of Figures
Figure 2.1: An example of Fiducial Marker ................................................ 3 Figure 2.2, CSCW System configuration ................................................... 4 Figure 2.3: CSCW AR system ................................................................ 4 Figure 2.4: Location based Mobile AR system ............................................ 6 Figure 2.5: Screenshot of BAUML viewer visualizing a section of the building model ..............................................................7 Figure 2.6: Screenshots of the bar code applications ...................................... 8 Figure 2.7: Visual code mobile phone application ........................................10 Figure 2.8: Image processing sequence .....................................................10 Figure 2.9: The paper with strong illumination gradient ..................................12 Figure 2.10: The result using a global threshold ...........................................12 Figure 2.11: Result of using adaptive thresholding ........................................13 Figure 2.12: The Mixed Interaction Space with face tracking ............................14 Figure 3.1: A Photo of P910i mobile phone .................................................18 Figure 3.2: Three different Bluetooth network topology ..................................20 Figure 3.3: Bluetooth protocol stack .........................................................21 Figure 3.4: Image shows the mixing result of red, green and blue ......................25 Figure 3.5: P910i video format ...............................................................27 Figure 4.1:The system architecture ..........................................................30 Figure 4.2: The hand drawn pattern used to help position recognition ..................32 Figure 4.3: Mobile phone was tracking the intersections .................................32 Figure 4.4: Zoomed out image ...............................................................34 Figure 4.5: Converted binary image .........................................................35 Figure 4.6: Luminance distribution ..........................................................35 Figure 4.7: Adaptive thresholding ............................................................36 Figure 4.8: One frame of captured stream ...................................................36 Figure 4.9: Using fixed value of threshold ..................................................36
vii

Figure 4.10: Result of using adaptive thresholding ........................................37 Figure 4.11: The final result ..................................................................37 Figure 4.12: Intersection finding .................................................................39 Figure 4.13: Before A went out ...............................................................43 Figure 4.14: After A went out .................................................................43 Figure 4.15: The new pattern .................................................................45 Figure 4.16: Tracking algorithm .............................................................45 Figure 5.1: The main process flow for the client ..........................................49 Figure 5.2: Client side class diagram ........................................................54 Figure 5.3: Our threshold algorithm .........................................................59 Figure 5.4: Binary result ......................................................................60 Figure 5.5: Minimum size test ................................................................61 Figure 5.6: Screenshot of a running mobile phone application ...........................65 Figure 5.7: Process flowchart of the server application ...................................66 Figure 5.8: Class diagram for the server application .......................................69 Figure 5.9: Screenshot of early implementation ...........................................74 Figure 5.10: Screenshot of the server application ..........................................75

viii

List of Tables
Table 3.1: The devices used for this project ................................................17 Table 4.1: Control packet structure ...........................................................42 Table 4.2: Raw video data packet structure ..................................................42 Table 4.3: Position data packet structure ....................................................42

ix

Chapter 1 Introduction
1.1 The mobile phone as a Swiss army knife

Nowadays, portable computing devices are becoming more popular in daily use, especially smart phones. Because people usually carry a phone with them at all times, it is interesting to consider alternative uses to which the phone might be put. For example, a phone has a built-in screen light, and it can therefore be used as a torch. Whilst it is not very bright and the light is not focused into a beam, it may help someone to find a key hole on a dark night. The purpose of my research is to explore ways in which we could make use of the features of a mobile device to control a computer.

From the technical view point, the computing capability of embedded devices is steadily increasing. Phones equipped with Bluetooth technology are available, and permit rapid communication with other computing devices. The onboard camera is becoming a standard part of most current mobile phones. It can be used as an alternative to working with a stylus on a tiny screen. Also, mobile phones have similar size and shape to a mouse. These provide the possibility of creating such an application to provide wireless services between a smart phone and a computer. If the idea could be implemented, it could bring a new convenient tool to computer users. Furthermore, if some image processing were used in such a system, the onboard camera in the smart phone could play the role of input device to the computers, which could bring some more interesting features to those mobile devices. For example, a user could use the smart phone as a wireless mouse and handwriting board, sitting wherever he likes. Gently touching his mobile screen by stylus, he can easily control the projector screen or point out the notes for students through the media of a computer.
1

Our research question then was, could we develop a system which would provide some functionalities based on the connection of computers and handheld devices. For example, the mobile phone could control music player, the slides demonstration or be used as the computer mouse. In this project, we experimented with using a mobile phone as a mouse to control a laptop or desktop PC. We tested our ideas not only of the possibility but also of the usability. The framework of our system includes smart phone touch screen input, onboard camera handling in smart phone and Bluetooth communication between the PC and the mobile device.

1.2 The structure of the thesis

There are seven chapters in this thesis. This chapter explains the original idea and the purpose of this project. In the next chapter, some other projects or related articles will be introduced. The chapter three is intended to introduce the experimental environment and the knowledge required for building the system. In chapter four, our system design is presented and in the following chapter the implementation of the design is explained in detail. In chapter six, some conclusions are drawn from the experiment. In the last part, chapter seven, some advice and possible further improvements are listed.

2

Chapter 2 Review of literature
There are many examples in the literature in which image processing is used to support navigation tasks of various kinds. In this chapter we review some of this work that is relevant to the project. In most situations where image processing has been used for navigation, an important goal has been to achieve real time performance and low latency 足 at least 10 frames per second. A number of methods have been used to simplify the image processing to meet the real time criterion. Some are discussed in the review.

A number of system use Fiducial Markers positioned in the environment.

Figure 2.1: An example of Fiducial Marker

A Fiducial Marker is a large simple icon drawn in black on a white background. It is reasonably easy to recognize because of the high contrast (black/white). Also, a reasonable range of shapes is possible to identify different locations.

2.1 Marker Tracking and HMD Calibration for a Video-based Augmented Reality Conferencing System (Kato and Billinghurst, 1999)

The system Kato and Billinghurst (1999) introduced provided a new user interface technology, Human-Human interface instead of Human-Computer interface. It
3

supports the idea of 3D Computer Supported Collaborative Work ( CSCW), which let users interact with each other by the computer generated shared virtual object. The system structure is shown below, the figure is from their paper.

Figure 2.2, CSCW System configuration

The system employed an optical see-through augmented reality technique. The AR user could see the real world directly through the head mounted display with superimposed virtual objects. The share object was marked by six fiducial markers which were used to calculate the virtual whiteboard coordinates and all 3D relative calibration. And also there were some fiducial markers placed out of the shared whiteboard for positioning the desktop users. The video from desktop users' camera can be directly positioned on those markers. Hence, the system could provide a multiuser workplace. All users could work on the same virtual whiteboard at the same time.

Figure 2.3: CSCW AR system. The left is a user who was doing the test with the system, The right is the image the user saw, which was drawn on the HMD

They have solved two major problems for the project, "calibration of the HMD and camera, and estimating an accurate position and pose of fiducial markers" (Kato and
4

Billinghurst, 1999). The essential idea of the solution is to find the corresponding transformation matrices. The shared object coordinates then could be transformed into the camera's coordinate system, hence, the eye's coordinate system. Finally, the HMD (Head Mounted Display) screen coordinates could be calculated by the perspective projection of the objects in eye's 3D coordinates.

The experimental results mentioned in the paper showed that the model they built worked more accurately when the objects were close to the camera. When the camera was moving away from the objects, the accuracy decreased. This problem might have been caused by the size of the markers. As the outline contour of markers were involved in 3D coordinate calculation, the smaller marker would give less accuracy of the shared whiteboard 3D coordinates.

2.2 Location based Applications for Mobile Augmented Reality (Reitmayr and Schmalstieg, 2003)

This is an indoor location navigation system. The user could navigate around a building with some necessary equipment. The equipment included: a HMD; helmet with camera and inertial sensor; a notebook computer for data processing and a pair of special gloves for capturing hand posture. The structure of the building was known by the computer. More importantly, the building was labeled with optical marks (fiducial markers) which can be captured by the camera and recognized by computer. Moreover, the gloves have attached optical markers which are used to locate the position of the user and interact with the user. When a user is walking through the building, the augmented reality technology is used to help the user to navigate the building. They built two location based applications which are based on their mobile augmented reality system. One is Signpost. Signpost is a kind of location finding software. The user could know where he/she is and the shortest path would be shown on the HMD. The other software is a book retrieving system. It could give the location hints about a book.
5

Fiducial Markers

Figure 2.4: Location based Mobile AR system. The top shows the environment; the bottom left shows the HMD, helmet and the inertial sensor; the bottom right shows the image displayed in HMD

The basic idea of this system is that optical marker tracking is used for locating position and augmented reality is used to interact with the user.

Comparing to our project, location finding could be based on a similar solution, tracking simple physical markers. Augmented reality is not necessary in our case
6

since we are concerned with navigation on a computer screen, not a real world situation. Also, movement is more important than the actual position to us. In their system, the position of each marker is well-known before navigating whereas, in our system, the position of markers will be unknown it brings us more flexibility as the surface used could be created at any time as long as it satisfies the surface requirement. If we define some less constrained rules, the surface could even be drawn by hand.

2.3 Rapid and Accurate Deployment of Fiducial Markers for Augmented Reality (Schall, Newman and Schmalstieg, 2005)

This paper discussed a way to rapidly generate the virtual 3D model of environment. Several surveying areas were selected before experiments. Also, some common markers must be positioned between two adjacent surveying areas and the common markers were used for transforming the spatial relationship between markers. During the surveying, each recognized marker would be saved in 3D Cartesian coordinates converted from local polar coordinate system. After all survey points had been checked in sequence, the 3D model was also created based on the saved information.

Figure 2.5: Screenshot of BAUML viewer visualizing a section of the building model
7

The most important technique in this paper is to use the spatial relationship to transform the different local coordinates to global coordinates. In this way, the final 3D model could be generated dynamically and simultaneously while the camera was moving. Common points are essential to this method. They were used to connect all the discrete points (objects) so that all local information could be transformed into global coordinates. The idea of bridging neighbouring areas with common markers could help to solve local-global converting problems.

The Fiducial Marker systems are generally implemented on desktop computers and dealing with the three dimensional space positioning problems. A system designed to work on a mobile phone may use similar ideas. In some systems, one dimensional or two dimensional barcodes are used with mobile devices.

2.4 Toolkit for Bar Code Recognition and Resolving on Camera Phones 足 Jump Starting the Internet of Things (Adelmann, Langheinrich and Floerkemeier, 2006)

Figure 2.6: Screenshots of the bar code applications

This paper described a system design and some prototypical applications. The system recognizes 1D barcodes using the onboard camera of a mobile phone and displays some relevant information retrieved from a server. As daily use of mobile phone becomes more popular, using the onboard camera of a mobile phone instead of extra scanning devices is a handy way for a user to retrieve information on a
8

specific product. Basically, the system was designed with two parts, the barcode recognition client and the information server. The client captures the image of the barcode printed on the package of the product and recognizes the code from the still image. The code, is then sent to the information server. The server will query some relevant information from the local database or the Internet when the retrieval request is received. Finally, the information will be sent back to the client and displayed to the user. The information server could be mobile based software or PC software.

As mentioned in the paper, the toolkit, especially the recognition algorithm, was designed for low computing resource devices like mobile phones. Although, there are some more professional barcode recognition algorithms available with more reliable results, those are still too expensive in terms of image processing time. Note that recovering bar codes from images is a different approach to that used in shops to scan products. The shop scanners work by scanning a laser beam over the product and using specialized electronics to process the result. The image analysis was based on a "scanlines" method. To improve the results' reliability, they used multiple scanlines instead of one line. The final result was determined by the majority of the multiple scanlines. The "scanlines" method could also be used in our project to process images as fast as possible. In our case, we have to deal with video stream continuously instead of some still images. The algorithm we choose must be fast enough to do real time processing.

2.5 Using camera-equipped mobile phones for interacting with real-world objects (Rohs and Gfeller, 2004)

"The idea described in this paper is to use the built-in cameras of consumer mobile phones as sensors for 2-dimensional visual codes" (Rohs and Gfeller, 2004). The idea of this paper is to code the items into small images and use the onboard camera to find the items by figuring out the corresponding codes of the small images though an
9

image recognition algorithm.

Figure 2.7: Visual code mobile phone application. The left is the visual code example and the right is the running application

To reduce the effect of low quality images, which could be distorted, blurred with strong illumination gradient affection, captured from the onboard cameras, the paper gives some ideas to follow.

1. using binary images ( black and white image) instead of colour or grayscale images for further processing 2. using adaptive thresholding method to eliminate the effects of lighting level and gradients 3. the visual coding schema should be carefully designed to reduce the complexity of image recognition algorithm

Also, the steps of achieving the goal is outlined clearly.

Captured image

Grayscale image

Binary image

Location finding algorithm

Figure 2.8: Image processing sequence
10

The steps shown above are very helpful for most image processing projects, especially for the low complexity images. Our goal is to have a similar way to calculate the motion of the mobile phone but a different visual coding schema using less time. For the purpose of mouse control by mobile phone, the high accuracy of code recognition is not necessary; instead, detecting the movement of the mobile is important. Therefore, the code could be simpler, even hand drawn grids could fulfill the task.

An important aspect of the implementation described above was the use of adaptive thresholding.

2.6 Adaptive image thresholding algorithm (Fisher, Perkins, Walker and Wolfart, 1994)

This paper talks about adaptive thresholding. Thresholding is a way to segment a multi-value image to a two-value image. Changing "The intensity values above a threshold to a foreground value and all the remaining pixels to a background value." (Fisher, Perkins, Walker and Wolfart, 1994). Thresholding is the most widely used method to generate a binary image for further processing. The simplest way of using thresholding is to calculate the average value of all pixels in an image and then comparing to each pixel with the average. However, in most cases, the light is not evenly spread over the surface. Sometimes, a strong illumination gradient could affect the quality of the thresholding results greatly, which could stop further processing. Some examples from a page image show the effect of illumination gradient.

11

Figure 2.9: The paper with strong illumination gradient

Figure 2.10: The result using a global threshold
12

As can be seen above, the poor result is caused by the illumination issue. The better solution for this problem is to use local adaptive thresholding instead of global thresholding. For each pixel, only the pixels within certain area around that pixel are taken into account. Therefore, the threshold for each pixel only depends on its neighbouring pixels.

Figure 2.11: Result of using adaptive thresholding

In our experiment, real time image processing is needed and the lighting conditions could keep changing during the movement of mobile device. In particular, because the phone is being used close to the scanned surface, there is often shadow from the phone and users hand. Adaptive thresholding, thus, would be the better choice to achieve the greatest quality.

A mobile phone system that is most like our proposed system is a Mixed Interaction Spaces. It allows a user to use a mobile phone as a 3D cursor.

13

2.7 Mixed Interaction Spaces (Hansen, Eriksson and Lykke-Olesen, 2005)

Mixed Interaction Spaces: This paper "describes a new interaction technique for mobile devices named Mixed Interaction Space that uses the camera of the mobile device to track the position, size and rotation of a fixed-point" (Hansen, Eriksson and Lykke-Olesen, 2005). The technique is to use the space surrounding the mobile device. In particular, it uses the human face as a physical mark instead of printed or hand drawn mark. All space surround the mobile device is available for use. When the camera is moving, the space is changing, then, the change is converted as 4 dimensional vector as input. The four dimensions include zoom in and zoom out, up and down, left and right, also the rotation. The following shows the space and tracking.

Figure 2.12: The Mixed Interaction Space with face tracking

The highlight of this idea is using the face instead of other fixed-points as the tracing marks. This avoid the need for other devices or related materials. Generally, physical marks like points are selected from some patterned surface or still images. Using
14

MIXIS is more convenient. When user interacts with mobile device, only thing he/she has to do is moving the mobile in front of his/her face. Whereas, in other systems, there are still more devices involved, for example, the coded patterns on the surface or some kind of circle or cross is used to trace the change of movement. Furthermore, since it explores 3D space instead of 2D space, it could provide more input information by same operations than other tracking methods. However, the MIXIS technique limits the way of interaction. This works well for applications on the phone. For example, panning or zooming a map display. It is not convenient for controlling something else as it is too easy to lose the face from the camera view. The user has to look at the camera while using it, or at least, the user has to face the camera. As the MIXIS is designed for mobile applications, there are no other devices like computers involved, the mobile device could take advantage of this technique. If the PC and mobile device are combined together as a whole system, the mobile device play the role of an input device, and is not the focus of the user's attention. This system is not ideal for our application.

As the image recognition processes are generally very complex, the high performance hardware is required. The system on mobile phone used CAMSHIFT to reduce amout of image analysis due to its slow processor.

2.8 Computer Vision Face Tracking For Use in a Perceptual User Interface (Bradski, 1998)

People from Intel lab introduced an algorithm, named Continuously Adaptive Mean Shift (CAMSHIFT) algorithm, which is used to dynamically track human faces. As described in the paper, CAMSHIFT is an improved algorithm based on the Mean Shift algorithm. The Mean Shift algorithm was designed to deal with a static model of colour probability distribution whereas CAMSHITF can process the model finding with a dynamically changing colour probability distribution. A simple description of their face tracking process is listed below:
15

1. record the face image 2. use HSV(Hue Saturation Value) colour system to convert the image to 1D histograms, and save it as a colour model 3. for each frame from the camera, convert all pixels to a probability of flesh image. 4. use CAMSHIFT to locate the centroid of the last search window and calculate the new search window centered at the centroid found for next frame.

The algorithm can track four dimensional changes for a user, X, Y, Z and head roll, which could be used in variety of areas. Also, it has the ability to cope with noise. An example mentioned in their paper showed after addeding 0, 10, 30 and 50% uniform noise data to the original video frames the result was still reliable.

The results generated from their experiments had proved their algorithm is a reliable solution for human face or hand tracking. The CAMSHIFT algorithm is somewhat complex or computational expensive algorithm since a great deal of mathematical calculations is involved. However the idea of performing image analysis in a search window based on the result form the last frame is used in our project.

16

Chapter 3 Technical Background
Before entering the more complex parts, I will give an introduction of the experimental environment and the knowledge required for building the system. In this chapter, the environment will be described first. Bluetooth technology and the camera related graphics system will be explained respectively. Finally, some methods that I have used to understand these ideas are explained.

3.1 Experiment environment

The experimental environment contains two parts, the hardware and the software. The hardware includes the mobile phone and the PC. The software involves the development tools that are used in this project.

3.1.1 Hardware environment

To make the mobile device interact with the PC, we have to run our software in two systems, the mobile system and the PC system. The development platform for both systems is the PC. Table 3.1 shows the devices.

Desktop Computer

Mobile phone SE P910i

CPU RAM Accessory

Intel Pentium速 4 at 2.60GHz 504MB B